[TOC]
# 任务10：自然语言处理中的关键词抽取

## 1 任务目标

1. 了解文本关键词抽取的背景和意义
2. 掌握基于TF-IDF的关键词抽取的原理与实践
3. 掌握基于TextRank的关键词抽取的原理与实践
4. 掌握基于Word2Vec词聚类的关键词抽取的原理与实践


## 2 任务描述

在自然语言处理领域，处理海量的文本文件最关键的是要把用户最关心的问题提取出来。

而无论是对于长文本还是短文本，往往可以通过几个关键词就可以对文本信息进行高度凝练，通过3-5个词语准确概括文本的主题，帮助读者快速理解文本信息。

与此同时，不管是基于文本的推荐还是基于文本的搜索，对于文本关键词的依赖也很大，关键词提取的准确程度直接关系到推荐系统或者搜索系统的最终效果。

因此，文本关键词提取（Automatic Keyphrase extraction）在文本挖掘领域是一个很重要的部分。

那么，给你一篇很长的文章，如果要在完全不加以人工干预的情况下用计算机提取它的关键词，请问怎样才能正确完成任务呢？

今天我们就来学习不同关键词提取的方法。



## 3 知识准备

一篇文档的关键词等同于最能表达文档主旨的N个词语，即对于文档来说最重要的词，因此，可以将文本关键词抽取问题转化为词语重要性排序问题，选取排名前TopN个词语作为文本关键词。

目前，主流的文本关键词抽取方法主要有以下两大类：

### 3.1 基于统计的关键词提取方法

此类方法根据统计信息，如词频，来计算得到文档中词语的权重，按权重值排序提取关键词。

基于统计特征的关键词的重点在于特征量化指标的计算，不同的量化指标得到的结果也不尽相同。

同时，不同的量化指标作为也有其各自的优缺点，在实际应用中，通常是采用不同的量化指标相结合的方式得到Topk个词作为关键词。

#### 3.1.1 TF-IDF方法提取关键词

说到关键词，一个容易想到的思路，就是找到出现次数最多的词。如果某个词很重要，它应该在这篇文章中多次出现。

于是，我们进行**词频**（Term Frequency，缩写为TF）统计。

一个词的TF是指这个词在文档中出现的频率，假设一个词w在文本中出现了m次，而文本中词的总数为n，那么它的词频可以表示为：


```math
TF_w = \frac{m}{n}
```

假设我们要分析《中国人工智能的发展》这篇长文，通过上式计算词频，计算结果我们可以想到，出现次数最多的词"的"、"是"、"在"这一类最常用的词。

这里我们就要回顾以前所学到的停用词知识：我们必须过滤掉对结果毫无帮助的词。

当我们把该停用的词过滤掉以后，我们还有可能遇到另一个问题：
> 要是“中国”“人工智能”“发展”三个词的频率一样怎么办？作为关键词，它们的重要性是一样的吗？

显然不是这样。因为“中国”是很常见的词，相对而言，“人工智能”和“发展”不那么常见。

如果这三个词在一篇文章的出现次数一样多，有理由认为，“人工智能”和“发展”的重要程度要大于“中国”，也就是说，在关键词排序上面，“人工智能”和“发展”应该排在“中国”的前面。

这就说明我们需要一个**重要性调整系数**，衡量一个词是不是常见词。
- 如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。

用统计学语言表达，就是在词频的基础上，要对每个词分配一个"重要性"权重。最常见的词（"的"、"是"、"在"）给予最小的权重，较常见的词（“中国”）给予较小的权重，较少见的词（“人工智能”、“发展”）给予较大的权重。

这个权重叫做**“逆文档频率”**（Inverse Document Frequency，缩写为IDF），它的大小与一个词的常见程度成反比。

一个词的IDF是根据语料库得出的，表示这个词在整个语料库中出现的频率。假设整个语料库中，包含词w的文本一共有M篇，语料库中的文本一共有N篇，则

```math
IDF_w = log_2\frac{N}{M}
```

将这两个值相乘，就得到了一个词的TF-IDF值。某个词对文章的重要性越高，它的TF-IDF值就越大。所以，排在最前面的几个词，就是这篇文章的关键词。

所以由上面两个式子可得词w的TF-IDF值为：


```math
TFIDF_w = TF_w*IDF_w
```

除了自动提取关键词，TF-IDF算法还可以用于许多别的地方。

比如，信息检索时，对于每个文档，都可以分别计算一组搜索词（“中国”、“人工智能”、“发展”）的TF-IDF，将它们相加，就可以得到整个文档的TF-IDF。这个值最高的文档就是与搜索词最相关的文档。

###### TF-IDF优点：
- 简单快速，相对容易理解，结果比较符合实际情况。

###### TF-IDF缺点：
- 严重依赖语料库，需要选取质量较高且和所处理文本相符的语料库进行训练。
- 这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的；例如文本的标题、文本的首句和尾句等含有较重要的信息，才应该赋予较高的权重。
    - 针对这个问题有一种解决方法：对全文的第一段和每一段的第一句话，给予较大的权重。
- 对于IDF来说，它本身是一种试图抑制噪声的加权，本身倾向于文本中频率小的词，这使得TF-IDF算法的精度不高。

#### 3.1.2 TextRank方法提取关键词

TextRank算法是由网页重要性排序算法PageRank算法迁移而来：PageRank算法根据万维网上页面之间的链接关系计算每个页面的重要性。

TextRank算法将词视为“万维网上的节点”，根据词之间的共现关系计算每个词的重要性，并将PageRank中的有向边变为无向边。

所以，在介绍TextRank算法之前，先介绍一下PageRank算法。

##### 3.1.2.1 什么是PageRank算法

1. 概念
    PageRank算法的起源要从搜索引擎的发展讲起。
    
    早期的搜索引擎普遍采用分类目录方法，即通过人工对网页进行分类，整理出高质量的网站。随着网页的增多，人工分类的方法变得不现实，人们开始尝试使用文本检索的方法，即通过计算用户查询的关键词与网页内容的相关程度来返回搜索结果。
    
    这种方法突破了网页数量的限制，但是这种方法的效果并不总是很好，因为某些网站会刻意“操作”某些关键词从而使自己的搜索排名靠前。
    
    这一问题在1998年4月的第七届国际万维网大会上得以解决——Larry Page和Sergey Brin提出了PageRank算法。该算法通过计算网页链接的数量和质量来粗略估计网页的重要性，算法创立之初即应用在谷歌的搜索引擎中，对网页进行排名。

2. 核心思想
    - 如果一个网页被很多其他网页链接到，说明这个网页比较重要，即该网页的PR值（PageRank值）会相对较高；
    - 如果一个PR值很高的网页链接到一个其他网页，那么被链接到的网页的PR值会相应地因此而提高。
    - 以投票机制的观点来看，一个网页的得票数由所有链向它的网页的得票数经过递归算法来得到，有到一个网页的超链接相当于对该网页投了一票。

3. 举例说明
假如有以下两种情形：
    1. 如图所示，这是一个只由4个网页组成的集合：A、B、C和D，如果网页B、C、D都链向网页A，且网页B、C、D均没有链出：
    
        ![image](https://img-blog.csdn.net/20180517170836160?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dvdHVpMTg0Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
    
    那么网页A的PR值将是网页B、C、D的PR值之和:
        
        ```math
        PR(A)=PR(B)+PR(C)+PR(D)
        ```
    
    2. 如图所示，假设一个只由4个网页组成的集合：A、B、C和D。网页B有链接链向网页C，网页D有链接链向网页A、B、C，一个网页不能多次投票，所以网页B投给它链向的网页1/2票，网页D投给它链向的网页1/3票：
    
        ![image](https://img-blog.csdn.net/20180517171714751?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dvdHVpMTg0Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
    
    那么网页A的PR值将是：
    
    ![image](https://img-blog.csdn.net/2018051720205834)

    即，在一个网页为其他网页投票时，根据链出总数平分该网页的PR值，将其作为该网页为其链向网页所投票数，即：
    
![image](https://img-blog.csdn.net/20180517202155765)
    3. 再抽象一下，建立一个简化模型，对于任意的网页i，它的PR值可以表示如下：

![image](https://img-blog.csdn.net/20180517215941149)

##### 3.1.2.2 从PageRank算法到TextRank算法

TextRank算法是由PageRank算法改进而来的，二者的思想有相同之处。

区别在于：
- PageRank算法根据网页之间的链接关系构造网络，而TextRank算法根据词之间的共现关系构造网络；
- PageRank算法构造的网络中的边是有向无权边，而TextRank算法构造的网络中的边是无向有权边。

TextRank算法的核心公式如下，其中
```math
w_ji
```
用于表示两个节点之间的边连接具有不同的重要程度：
![image](https://img-blog.csdn.net/20180518160433745)

TextRank用于关键词提取的算法如下：
1. 把给定的文本T按照完整句子进行分割，即

![image](https://pic2.zhimg.com/80/v2-8c209ef93e503864960a934bc6a5b389_hd.jpg)

2. 对于每个句子Si属于T，进行分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词，即

![image](https://pic3.zhimg.com/80/v2-d9ab45963e0e227ebc98e871802fa31a_hd.jpg)

，其中 ti,j 是保留后的候选关键词。

3. 构建候选关键词图G = (V,E)，其中V为节点集，由第二步生成的候选关键词组成，然后采用共现关系（co-occurrence）构造任两点之间的边，两个节点之间存在边仅当它们对应的词汇在长度为K的窗口中共现，K表示窗口大小，即最多共现K个单词。
4. 根据上面公式，迭代传播各节点的权重，直至收敛。
5. 对节点权重进行倒序排序，从而得到最重要的T个单词，作为候选关键词。
6. 由上一步得到最重要的T个单词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词。
    - 例如，文本中有句子“Matlab code for plotting ambiguity function”，如果“Matlab”和“code”均属于候选关键词，则组合成“Matlab code”加入关键词序列。

所以， TextRank算法提取关键词短语的方法基于关键词提取，可以简单认为：如果提取出的若干关键词在文本中相邻，那么构成一个被提取的关键短语。

此类方法简单易行，适用性较强，然而仍未考虑词序问题。


### 3.2 基于机器学习的关键词提取方法

此方法包括了SVM、朴素贝叶斯等有监督学习方法，以及K-means、层次聚类等无监督学习方法。

在此类方法中，模型的好坏取决于特征提取，而深度学习正是特征提取的一种有效方式。

在前面的学习中我们知道，由Google推出的Word2Vec词向量模型，是自然语言领域中具有代表性的学习工具。

它在训练语言模型的过程中将词典映射到一个更抽象的向量空间中，每一个词语通过高维向量表示，该向量空间中两点之间的距离就对应两个词语的相似程度。

本任务中我们重点来说一下Word2Vec词聚类方法抽取关键词的原理。

#### 3.3.1 Word2Vec词聚类方法

我们知道，Word2Vec利用浅层神经网络模型自动学习词语在语料库中的出现情况，可以把词语嵌入到一个高维的空间中，通常在100-500维，在新的高维空间中词语被表示为词向量的形式。

与传统的文本表示方式相比，Word2Vec生成的词向量表示，词语之间的语义关系在高维空间中得到了较好的体现，即语义相近的词语在高维空间中的距离更近；同时，使用词向量避免了词语表示的“维度灾难”问题。

Word2Vec词聚类文本关键词抽取方法的主要思路如下：

对于用词向量表示的文本词语，通过K-Means算法对文章中的词进行聚类，选择聚类中心作为文章的一个主要关键词，计算其他词与聚类中心的距离即相似度，选择topN个距离聚类中心最近的词作为文本关键词，而这个词间相似度可用Word2Vec生成的向量计算得到。

假设Dn为测试语料的大小，使用该方法进行文本关键词抽取的步骤如下所示：

1. 对语料进行Word2vec模型训练，得到词向量文件；
2. 对于给定的文本D进行分词、词性标注、去重和去除停用词等数据预处理操作。本分采用结巴分词，保留'n','nz','v','vd','vn','l','a','d'这几个词性的词语，最终得到n个候选关键词，即D=[t1,t2,…,tn] ；
3. 遍历候选关键词，从词向量文件中抽取候选关键词的词向量表示，即WV=[v1，v2，…，vm]；
4. 对候选关键词进行K-Means聚类，得到各个类别的聚类中心；
    - k-Means算法是一种聚类算法，它是一种无监督学习算法，目的是将相似的对象归到同一个簇中。
5. 计算各类别下，组内词语与聚类中心的距离（欧几里得距离），按聚类大小进行升序排序；
6.  对候选关键词计算结果得到排名前TopN个词汇作为文本关键词。


## 4 任务实施

### 4.1 实施思路
上面讲了这么多理论细节，那么如何完成本节课的任务呢？

1. 准备数据集
2. 实现基于 TF-IDF 算法的关键词抽取
3. 实现基于 TextRank 算法的关键词抽取
4. 实现基于Word2Vec词聚类的文本关键词抽取方法

#### 步骤1：准备数据集

本次任务将汽车行业的10篇专利作为样本数据集。

文件中依顺序包含编号（id）、标题（title）和摘要（abstract）三个字段，其中标题和摘要都要参与到关键词的抽取。

各位可根据自己的样本数据进行数据读取相关代码的调整。

#### 步骤2：实现基于 TF-IDF 算法的关键词抽取

Python第三方工具包Scikit-learn提供了TFIDF算法的相关函数，本文主要用到了sklearn.feature_extraction.text下的TfidfTransformer和CountVectorizer函数。其中，CountVectorizer函数用来构建语料库的中的词频矩阵，TfidfTransformer函数用来计算词语的tfidf权值。

1. 读取样本源文件sample_data.csv;
2. 获取每行记录的标题和摘要字段，并拼接这两个字段；
3. 加载自定义停用词表stopWord.txt，并对拼接的文本进行数据预处理操作，包括分词、筛选出符合词性的词语、去停用词，用空格分隔拼接成文本;
4. 遍历文本记录，将预处理完成的文本放入文档集corpus中；
5. 使用CountVectorizer()函数得到词频矩阵，a[j][i]表示第j个词在第i篇文档中的词频；
6. 使用TfidfTransformer()函数计算每个词的tf-idf权值；
7. 得到关键词以及对应的tf-idf矩阵；
8. 遍历tf-idf矩阵，打印每篇文档的词汇以及对应的权重；
9. 对每篇文档，按照词语权重值降序排列，选取排名前topN个词最为文本关键词，并写入数据框中；
10. 将最终结果写入文件keys_TFIDF.csv中。


```
#!/usr/bin/python
# coding=utf-8
# 采用TF-IDF方法提取文本关键词

import sys,codecs
import pandas as pd
import numpy as np
import jieba.posseg
import jieba.analyse
from sklearn import feature_extraction
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
"""
       TF-IDF权重：
           1、CountVectorizer 构建词频矩阵
           2、TfidfTransformer 构建tfidf权值计算
           3、文本的关键字
           4、对应的tfidf矩阵
"""
# 数据预处理操作：分词，去停用词，词性筛选
def dataPrepos(text, stopkey):
    l = []
    pos = ['n', 'nz', 'v', 'vd', 'vn', 'l', 'a', 'd']  # 定义选取的词性
    seg = jieba.posseg.cut(text)  # 分词
    for i in seg:
        if i.word not in stopkey and i.flag in pos:  # 去停用词 + 词性筛选
            l.append(i.word)
    return l

# tf-idf获取文本top10关键词
def getKeywords_tfidf(data,stopkey,topK):
    idList, titleList, abstractList = data['id'], data['title'], data['abstract']
    corpus = [] # 将所有文档输出到一个list中，一行就是一个文档
    for index in range(len(idList)):
        text = '%s。%s' % (titleList[index], abstractList[index]) # 拼接标题和摘要
        text = dataPrepos(text,stopkey) # 文本预处理
        text = " ".join(text) # 连接成字符串，空格分隔
        corpus.append(text)

    # 1、构建词频矩阵，将文本中的词语转换成词频矩阵
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(corpus) # 词频矩阵,a[i][j]:表示j词在第i个文本中的词频
    # 2、统计每个词的tf-idf权值
    transformer = TfidfTransformer()
    tfidf = transformer.fit_transform(X)
    # 3、获取词袋模型中的关键词
    word = vectorizer.get_feature_names()
    # 4、获取tf-idf矩阵，a[i][j]表示j词在i篇文本中的tf-idf权重
    weight = tfidf.toarray()
    # 5、打印词语权重
    ids, titles, keys = [], [], []
    for i in range(len(weight)):
        print (u"-------这里输出第", i+1 , u"篇文本的词语tf-idf------")
        ids.append(idList[i])
        titles.append(titleList[i])
        df_word,df_weight = [],[] # 当前文章的所有词汇列表、词汇对应权重列表
        for j in range(len(word)):
            print (word[j],weight[i][j])
            df_word.append(word[j])
            df_weight.append(weight[i][j])
        df_word = pd.DataFrame(df_word,columns=['word'])
        df_weight = pd.DataFrame(df_weight,columns=['weight'])
        word_weight = pd.concat([df_word, df_weight], axis=1) # 拼接词汇列表和权重列表
        word_weight = word_weight.sort_values(by="weight",ascending = False) # 按照权重值降序排列
        keyword = np.array(word_weight['word']) # 选择词汇列并转成数组格式
        word_split = [keyword[x] for x in range(0,topK)] # 抽取前topK个词汇作为关键词
        word_split = " ".join(word_split)
        keys.append(word_split.encode("utf-8"))

    result = pd.DataFrame({"id": ids, "title": titles, "key": keys},columns=['id','title','key'])
    return result


def main():
    # 读取数据集
    dataFile = 'data/sample_data.csv'
    data = pd.read_csv(dataFile)
    # 停用词表
    stopkey = [w.strip() for w in codecs.open('data/stopWord.txt', 'r').readlines()]
    # tf-idf关键词抽取
    result = getKeywords_tfidf(data,stopkey,10)
    result.to_csv("result/keys_TFIDF.csv",index=False)

if __name__ == '__main__':
    main()
```
最终运行结果如下图所示：

![image](https://s2.ax1x.com/2019/10/19/Kn6eqf.md.png)
#### 步骤2：实现基于TextRank方法提取文本关键词

Jieba库中包含jieba.analyse.textrank函数可直接实现TextRank算法：

```
# 其中(("ns", "n", "vn", "v"))表示词性为地名、名词、动名词、动词
jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))
```

1. 读取样本源文件sample_data.csv;
2. 获取每行记录的标题和摘要字段，并拼接这两个字段；
3. 加载自定义停用词表stopWord.txt；
4. 遍历文本记录，采用jieba.analyse.textrank函数筛选出指定词性，以及topN个文本关键词，并将结果存入数据框中；
5. 将最终结果写入文件keys_TFIDF.csv中。


```
#!/usr/bin/python
# coding=utf-8
# 实现基于TextRank方法提取文本关键词

import sys
import pandas as pd
import jieba.analyse
"""
       TextRank权重：
            1、将待抽取关键词的文本进行分词、去停用词、筛选词性
            2、以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图
            3、计算图中节点的PageRank，注意是无向带权图
"""

# 处理标题和摘要，提取关键词
def getKeywords_textrank(data,topK):
    idList,titleList,abstractList = data['id'],data['title'],data['abstract']
    ids, titles, keys = [], [], []
    for index in range(len(idList)):
        text = '%s。%s' % (titleList[index], abstractList[index]) # 拼接标题和摘要
        jieba.analyse.set_stop_words("data/stopWord.txt") # 加载自定义停用词表
        print ("\"",titleList[index],"\"" , " 10 Keywords - TextRank :")
        keywords = jieba.analyse.textrank(text, topK=topK, allowPOS=('n','nz','v','vd','vn','l','a','d'))  # TextRank关键词提取，词性筛选
        word_split = " ".join(keywords)
        print (word_split)
        keys.append(word_split.encode("utf-8"))
        ids.append(idList[index])
        titles.append(titleList[index])

    result = pd.DataFrame({"id": ids, "title": titles, "key": keys}, columns=['id', 'title', 'key'])
    return result

def main():
    dataFile = 'data/sample_data.csv'
    data = pd.read_csv(dataFile)
    result = getKeywords_textrank(data,10)
    result.to_csv("result/keys_TextRank.csv",index=False)

if __name__ == '__main__':
    main()
```
最终运行结果如下图所示：
![image](https://s2.ax1x.com/2019/10/19/Kn6BW9.png)

#### 步骤4：基于Word2Vec词聚类的文本关键词抽取方法

Python第三方工具包Scikit-learn提供了K-Means聚类算法的相关函数，本文用到了sklearn.cluster.KMeans()函数执行K-Means算法，sklearn.decomposition.PCA()函数用于数据降维以便绘制图形。


1. 读取样本源文件sample_data.csv;

2. 获取每行记录的标题和摘要字段，并拼接这两个字段；

3. 加载自定义停用词表stopWord.txt，并对拼接的文本进行数据预处理操作，包括分词、筛选出符合词性的词语、去重、去停用词，形成列表存储；

4. 读取词向量模型文件'wiki.zh.text.vector'（本任务已提供），从中抽取出所有候选关键词的词向量表示，存入文件中；

5. 读取文本的词向量表示文件，使用KMeans()函数得到聚类结果以及聚类中心的向量表示；

6. 采用欧式距离计算方法，计算得到每个词语与聚类中心的距离；

7. 按照得到的距离升序排列，选取排名前topN个词作为文本关键词，并写入数据框中；

9. 将最终结果写入文件keys_word2vec.csv中。


首先获取文本词向量表示：
```
# 采用Word2Vec词聚类方法抽取关键词1——获取文本词向量表示
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')  # 忽略警告
import sys, codecs
import pandas as pd
import numpy as np
import jieba
import jieba.posseg
import gensim

# 返回特征词向量
def getWordVecs(wordList, model):
    name = []
    vecs = []
    for word in wordList:
        word = word.replace('\n', '')
        try:
            if word in model:  # 模型中存在该词的向量表示
                name.append(word.encode('utf8'))
                vecs.append(model[word])
        except KeyError:
            continue
    a = pd.DataFrame(name, columns=['word'])
    b = pd.DataFrame(np.array(vecs, dtype='float'))
    return pd.concat([a, b], axis=1)

# 数据预处理操作：分词，去停用词，词性筛选
def dataPrepos(text, stopkey):
    l = []
    pos = ['n', 'nz', 'v', 'vd', 'vn', 'l', 'a', 'd']  # 定义选取的词性
    seg = jieba.posseg.cut(text)  # 分词
    for i in seg:
        if i.word not in l and i.word not in stopkey and i.flag in pos:  # 去重 + 去停用词 + 词性筛选
            l.append(i.word)
    return l

# 根据数据获取候选关键词词向量
def buildAllWordsVecs(data, stopkey, model):
    idList, titleList, abstractList = data['id'], data['title'], data['abstract']
    for index in range(len(idList)):
        id = idList[index]
        title = titleList[index]
        abstract = abstractList[index]
        l_ti = dataPrepos(title, stopkey)  # 处理标题
        l_ab = dataPrepos(abstract, stopkey)  # 处理摘要
        # 获取候选关键词的词向量
        words = np.append(l_ti, l_ab)  # 拼接数组元素
        words = list(set(words))  # 数组元素去重,得到候选关键词列表
        wordvecs = getWordVecs(words, model)  # 获取候选关键词的词向量表示
        # 词向量写入csv文件，每个词400维
        data_vecs = pd.DataFrame(wordvecs)
        data_vecs.to_csv('result/vecs/wordvecs_' + str(id) + '.csv', index=False)
        print ("document ", id, " well done.")

def main():
    # 读取数据集
    dataFile = 'data/sample_data.csv'
    data = pd.read_csv(dataFile)
    # 停用词表
    stopkey = [w.strip() for w in codecs.open('data/stopWord.txt', 'r').readlines()]
    # 词向量模型
    inp = 'wiki.zh.text.vector'
    model = gensim.models.KeyedVectors.load_word2vec_format(inp, binary=False)
    buildAllWordsVecs(data, stopkey, model)

if __name__ == '__main__':
    main()
```
文章1的部分结果显示如下：
![image](https://s2.ax1x.com/2019/10/19/Kn26Lq.md.png)

接着进行聚类分析：

```
#!/usr/bin/python
# coding=utf-8
# 采用Word2Vec词聚类方法抽取关键词2——根据候选关键词的词向量进行聚类分析
import sys,os
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math

# 对词向量采用K-means聚类抽取TopK关键词
def getkeywords_kmeans(data,topK):
    words = data["word"] # 词汇
    vecs = data.ix[:,1:] # 向量表示

    kmeans = KMeans(n_clusters=1,random_state=10).fit(vecs)
    labels = kmeans.labels_ #类别结果标签
    labels = pd.DataFrame(labels,columns=['label'])
    new_df = pd.concat([labels,vecs],axis=1)
    df_count_type = new_df.groupby('label').size() #各类别统计个数
    vec_center = kmeans.cluster_centers_ #聚类中心

    # 计算距离（相似性） 采用欧几里得距离（欧式距离）
    distances = []
    vec_words = np.array(vecs) # 候选关键词向量，dataFrame转array
    vec_center = vec_center[0] # 第一个类别聚类中心,本例只有一个类别
    length = len(vec_center) # 向量维度
    for index in range(len(vec_words)): # 候选关键词个数
        cur_wordvec = vec_words[index] # 当前词语的词向量
        dis = 0 # 向量距离
        for index2 in range(length):
            dis += (vec_center[index2]-cur_wordvec[index2])*(vec_center[index2]-cur_wordvec[index2])
        dis = math.sqrt(dis)
        distances.append(dis)
    distances = pd.DataFrame(distances,columns=['dis'])

    result = pd.concat([words, labels ,distances], axis=1) # 拼接词语与其对应中心点的距离
    result = result.sort_values(by="dis",ascending = True) # 按照距离大小进行升序排序

    # 将用于聚类的数据的特征维度降到2维
    # pca = PCA(n_components=2)
    # new_pca = pd.DataFrame(pca.fit_transform(new_df))
    # print( new_pca)
    # 可视化
    # d = new_pca[new_df['label'] == 0]
    # plt.plot(d[0],d[1],'r.')
    # d = new_pca[new_df['label'] == 1]
    # plt.plot(d[0], d[1], 'go')
    # d = new_pca[new_df['label'] == 2]
    # plt.plot(d[0], d[1], 'b*')
    # # plt.gcf().savefig('kmeans.png')
    # plt.show()

    # 抽取排名前topK个词语作为文本关键词
    wordlist = np.array(result['word']) # 选择词汇列并转成数组格式
    word_split = [wordlist[x] for x in range(0,topK)] # 抽取前topK个词汇
    word_split = " ".join(word_split)
    return word_split

def main():
    # 读取数据集
    dataFile = 'data/sample_data.csv'
    articleData = pd.read_csv(dataFile)
    ids, titles, keys = [], [], []

    rootdir = "result/vecs" # 词向量文件根目录
    fileList = os.listdir(rootdir) #列出文件夹下所有的目录与文件
    # 遍历文件
    for i in range(len(fileList)):
        filename = fileList[i]
        path = os.path.join(rootdir,filename)
        if os.path.isfile(path):
            data = pd.read_csv(path, encoding='utf-8') # 读取词向量文件数据
            artile_keys = getkeywords_kmeans(data,10) # 聚类算法得到当前文件的关键词
            # 根据文件名获得文章id以及标题
            (shortname, extension) = os.path.splitext(filename) # 得到文件名和文件扩展名
            t = shortname.split("_")
            article_id = int(t[len(t)-1]) # 获得文章id
            artile_tit = articleData[articleData.id==article_id]['title'] # 获得文章标题
            artile_tit = list(artile_tit)[0] # series转成字符串
            ids.append(article_id)
            titles.append(artile_tit)
            keys.append(artile_keys.encode("utf-8"))
    # 所有结果写入文件
    result = pd.DataFrame({"id": ids, "title": titles, "key": keys}, columns=['id', 'title', 'key'])
    result = result.sort_values(by="id",ascending=True) # 排序
    result.to_csv("result/keys_word2vec.csv", index=False)

if __name__ == '__main__':
    main()
```

结果显示如下：

![image](https://s2.ax1x.com/2019/10/19/Kn2ge0.md.png)
#### 分析总结
本任务总结了三种常用的抽取文本关键词的方法：TF-IDF、TextRank和Word2Vec词向量聚类，并做了原理、流程以及代码的详细描述。

因本任务使用的测试语料较为特殊且数量较少，结果分析不一定十分准确。但是根据观察可以发现，得到的十个文本关键词都包含有文本的主旨信息，其中TF-IDF和TextRank方法的结果较好，Word2Vec词向量聚类方法的效果不佳。

原因是对单文档直接应用Word2Vec词向量聚类方法时，选择聚类中心作为文本的关键词本身就是不准确的，因此与其距离最近的N个词语也不一定是关键词，因此用这种方法得到的结果效果不佳；

而TextRank方法是基于图模型的排序算法，在单文档关键词抽取方面有较为稳定的效果，因此较多的论文是在TextRank的方法上进行改进而提升关键词抽取的准确率。

另外，本文的实验目的主要在于讲解三种方法的思路和流程，实验过程中的某些细节仍然可以改进。

例如Word2Vec模型训练的原始语料可加入相应的专业性文本语料；

标题文本往往包含文档的重要信息，可对标题文本包含的词语给予一定的初始权重；

测试数据集可采集多个分类的长文本，与之对应的聚类算法KMeans()函数中的n_clusters参数就应当设置成分类的个数；

根据文档的分词结果，去除掉所有文档中都包含某一出现频次超过指定阈值的词语；等等。


## 5 任务拓展
**jieba中的基于 TF-IDF 算法的关键词抽取**

```
jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())
```

- sentence 为待提取的文本
- topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20
- withWeight 为是否一并返回关键词权重值，默认值为 False
- allowPOS 仅包括指定词性的词，默认值为空，即不筛选
- jieba.analyse.TFIDF(idf_path=None) 新建 TFIDF 实例，idf_path 为 IDF 频率文件


## 6 任务实训


### 6.1 实训目的
1. 了解文本关键词抽取的背景和意义
2. 熟练三种关键词抽取方法

### 6.2 实训内容
本任务还提供了另一套[文本数据集](https://raw.githubusercontent.com/DesertsX/JianShuJiaoYou/master/JianShuJiaoYou-All-Data.txt)，根据本节课学习的知识，你能对其进行关键词提取吗？试试看吧！

