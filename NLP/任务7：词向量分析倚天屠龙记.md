[TOC]
# 任务7：词向量分析倚天屠龙记

## 1 任务目标

1. 掌握word2vec 的理论精髓
2. 学会用gensim训练词向量，并寻找相似词

## 2 任务描述

NLP 里面，最细粒度的是词语，词语组成句子，句子再组成段落、篇章、文档。所以处理 NLP 的问题，首先就要拿词语开刀。

但是NLP中所建立的数学模型一般只接受数值输入，而我们所谓的词语则是符号形式的，这样的话怎么利用自然语言处理技术来直接处理我们的词语呢？

这就讲到了词向量。

在本章任务中，我们会通过学习词向量来分析《倚天屠龙记》中的人物关系，从而更深刻地理解词向量为自然语言处理任务带来的便利。


## 3 知识准备


### 3.1 什么是Word2vec？
我们知道，如果要判断一个词的词性，比如判断一个词是动词还是名词。

如果用机器学习的思路，那就是我们有一系列样本(x,y)，这里 x 是词语，y 是它们的词性，我们要构建 f(x)->y 的映射。

在 NLP 中，把 x 看做一个句子里的一个词语，y 是这个词语的上下文词语，那么这里的 f，便是 NLP 中经常出现的『语言模型』（language model），这个模型的目的，就是判断 (x,y) 这个样本，是否符合自然语言的法则。

更通俗点说就是：词语x和词语y放在一起，是不是人类语言。

但这里的数学模型 f（比如神经网络）只接受数值型输入，而 NLP 里的词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（Word embedding)。

而 Word2vec（词向量），就是词的向量表征（也称为词嵌入，Word embedding) 的一种。

词向量是将字词映射到向量空间的模型，简单理解就是把字词转换成向量，从而能让计算机进行操作和运算。

在这种方式里，每个词被表示成一个实数向量（One-Hot Vector），其长度为字典大小，每个维度对应一个字典里的每个词，除了这个词对应维度上的值是1，其他元素都是0。

### 3.2 什么是One-Hot Vector

one-hot向量是将类别变量转换为机器学习算法易于利用的一种形式的过程。

这个向量的表示为一项属性的特征向量，也就是同一时间只有一个激活点（即不为0的点），这个向量只有一个特征是不为0的，其他都是0，特别稀疏。

举个例子：

假如有一个性别特征，那么这个特征就只有["男性","女性"]两个特征值。

如果这个特征进行one-hot编码，

则"男性"的编码为"10"，女性的编码为"01"。

在此基础上，如果我们加上尺码特征["M","L","XL"]，

其中"M"可以表示为"100"，"L"表示为"010"，"XL"表示为"001"。

那么一个样本["男性","L"]的one-hot编码就可以表示为[10010]，这样一个5维的向量就可以用来表示我们的样本。

###### One-hot的优缺点：
优点：
- 能够处理非连续型数值特征，也就是离散值。
- 在一定程度上也扩充了特征。比如性别本身是一个特征，经过one hot编码以后，就变成了男或女两个特征，将离散特征通过one-hot编码映射到欧式空间，在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，这有利于我们后续的计算。
- 将离散型特征使用one-hot编码，可以会让特征之间的距离计算更加合理。比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是
    ```math
    x_1 = (1), x_2 = (2), x_3 = (3)
    ```
    两个工作之间的距离是
    ```math
    (x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2
    ```
    那么
    ```math
    x_1,x_3
    ```
    工作之间就越不相似吗？
    显然这样的表示，计算出来的特征的距离是不合理。那如果使用one-hot编码，则得到
    ```math
    x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)
    ```
    那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。
    
所以当词向量训练好后，语义相关性更高的词语会在投影上距离很近，而语义无关的词则会距离很远：
![image](https://wiki.jikexueyuan.com/project/deep-learning/images/04-01.png)

由图可见，a, the, these; big, huge等词距离很近，而say, business; decision, japan等词的投影距离较远。

不足：
- one-hot处理一般只用于特征并不总是连续值，而是是分类值和离散值的情况中。如果原本的标签编码是有序的，那one hot编码就会导致丢失顺序信息，并不适用。
- 如果特征的特征值数目特别多，特征向量就会非常大，且非常稀疏。

根据上述分析，当我们在自然语言处理的过程中，我们可以将文本分词，把分词后的所有单词作为一个总体的特征数目，进行one-hot编码，其中每一个单词就是一个one-hot向量，然后文本中每个句子也同样分词，分词后将这些单词作为一个句子的表示方式，这样一个句子就是一个二维向量，向量的行数是这个句子包含的单词总数，如此还可以用one-hot向量组成表示为文章。

### 3.3 Skip-gram 和 CBOW 模型
上面我们提到了语言模型，语言模型就是用来计算一个句子的概率的模型，也就是判断一句话是否是易于理解的人类语言的概率大小。

语言模型可用于提升语音识别和机器翻译的性能。

例如，在语音识别中，给定一段“厨房里食油用完了”的语音，有可能会输出“厨房里食油用完了”和“厨房里石油用完了”这两个读音完全一样的文本序列。如果语言模型判断出前者的概率大于后者的概率，我们就可以根据相同读音的语音输出“厨房里食油用完了”的文本序列。

在机器翻译中，如果对英文“you go first”逐词翻译成中文的话，可能得到“你走先”“你先走”等排列方式的文本序列。如果语言模型判断出“你先走”的概率大于其他排列方式的文本序列的概率，我们就可以把“you go first”翻译成“你先走”。

Word2vec主要分为两种模式：
1. 基于Hierarchical Softmax的CBOW模型：训练输入某个词的上下文的词向量，输出为该词的词向量。
2. 基于Negative Sampling的CBOW模型：输入一个词的词向量，输出上下文的词向量。


简单理解就是：
1. 如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』。
2. 如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』。

#### 3.3.1 Skip-gram
首先讨论最简单的情形：

上面说到， y 是 x 的上下文，所以 y只取上下文里一个词语的时候，语言模型就变成：

> 用当前词 x 预测它的下一个词 y。

但如上面所说，一般的数学模型只接受数值型输入，这里的 x 该怎么表示呢？ 显然不能用 Word2vec，因为这是我们训练完模型的产物，现在我们想要的是 x 的一个原始输入形式。

所以这里我们考虑用one-hot encoder进行输入。

我们来看看Skip-gram的网络结构：
![image](https://pic4.zhimg.com/80/v2-a1a73c063b32036429fbd8f1ef59034b_hd.jpg)

其中x 就是上面提到的 one-hot encoder形式的输入，y是在这V个词上输出的概率，我们希望跟真实的 y 的 one-hot encoder 一样。


当模型训练完后，最后得到的其实是神经网络的权重，比如现在输入一个 x 的 one-hot encoder: [1,0,0,…,0]，对应我们想要表达的任意一个词语比如说『吴彦祖』，则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，这些权重的个数，跟隐含层节点数是一致的，从而这些权重组成一个向量 vx 来表示x，而因为每个词语的 one-hot encoder 里面 1 的位置是不同的，所以，这个**向量 vx** 就可以用来唯一表示 x。

所以其实，我们把输出节点置成 [1,0,0,…,0]，它也能表示『吴彦祖』这个单词，但是激活的是隐含层到输出层的权重，这些权重的个数，跟隐含层一样，也可以组成**一个向量 vy**，跟上面提到的 vx 维度一样，并且可以看做是词语『吴彦祖』的另一种词向量。

那么当 y 有多个词时，这个推理过程又是什么样的呢？

这里给出一份[阅读资料](https://arxiv.org/abs/1411.2738)供同学们自己去思考。

#### 3.3.2 CBOW
跟 Skip-gram 相似，只不过:
Skip-gram 是预测一个词的上下文，而 CBOW 是用上下文预测这个词。

网络结构如下：
![image](https://pic3.zhimg.com/80/v2-d1ca2547dfb91bf6a26c60782a26aa02_hd.jpg)

 跟Skip-gram 的模型并联不同，这里是输入变成了多个单词，所以要对输入处理下（一般是求和然后平均），输出的 cost function 不变，在此依然不展开，依然建议同学们阅读上文的[参考资料](https://arxiv.org/abs/1411.2738)。

## 4 任务实施

### 4.1 实施思路
上面讲了这么多理论细节，其实在真正应用的时候，只需要调用 Gensim （一个 Python 第三方库）的接口就可以。

但对理论的探究仍然有必要，你能更好地知道参数的意义、模型结果受哪些因素影响，以及举一反三地应用到其他问题当中，甚至更改源码以实现自己定制化的需求。

本次任务我们将使用 Gensim 和 jieba 这两个库，来完成对《倚天屠龙记》中的相似词挖掘。

### 4.2 实施步骤


#### 步骤1：读入数据文件

```
# coding: utf-8
import io
import sys
import jieba

file_name = r'..\Word2vec\data'
cut_file = r'..\Word2vec\data_cut.txt'
```
原文部分内容如下：

![image](https://s2.ax1x.com/2019/10/16/KFfsaj.png)
#### 步骤2：数据预处理
用结巴分词，以及使用简单的空格替换过滤掉邮件中的非中文字符；

```
# 此函数作用是对初始语料进行分词处理后，作为训练模型的语料
def cut_txt(old_file, cut_file):
    print ("begin")
    try:
        # read file context
        fi = io.open(old_file, 'r', encoding='utf-8')
        text = fi.read()  # 获取文本内容

        # cut word
        new_text = jieba.cut(text, cut_all=False)  # 精确模式
        str_out = ' '.join(new_text).replace('，', '').replace('。', '').replace('？', '').replace('！', '') \
        .replace('“', '').replace('”', '').replace('：', '').replace('…', '').replace('（', '').replace('）', '') \
        .replace('—', '').replace('《', '').replace('》', '').replace('、', '').replace('‘', '') \
        .replace('’', '')     # 去掉标点符号

        # write to cut_file
        fo = io.open(cut_file, 'w', encoding='utf-8')
        fo.write(str_out)
    except BaseException as e:  # 因BaseException是所有错误的基类，用它可以获得所有错误类型
        print(Exception, ":", e)    # 追踪错误详细信息
    print  ("end")
```
处理以后的文本为：

![image](https://s2.ax1x.com/2019/10/16/KFf2R0.png)
#### 步骤3：训练模型，生成词向量

```
import sys
import logging
from gensim.models import word2vec


train_file_name = r'..t\Word2vec\datat_cut.txt'
save_model_file = r"..\Word2vec\倚天屠龙记.model"

def model_train(train_file_name, save_model_file):  # model_file_name为训练语料的路径,save_model为保存模型名
    print("begin")
    try:
        # 模型训练，生成词向量
        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
        sentences = word2vec.Text8Corpus(train_file_name)  # 加载语料
        model = word2vec.Word2Vec(sentences, size=200)  # 训练skip-gram模型; 默认window=5
        model.save(save_model_file)
        model.wv.save_word2vec_format(save_model_file + ".bin", binary=True)   # 以二进制类型保存模型以便重用
    except BaseException as e:  # 因BaseException是所有错误的基类，用它可以获得所有错误类型
        print(Exception, ":", e)    # 追踪错误详细信息
    print("end")
```

#### 步骤4：测试模型
我们的测试集中文件名低于1000的为正常邮件：

```
def word2vec_test():
    print ('word2vec_test begin.')
    try:
        # 加载日志输出配置
        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

        # 加载文件切词
        print ('加载文件:%s 切词后存放为:%s.' % (file_name, train_file_name))
        if not os.path.exists(file_name):    # 判断文件是否存在，参考：https://www.cnblogs.com/jhao/p/7243043.html
            print ('加载文件切词失败。')
            exit(0)
        else:
            cut_txt(file_name, train_file_name)  # 须注意文件必须先另存为utf-8编码格式

        # 训练模型
        print( '从文件:%s 训练模型存放在: %s' % (train_file_name, model_file))
        if not os.path.exists(model_file):     # 判断文件是否存在
            model_train(train_file_name, model_file)
        else:
            print('此训练模型已经存在，不用再次训练')

        # 加载已训练好的模型
        print ('从文件:%s 中加载模型' % model_file)
        # model_1 = gensim.models.KeyedVectors.load_word2vec_format(model_file_bin, binary=True)
        model_1 = word2vec.Word2Vec.load(model_file)

        # 计算两个词的相似度/相关程度
        y1 = model_1.similarity(u"赵敏", u"韦一笑")
        print ( u"赵敏和韦一笑的相似度为: %g" % y1)
        y2 = model_1.similarity ( u"赵敏", u"赵敏" )
        print ( u"赵敏和赵敏的相似度为: %g" % y2 )
        y3 = model_1.similarity ( u"赵敏", u"周芷若" )
        print ( u"赵敏和周芷若的相似度为: %g" % y3 )
        print ("-------------------------------\n")

        # 计算某个词的相关词列表
        y2 = model_1.most_similar(u"张三丰", topn=20)  # 20个最相关的
        print(u"和张三丰最相关的词有:\n")
        for item in y2:
            print ("%s: %g" % (item[0], item[1]))
        print("-------------------------------\n")
    except Exception as e:
            print( "Exception", e)
    print ('word2vec_test end.')

if __name__ == "__main__":
    word2vec_test()
```
最终结果如下：

![image](https://s2.ax1x.com/2019/10/16/KFfTo9.png)

看得出来，赵敏和自己的相似度为1；赵敏和周芷若的相似度0.98277，比较接近；赵敏和韦一笑的相似度为0.80652，远不如赵敏和周芷若相似；最后，查找与张三丰最相似的词，直觉上来看，排前6的都有一定道理，灭绝师太和他都是一代掌门，也是宗师，最相似；排到2到6的都是他的徒弟和徒孙。




## 5 任务拓展
Word2Vec 参数：
```
class gensim.models.word2vec.Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)
```
　主要参数介绍如下：
1. sentences：我们要分析的语料，可以是一个列表，或者从文件中遍历读出（word2vec.LineSentence(filename) ）。
2. size：词向量的维度，默认值是100。这个维度的取值一般与我们的语料的大小相关，如果是不大的语料，比如小于100M的文本语料，则使用默认值一般就可以了。如果是超大的语料，建议增大维度。
3. window：即词向量上下文最大距离，window越大，则和某一词较远的词也会产生上下文关系。默认值为5，在实际使用中，可以根据实际的需求来动态调整这个window的大小。

    如果是小语料则这个值可以设的更小。对于一般的语料这个值推荐在[5；10]之间。

4. sg：即我们的word2vec两个模型的选择了。如果是0， 则是CBOW模型；是1则是Skip-Gram模型；默认是0即CBOW模型。

5. hs：即我们的word2vec两个解法的选择了。如果是0， 则是Negative Sampling；是1的话并且负采样个数negative大于0， 则是Hierarchical Softmax。默认是0即Negative Sampling。
6. negative：即使用Negative Sampling时负采样的个数，默认是5。推荐在[3,10]之间。这个参数在我们的算法原理篇中标记为neg。
7. cbow_mean：仅用于CBOW在做投影的时候，为0，则算法中的xw为上下文的词向量之和，为1则为上下文的词向量的平均值。在我们的原理篇中，是按照词向量的平均值来描述的。个人比较喜欢用平均值来表示xw,默认值也是1,不推荐修改默认值。
8. min_count：需要计算词向量的最小词频。这个值可以去掉一些很生僻的低频词，默认是5。如果是小语料，可以调低这个值。
9. iter：随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值。
10. alpha：在随机梯度下降法中迭代的初始步长。算法原理篇中标记为η，默认是0.025。
11. min_alpha: 由于算法支持在迭代的过程中逐渐减小步长，min_alpha给出了最小的迭代步。

## 6 任务实训


### 6.1 实训目的
1. 掌握数据预处理的方法
2. 掌握通过词向量训练模型

### 6.2 实训内容
本章的任务在进行分词处理的时候，仅是使用了Python的replace函数。

根据前几章的学习，相信你们已经对如何使用停用词表对数据进行过滤有一定的了解了，那么就利用自定义或者下载的停用词表对数据再进行一次处理吧。并且在训练模型的过程中使用CBOW 模型进行训练，看看结果与现在的有何不同之处。


