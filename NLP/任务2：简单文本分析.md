[TOC]
# 任务2：简单文本分析

## 1 任务目标

1. 掌握基本的文本处理技能
1. 学会使用python的nltk库对中英文字符串进行处理

## 2 任务描述

文本分析是指对文本的表示及其特征项的选取，它把从文本中抽取出的特征词进行量化来表示文本信息。将它们从一个无结构的原始文本转化为结构化的计算机可以识别处理的信息，即对文本进行科学的抽象，建立它的数学模型，用以描述和代替文本。使计算机能够通过对这种模型的计算和操作来实现对文本的识别。由于文本是非结构化的数据,要想从大量的文本中挖掘有用的信息就必须首先将文本转化为可处理的结构化形式。

那么我们要如何进行文本分析呢？本章就来介绍一下如何用nltk导入、过滤以及对文本中的词组进行分析，以及一些nltk用于自然语言处理的初步手段。
## 3 知识准备

### 3.1 NLTK库
#### 3.1.1 NLTK库介绍
在上一章中我们知道NLTK库中自带语料库，和词性分类库。除此之外，它还可以被用于用于分类，标记化，词干分析，标记，解析和语义推等不同的领域，NLTK被称为“使用Python进行计算语言学教学和研究的绝佳工具”，今天我们就来学习一下这套自然语言处理工具集吧。
#### 3.1.1 NLTK安装和使用
1. 在Windows和Linux环境下都可以直接通过在命令行输入如下命令进行安装：

```
pip install nltk
```

2. 打开Python解释器或者编译器直接导入就可以使用：

```
import nltk
```

#### 3.1.2 NLTK的语料库
- 我们可以通过**download函数**下载相关的套件：

```
import nltk

# 安装nltk相关套件
nltk.download()
```
输入上述代码后会出现如下界面：
![image](https://s2.ax1x.com/2019/10/13/ux3JCd.png)
我们可以看到下载器上Collections 选项卡显示软件包如何被打包分组我们选择book 标记所在行进行下载，就可以获取本任务所需部分数据。这些数据包括约30 个压缩文件，需要100MB 硬盘空间。

- 我们可以这样**加载其中的book数据**：

```
from nltk.book import *
```
这条语句是说“从NLTK 的book 模块加载所有的东西”。 这个book 模块包含你阅读本章所需的所有数据。在输出欢迎信息之后，将会加载几本书的文本（这将需要几秒钟）。
![image](https://s2.ax1x.com/2019/10/13/ux3xaD.png)

#### 3.1.3 NLTK对自带语料库的处理
1. 函数concordance搜索指定内容
    - 除了阅读文本之外，还有很多方法可以用来研究文本内容。词语索引视角显示一个指定单词的每一次出现，连同一些上下文一起显示。
    - 我们可以使用**函数concordance**搜索指定内容。
    - 下面我们输入text后面跟一个点，再输入函数名concordance来调用函数，然后将需要检索的单词放在括号里实现单词检索。
    - 比方说我们想要在text3: The book of Genesis 这本书当中找到「人们活多久」这个问题的解答，于是可以搜寻lived 这个单字出现在文本的哪些句子中。
    ：
    
    ```
    # 关键字检索
    text3.concordance("lived")
    ```
    ![image](https://s2.ax1x.com/2019/10/13/ux8UJJ.png)
    虽然每一行呈现的字数有限，但是可以透过lived的上下文判断语意：Adam 大约活了130 年、Seth 活了150年、Enos 活了90 年…等。
    另外，输入的关键字词与搜寻结果都有「不分英文大小写」的特性。
    ![image](https://s2.ax1x.com/2019/10/13/ux82JH.png)

2. 使用**similar函数**寻找近似单词
    - 根据某词的上下文，找到类似结构，就认定他们为近似字。假设我们现在要在text1 里找monstrous 字词，而monstrous 会出现在the ___ pictures 以及a ___ size 这样的结构当中，透过这个方法去比对，一旦以下字词( true、 contemptible 、 christian ) 会在text1文本出现在一样的结构中，就认定他们为近似字。
    - 我们可以使用**similar函数**寻找近似单词
    ```
    # 找到用法、意义与该单词相似的词  
    text1.similar("monstrous")
    
    # 找到用法、意义与该单词集合相似的词  
    text1.common_contexts(["monstrous","abundant"])
    ```
    ![image](https://s2.ax1x.com/2019/10/13/uxYks1.png)

#### 3.1.4 词汇多样性
我们可以透过计算「相异字词长度/总字词长度」的值，去比较不同文本之间涵盖词汇的丰富程度。

以text4 为例，透过set(text4) ，可以获得text4 文本所有的相异字词，像是1812 、1815 、Amendment 、Abandonment 、Above 、Accept 、Americans …等， len(set(text4))则为相异字词长( 9,754 ) ， len(text4) 为总字词长度( 145,735 )，两者相除后计算出来的值为0.0623。实际执行可以发现排序后的相异字词，有很多皆为年份，相异字词的内容多和法律制定有关。

```
# 相异字词
set(text4)

# 相异字词排序
sorted(set(text4))

# 定义词汇多样性的函数
def lexical_diversity(text):
return len(set(text)) / len(text)

lexical_diversity(text4)
```
![image](https://s2.ax1x.com/2019/10/13/uxYaWQ.png)

#### 3.1.5 文本数据可视化
1. 使用book.dispersion_plot()生成词汇分布图

    延续上面跟制定法律有关的 text4 文本，如果我们想要检索「制定美国民主」相关的字词出现在整篇的频率，也就是特定字词出现在文本的前、中、后的状况：
        
    ```
    # 构造文本的词汇分布图
    text4.dispersion_plot(["citizens", "democracy", "freedom", "duties", "America", "liberty", "constitution"])
    ```
![image](https://s2.ax1x.com/2019/10/13/uxtmmq.png)
数据可视化以后，很容易可以发现一些脉络有迹可循，比如说citizens虽然全篇都出现，但主要集中在前半段， America 在后半段才大量出现。其实这蛮接近我们对于法律的结构的认知，首先会先阐述公民与权利的大原则，最后才会奠基于大原则之上， America 会怎么做。

2. 使用搜索函数FreqDist()查询词汇分布频率
    1. 普通频率查找：
    
    ```
    fdist1=FreqDist(text1) # 查询文本text1中词汇分布情况
    
    fdist1.plot(50,cumulative=True) # 输出指定50个常用词累积频率图
    ```
    这样就可以输出text1中50个常用词的累积频率图，这些词占了所有标识的将近一半。
    ![image](https://s2.ax1x.com/2019/10/13/uxN7GD.png)
    
    2. 利用函数fdist1.hapaxes()进行低频词查找
    普通频率的分布我们现在掌握了，那么如何查询低频词汇呢？我们利用函数fdist1.hapaxes()低频词出现1次查找。
    
    ```
    # hapaxes()函数 返回低频词
	print(fdist1.hapaxes())
    ```
    ![image](https://s2.ax1x.com/2019/10/13/uxUByd.png)

3. 细粒度选择词
    高频词和低频词提取出的信息量有限，研究文本中的长词提取出更多的信息量。采用集合论的一些符号：P性质，V词汇，w单个词符，P(w)当且仅当w词符长度大于15。表示为：{w | w ∈ V & P(w)}
    ```
    V = set ( text1 )
    	longwords=[w for w in V if len(w) > 15] #[]中的模式是：[w for w in text if condition]，其中condition是一个python的判断条件，此处表示的是在V中长度大于15的单词w,
    	sorted(longwords)
    ```

![image](https://s2.ax1x.com/2019/10/13/uxaPk6.png)



## 4. 任务实施
根据前面的学习内容，我们已经了解了一些用nltk库来处理文本的方法，那么本章就以某部描写秦国时期的小说为例，来动手实现文本分析吧。
### 4.1 实施思路
下载本章资料的《大秦帝国.zip》文件，里面按照语料大小包含5个文件，分别是30852词的p1.txt、70046词的p2.txt、111970词的p3.txt、1182769词的p5.txt、419275词的p10.txt。

本章节选大秦帝国第一部673167字的dqdg.txt打开Python解释器，导入nltk，并对其进行文本分析。


### 4.2 实施步骤


#### 步骤1：读入文件

```
# 以读写模式[r+]打开文件dqdg.txt
	with open(r"dqdg.txt","r+") as f:    			    str=f.read()
```


#### 步骤2：查看用字量
查看大秦帝国第一部总共有多大的用字量，即不重复词和符合的尺寸：

```
len(set(str))
	len(str)/len(set(str))
```
实验可知用了4053个尺寸的词汇表，平均每个词使用了166次。
![image](https://s2.ax1x.com/2019/10/13/uxwnFP.png)



#### 步骤3：查看常用词
常用词分布如何呢？既然是大秦帝国，那么秦字使用了多少次呢？

```
str.count("秦")
	str.count("大秦")
	str.count("国")
```
![image](https://s2.ax1x.com/2019/10/13/uxwwSU.png)

#### 步骤4：词汇离散分布
我们再来分析一下词汇离散分布吧：

```
import nltk

# 中文显示坐标
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 5))
plt.rcParams['font.sans-serif'] = 'SimHei'

with open(r"dqdg.txt","r+") as f: # 以读写模式[r+]打开文件dqdg.txt
    str=f.read()

tokens = nltk.word_tokenize(str)   # 标记化
mytext = nltk.Text(tokens)         # 将文本转换为NLTK Text对象
mytext.dispersion_plot(["秦","国","商鞅","王"])

```
![image](https://s2.ax1x.com/2019/10/13/uxw6T1.md.png)
可以知道，秦用词3538次，大秦用了14次，因为讲的各国之间的事情，国也是高频词6536次。如上所述大秦帝国第一部总词汇表673167。

#### 步骤5：查看累积分布

```
fdist=FreqDist(str)
	fdist.plot()
```

![image](https://s2.ax1x.com/2019/10/13/uxwT0A.png)

## 5 任务拓展
词语搭配和双连词：
- 搭配：不经常在一起出现的词序列，如red wine是搭配而the wine就不是。另一个特点就是词不能被类似的词置换，如maroon wine（栗色酒）就不行
- bigrams（）：获取搭配，提前文本词汇的双连词


```
from nltk import bigrams
from collections import Counter
b = bigrams('This is a test')
print(Counter(b))
```
![image](https://s2.ax1x.com/2019/10/13/ux0p0s.png)



## 6 任务实训


### 6.1 实训目的
- 学会Python的[]模式
- 学会使用nltk分析文本


### 6.2 实训内容
实训内容：
1. 回到python的决策与控制
：
    - 按照我们意愿去处理关键特征
根据前面提到的[]模式，完成下列例子：
        - 查询词频在[0--100]的词有多少？
        - 查询词频在[100--1000]的词有多少？
        - 查询词频在[1000-5000]的词有多少？
        - 查询词频在[5000--]的词有多少？
- 答案：
    - len([w for w in V.values() if w<100])
    - len([w for w in V.values() if w>100 and w<1000])
    - len([w for w in V.values() if w>1000 and w<5000])
    - len([w for w in V.values() if w>5000])

2. 分析五月天的歌词sing.txt并且生成词汇频率分布图
答案：

```
答案：
# 呈現中文
	import matplotlib.pyplot as plt
	plt.figure(figsize=(10, 5)) 
	plt.rcParams['font.sans-serif'] = 'SimHei'# 詞彙分佈圖
	single.dispersion_plot(["love","戀愛","ing","happy"])

```

![image](https://s2.ax1x.com/2019/10/13/ux081O.png)
