
[TOC]
# 任务5：朴素贝叶斯

## 1 任务目标

1. 了解分类器的定义
2. 掌握贝叶斯的数学原理和数学公式
3. 掌握用Python编写贝叶斯分类器



## 2 任务描述

贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。而朴素朴素贝叶斯分类是贝叶斯分类中最简单，也是常见的一种分类方法。本章中我们试着来分析一个婚恋问题：
如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？
从而加深我们对贝叶斯定理的理解学习。


## 3 知识准备


### 3.1 分类问题综述
对于分类问题，其实谁都不会陌生，日常生活中我们每天都进行着分类过程。例如，当你看到一个人，你的脑子下意识判断他是学生还是社会上的人；你可能经常会走在路上对身旁的朋友说“这个人一看就很有钱、”之类的话，其实这就是一种分类操作。

既然是贝叶斯分类算法，那么分类的数学描述又是什么呢？
从数学角度来说，分类问题可做如下定义：
已知集合
```math
C = y_1, y_2, ...y_n
```
和
```math
I = x_1, x_2, ...x_n
```
我们可以确定映射规则
```math
y=f()
```
使得任意

```math
x_i \in I
```
有且仅有一个
```math
y_i \in C
```
使得

```math
y_i \in f(x_i)
```
成立。
其中C叫做类别集合，其中每一个元素是一个类别，而I叫做项集合（特征集合），其中每一个元素是一个待分类项，f叫做分类器。分类算法的任务就是构造分类器f。

分类算法的内容是要求给定特征，让我们得出类别，这也是所有分类问题的关键。那么如何由指定特征，得到我们最终的类别，也是我们下面要讲的，每一个不同的分类算法，对应着不同的核心思想。


### 3.2 朴素贝叶斯分类
#### 3.2.1 贝叶斯核心算法
那么既然是朴素贝叶斯分类算法，它的核心算法又是什么呢？
是下面这个贝叶斯公式：

```math
P(B|A) = \frac{P(A|B)P(B)}{P(A)}
```
换个表达式就会明朗很多，如下：

```math
P(\text{类别}|\text{特征}) = \frac{P(\text{特征}|\text{类别})P(\text{类别})}{P(\text{特征})}
```
我们最终求的p(类别|特征)即可！就相当于完成了我们的任务。

#### 3.2.2 朴素贝叶斯分类的优缺点
- 优点：
    1. 算法逻辑简单,易于实现
    2. 分类过程中时空开销小

- 缺点：
    - 理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。
    - 但是实际上并非总是如此，这是因为朴素贝叶斯模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的。
    - 在属性个数比较多或者属性之间相关性较大时，分类效果不好。
    - 而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。

## 4 任务实施
### 4.1 实施思路
现在给我们的问题是，如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？

帅？ | 性格好？| 身高？| 上进？| 嫁不嫁？ 
-----|---------|-------|-------|-------|
帅   | 不好    | 矮    |不上进 |不嫁   | 
不帅 | 好      | 矮    |上进   |不嫁   |
帅   | 好      | 矮    |上进   |嫁     | 
不帅 | 好      | 高    |上进   |嫁     |
帅   | 不好    | 矮    |上进   |不嫁   | 
帅   | 不好    | 矮    |上进   |不嫁   |
帅   | 好      | 高    |不上进 |嫁     |  
不帅 | 好      | 中等  |上进   |嫁     |
帅   | 好      | 中等  |上进   |嫁     | 
不帅 | 不好    | 高    |上进   |嫁     |
帅   | 好      | 矮    |不上进 |不嫁   |
帅   | 好      | 矮    |不上进 |不嫁   |

这是一个典型的分类问题，转为数学问题就是比较p(嫁|(不帅、性格不好、身高矮、不上进))与p(不嫁|(不帅、性格不好、身高矮、不上进))的概率，谁的概率大，我就能给出嫁或者不嫁的答案！

这里我们联系到朴素贝叶斯公式：

```math
P(\text{嫁}|\text{不帅、性格不好、身高矮、不上进}) =

\frac{P(\text{不帅、性格不好、身高矮、不上进}|\text{嫁})P(\text{嫁})}{P(\text{不帅、性格不好、身高矮、不上进})}
```
我们需要求
```math
P(\text{嫁}|\text{不帅、性格不好、身高矮、不上进})
```
不过这是我们不知道的，但是通过朴素贝叶斯公式可以转化为好求的三个量：

```math
P(\text{不帅、性格不好、身高矮、不上进}|\text{嫁})
```
```math
P(\text{嫁})
```
```math
P(\text{不帅、性格不好、身高矮、不上进})

```

这样将待求的量转化为其它可求的值，这就相当于解决了我们的问题！

### 4.2 实施步骤
- 根据已知训练数据统计，我们就可以得出上述待求量的概率。
- 回忆一下上文中我们提到的公式，为什么该公式成立呢？这个等式成立的条件不是需要特征之间相互独立吗？
- 对的！这也就是为什么朴素贝叶斯分类有朴素一词的来源，朴素贝叶斯算法是假设各个特征之间相互独立，那么这个等式就成立了！
- 我们就来分析一下为什么需要假设特征之间相互独立：
1. 我们这么想，假如没有这个假设，那么我们对右边这些概率的估计其实是不可做的，这么说，我们这个例子有4个特征，其中帅包括{帅，不帅}，性格包括{不好，好，爆好}，身高包括{高，矮，中}，上进包括{不上进，上进}，那么四个特征的联合概率分布总共是4维空间，总个数为2*3*3*2=36个。计算机扫描统计还可以，但是现实生活中，往往有非常多的特征，每一个特征的取值也是非常之多，那么通过统计来估计后面概率的值，变得几乎不可做，这也是为什么需要假设特征之间独立的原因。
2. 假如我们没有假设特征之间相互独立，那么我们统计的时候，就需要在整个特征空间中去找，比如统计
```math
P(\text{不帅、性格不好、身高矮、不上进})
```
我们就需要在嫁的条件下，去找四种特征全满足分别是不帅，性格不好，身高矮，不上进的人的个数，这样的话，由于数据的稀疏性，很容易统计到0的情况。 这样是不合适的。

根据上面俩个原因，朴素贝叶斯法对条件概率分布做了条件独立性的假设，由于这是一个较强的假设，朴素贝叶斯也由此得名！这一假设使得朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。

所以在假设特征互相独立的情况下，我们可以把上述式子拆成连乘的形式：
![image](https://pic3.zhimg.com/80/v2-e0abd30b1376c18c3dfd0d0bf4375c26_hd.png)
下面我们将一个一个的进行统计计算（在数据量很大的时候，根据中心极限定理，频率是等于概率的，这里只是一个例子，所以就进行统计即可）。

#### 步骤1：定义我们的数据集
根据给出的表格，我们可以用代码存储数据：

```
data = [
    {"handsome":"y", "temp":"n", "height":"high", "upward":"n", "class":"n" },
    {"handsome":"n", "temp":"y", "height":"short", "upward":"y", "class":"n"},
    {"handsome":"y", "temp":"y", "height":"short", "upward":"y", "class":"y" },
    {"handsome":"n", "temp":"y", "height":"high", "upward":"y", "class":"y" },
    {"handsome":"y", "temp":"n", "height":"short", "upward":"y", "class":"n" },
    {"handsome":"y", "temp":"n", "height":"short", "upward":"y", "class":"n"},
    {"handsome":"y", "temp":"y", "height":"high", "upward":"n", "class":"y" },
    {"handsome":"n", "temp":"y", "height":"mid", "upward":"y", "class":"y"},
    {"handsome":"y", "temp":"y", "height":"mid", "upward":"y", "class":"y"},
    {"handsome":"n", "temp":"n", "height":"high", "upward":"y", "class":"y"},
    {"handsome":"y", "temp":"y", "height":"short", "upward":"n", "class":"n"},
    {"handsome":"y", "temp":"y", "height":"short", "upward":"n", "class":"n" }]
    
import pandas as pd
# 读入数据
pd.DataFrame(data)
```


#### 步骤1： 先求分子

我们来一个个进行统计计算，第一个需要计算的概率是嫁的概率：
```math
P(\text{嫁})=?
```
首先我们整理训练数据中，嫁的样本数如下：

帅？ | 性格好？| 身高？| 上进？| 嫁不嫁？ 
-----|---------|-------|-------|-------|
帅   | 好      | 矮    |上进   |嫁     | 
不帅 | 好      | 高    |上进   |嫁     |
帅   | 好      | 高    |不上进 |嫁     |  
不帅 | 好      | 中等  |上进   |嫁     |
帅   | 好      | 中等  |上进   |嫁     | 
不帅 | 不好    | 高    |上进   |嫁     |

则：
```math
P(\text{嫁})=\frac{6}{12}=\frac{1}{2}
```
代码实现如下：
```
# 计算嫁与否的概率
def P(data,cls_val,cls_name="class"):
    count = 0.0
    for e in data:
        if e[cls_name] == cls_val:
            count += 1
    return count/len(data)

PY, PN = P(data,"y"), P(data, "n")
print("嫁的概率为：", PY)
print("不嫁的概率为：", PN)
```
![image](https://s2.ax1x.com/2019/10/15/KCaqYR.png)

那么再来看看：
```math
P(\text{不帅}|\text{嫁})=?
```
统计满足样本如下：
帅？ | 性格好？| 身高？| 上进？| 嫁不嫁？ 
-----|---------|-------|-------|-------|
不帅 | 好      | 高    |上进   |嫁     |
不帅 | 好      | 中等  |上进   |嫁     |
不帅 | 不好    | 高    |上进   |嫁     |
则：
```math
P(\text{不帅}|\text{嫁})=\frac{3}{6}=\frac{1}{2}
```
接下来还要计算一下：
```math
P(\text{性格不好}|\text{嫁})=?
```
统计满足样本如下：
帅？ | 性格好？| 身高？| 上进？| 嫁不嫁？ 
-----|---------|-------|-------|-------|
不帅 | 不好    | 高    |上进   |嫁     |
即：
```math
P(\text{性格不好}|\text{嫁})=\frac{1}{6}
```
接下来再计算一下：
```math
P(\text{矮}|\text{嫁})=?
```
统计满足样本如下：
帅？ | 性格好？| 身高？| 上进？| 嫁不嫁？ 
-----|---------|-------|-------|-------|
  帅 |   好    | 矮    |上进   |嫁     |
即：
```math
P(\text{矮}|\text{嫁})=\frac{1}{6}
```
最后需要统计的是：
```math
P(\text{不上进}|\text{嫁})=?
```
统计满足样本如下：
帅？ | 性格好？| 身高？| 上进？| 嫁不嫁？ 
-----|---------|-------|-------|-------|
  帅 |   好    | 高    |不上进 |嫁     |
即：
```math
P(\text{不上进}|\text{嫁})=\frac{1}{6}
```
上述计算条件概率的过程可以利用代码实现为：


```
# 定义函数计算条件概率
def PT(data,cls_val,attr_name,attr_val,cls_name="class"):
    count1 = 0.0
    count2 = 0.0
    for e in data:
        if e[cls_name] == cls_val:
            count1 += 1
            if e[attr_name] == attr_val:
                count2 += 1
    return count2/count1

print("不帅也嫁的概率为：",PT(data,"y", "handsome", "n"))
print("性格不好也嫁的概率为：",PT(data,"y", "temp", "n"))
print("长得不高也嫁的概率为：",PT(data,"y", "height", "short"))
print("没上进心也嫁的概率为：",PT(data,"y", "upward", "n"))
```
![image](https://s2.ax1x.com/2019/10/15/KCwMDO.png)

#### 步骤2：再求分母
```math
P(\text{不帅})=?
```
统计满足样本如下：
帅？ | 性格好？| 身高？| 上进？| 嫁不嫁？ 
-----|---------|-------|-------|-------|
不帅 | 好      | 矮    |上进   |不嫁   |
不帅 | 好      | 高    |上进   |嫁     |
不帅 | 好      | 中等  |上进   |嫁     |
不帅 | 不好    | 高    |上进   |嫁     |
即：
```math
P(\text{不帅})=\frac{1}{3}
```
再求性格不好的概率：
```math
P(\text{性格不好})=?
```
统计满足样本如下：
帅？ | 性格好？| 身高？| 上进？| 嫁不嫁？ 
-----|---------|-------|-------|-------|
帅   | 不好    | 矮    |不上进 |不嫁   | 
帅   | 不好    | 矮    |上进   |不嫁   | 
帅   | 不好    | 矮    |上进   |不嫁   |
不帅 | 不好    | 高    |上进   |嫁     |
即：
```math
P(\text{性格不好})=\frac{1}{3}
```
接着我们看看身高；
```math
P(\text{矮})=?
```
统计满足样本如下：
帅？ | 性格好？| 身高？| 上进？| 嫁不嫁？ 
帅   | 不好    | 矮    |不上进 |不嫁   | 
不帅 | 好      | 矮    |上进   |不嫁   |
帅   | 好      | 矮    |上进   |嫁     | 
帅   | 不好    | 矮    |上进   |不嫁   | 
帅   | 不好    | 矮    |上进   |不嫁   |
帅   | 好      | 矮    |不上进 |不嫁   |
帅   | 好      | 矮    |不上进 |不嫁   |
即：
```math
P(\text{矮})=\frac{7}{12}
```
最后我们看看上进心：
接着我们看看身高；
```math
P(\text{不上进})=?
```
统计满足样本如下：
帅？ | 性格好？| 身高？| 上进？| 嫁不嫁？ 
-----|---------|-------|-------|-------|
帅   | 不好    | 矮    |不上进 |不嫁   | 
帅   | 好      | 高    |不上进 |嫁     |  
帅   | 好      | 矮    |不上进 |不嫁   |
帅   | 好      | 矮    |不上进 |不嫁   |
即：
```math
P(\text{不上进})=\frac{1}{3}
```

到这里，要求p(不帅、性格不好、身高矮、不上进|嫁)的所需项全部求出来了，下面我带入进去即可：

```math
P(\text{嫁}|\text{不帅、性格不好、身高矮、不上进})
= (\frac{1}{2}*\frac{1}{6}*\frac{1}{6}*\frac{1}{6}*\frac{1}{2})/(\frac{1}{3}*\frac{1}{3}*\frac{7}{12}*\frac{1}{3})
```

上述计算条件概率的过程可以利用代码实现为：


```
# 计算单个属性的概率
def P_Attr(data,cls_val,cls_name):
    count = 0.0
    for e in data:
        if e[cls_name] == cls_val:
            count += 1
    return count/len(data)

print("不帅的概率为：",P_Attr(data, "n", "handsome"))
print("性格不好的概率为：",P_Attr(data,"n", "temp"))
print("长得不高的概率为：",P_Attr(data,"short", "height"))
print("没上进心的概率为：",P_Attr(data,"n", "upward"))
```
![image](https://s2.ax1x.com/2019/10/15/KCBz8A.png)

#### 步骤3：同理计算
根据前面的步骤，我们可以同理计算不嫁的概率：
```math
P(\text{不嫁}|\text{不帅、性格不好、身高矮、不上进})
```
最后我们可以发现：

```math
P(\text{不嫁}|\text{不帅、性格不好、身高矮、不上进})=(\frac{1}{2}*\frac{1}{6}*\frac{1}{2})/(\frac{1}{3}*\frac{1}{3}*\frac{7}{12}*\frac{1}{3})
```
很显然：


```math
P(\text{不嫁}|\text{不帅、性格不好、身高矮、不上进})>P(\text{嫁}|\text{不帅、性格不好、身高矮、不上进})
```
所以我们根据朴素贝叶斯算法可以给这个女生答案，是不嫁！！！！



## 5 任务拓展

### 支持向量机：
- 在机器学习中，支持向量机（英语：support vector machine，常简称为SVM，又名支持向量网络）是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法创建一个将新的实例分配给两个类别之一的模型，使其成为非概率二元线性分类器。SVM模型是将实例表示为空间中的点，这样映射就使得单独类别的实例被尽可能宽的明显的间隔分开。然后，将新的实例映射到同一空间，并基于它们落在间隔的哪一侧来预测所属类别。
- 除了进行线性分类之外，SVM还可以使用所谓的核技巧有效地进行非线性分类，将其输入隐式映射到高维特征空间中。
- 当数据未被标记时，不能进行监督式学习，需要用非监督式学习，它会尝试找出数据到簇的自然聚类，并将新数据映射到这些已形成的簇。将支持向量机改进的聚类算法被称为支持向量聚类，当数据未被标记或者仅一些数据被标记时，支持向量聚类经常在工业应用中用作分类步骤的预处理。
![image](https://s2.ax1x.com/2019/10/15/KCYpjg.png)

## 6 任务实训


### 6.1 实训目的
- 掌握Python编写朴素贝叶斯代码
- 学会使用pandas读取数据处理数据


### 6.2 实训内容
根据如下表格，判断天气是如何影响同学们出去打篮球的。
![image](https://s2.ax1x.com/2019/10/15/KCrRfJ.png)
答案：https://computational-communication.com/python-bayes/
