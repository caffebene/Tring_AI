# 任务3：中文文本预处理
## 任务目标
1.	学习并且掌握中文文本预处理库jieba的使用，熟悉三种分词模式
2.	了解jieba库的算法原理
3.	学习使用词云生成库wordcloud
## 任务描述
词云，又称文字云，是文本数据的视觉表示，由词汇组成类似云的彩色图形，用于展示大量文本数据。通常用于描述网站上的关键字元数据（标签），或可视化自由格式文本。 每个词的重要性以字体大小或颜色显示。 词云的作用：
- 快速感知最突出的文字
- 快速定位按字母顺序排列的文字中相对突出的部分

词云的本质是点图，是在相应坐标点绘制具有特定样式的文字的结果。词云适合在需要对比大量文本 的场景下使用。
下面我们就试着来创建属于自己的词云吧！
## 知识准备
### 1. Jieba库
#### 1.1	分词
##### 1.1.1	分词介绍
NLP的底层任务由易到难大致可以分为词法分析、句法分析和语义分析。分词是词法分析（还包括词性标注和命名实体识别）中最基本的任务，可以说既简单又复杂。说简单是因为分词的算法研究已经很成熟了，大部分的准确率都可以达到95%以上，说复杂是因为剩下的5%很难有突破，主要因为三点：
- 粒度，不同应用对粒度的要求不一样，比如“苹果手机”可以是一个词也可以是两个词
- 歧义，比如上一个任务中提到的“下雨天留人天留我不留”
- 未登录词，比如“skrrr”、“打call”等新兴词语
##### 1.1.2. jieba算法详解
###### 1.1.2.1 最大匹配分词算法
jieba分词主要是基于统计词典，首先构造一个前缀词典trie。在计算机科学中，trie，又称前缀树或字典树，是一种有序树，用于保存关联数组，其中的键通常是字符串。Trie树由词的公共前缀构成节点，降低了存储空间的同时提升查找效率。最大匹配分词将句子与Trie树进行匹配，在匹配到根结点时由下一个字重新开始进行查找。比如正向（从左至右）匹配“他说的确实在理”，得出的结果为“他／说／的确／实在／理”。如果进行反向最大匹配，则为“他／说／的／确实／在理”。
	可见，词典分词虽然可以在O(n)时间对句子进行分词，但是分词性能的好坏完全依赖于词库。
![image.png](https://i.loli.net/2019/10/14/W7aCnbSh2zgRMEV.png)
###### 1.1.2.2 最短路径分词算法
最短路径分词算法首先将一句话中的所有词匹配出来，构成词图（有向无环图DAG），最终的DAG是以{k : [k , j , ..] , m : [m , p , q] , ...}的字典结构存储，其中k和m为词在文本sentence中的位置，k对应的列表存放的是文本中以k开始且词sentence[k: j + 1]在前缀词典中的 以k开始j结尾的词的列表，即列表存放的是sentence中以k开始的可能的词语的结束位置，这样通过查找前缀词典就可以得到词。之后寻找从起始点到终点的最短路径作为最佳组合方式：
![](https://i.bmp.ovh/imgs/2019/10/e38f9c14e05fd854.png)
###### 1.1.2.3. 动态规划最大概率路径
在得到所有可能的切分方式构成的有向无环图后，我们发现从起点到终点存在多条路径，多条路径也就意味着存在多种分词结果，有向无环图DAG的每个节点，都是带权的，对于在前缀词典里面的词语，其权重就是它的词频；我们想要求得route = (w1,w2,w3,...,wn)，使得 最大。
如果需要使用动态规划求解，需要满足两个条件：
1.	重复子问题
2.	最优子结构

我们来分析一下最大概率路径问题，是否满足动态规划的两个条件。
1.	重复子问题

对于节点wi和其可能存在的多个后继节点Wj和Wk：（阴影是要的）
```
任意通过Wi到达Wj的路径的权重 = 该路径通过Wi的路径权重 + Wj的权重，也即{Ri -> j} = {Ri + weight(j)} 
```
```
任意通过Wi到达Wk的路径的权重 = 该路径通过Wi的路径权重 + Wk的权重，也即{Ri -> k} = {Ri + weight(k)}
```
即对于拥有公共前驱节点Wi的节点Wj和Wk，需要重复计算达到Wi的路径的概率。
2.	最优子结构

对于整个句子的最优路径Rmax和一个末端节点Wx，对于其可能存在的多个前驱Wi，Wj，Wk...，设到达Wi，Wj，Wk的最大路径分别是Rmaxi，Rmaxj，	Rmaxk，有
```
Rmax = max(Rmaxi,Rmaxj,Rmaxk,...)+weight(Wx)
```
于是，问题转化为，求解Rmaxi，Rmaxj，Rmaxk，...等，
组成了最优子结构，子结构里面的最优解是全局的最优解的一部分。

状态转移方程为，
```
Rmax = max{(Rmaxi,Rmaxj,Rmaxk,...) + weight(Wx)}
```
于是这就转化成了一个自底向上的动态规划问题，它从sentence的最后一个字（N-1）开始倒序遍历sentence的每个字（idx）的方式，计算子句sentence[idx ~ N-1]的概率对数得分。然后将概率对数得分最高的情况以（概率对数，词语最后一个位置）这样的元组保存在route中。
#### 1.2.	Jieba的三种分词模式
- 精确模式，试图将句子最精确地切开，适合文本分析；
- 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；
- 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。
jieba.cut的默认参数只有三个,jieba源码如下
```
cut(self, sentence, cut_all=False, HMM=True)
```
分别为:输入文本 是否为全模式分词 与是否开启HMM进行中文分词
```
jieba.cut_for_search()：
```
仅一个参数，为分词的字符串，该方法适合用于搜索引擎构造倒排索引的分词，粒度比较细。
```
# 三种分词模式与一个参数
import jieba

seg_list = jieba.cut("我来到北京清华大学", cut_all=True, HMM=False)
print("Full Mode: " + "/ ".join(seg_list))  # 全模式

seg_list = jieba.cut("我来到北京清华大学", cut_all=False, HMM=True)
print("Default Mode: " + "/ ".join(seg_list))  # 默认模式

seg_list = jieba.cut("他来到了网易杭研大厦", HMM=False)
print(", ".join(seg_list))

seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造", HMM=False)  # 搜索引擎模式
print(", ".join(seg_list))
```
运行结果：
![](https://i.bmp.ovh/imgs/2019/10/c926e31418ac6fa9.png)
#### 1.3.	jieba分词器添加自定义词典
jieba分词器还有一个方便的地方是开发者可以指定自己的自定义词典，以便包含词库中没有的词，虽然jieba分词有新词识别能力，但是自行添加新词可以保证更高的正确率。
使用的函数为：
```
jieba.load_userdict(filename) # filename为自定义词典的路径
```
在使用的时候，词典的格式和jieba分词器本身的分词器中的词典格式必须保持一致，一个词占一行，每一行分成三部分，一部分为词语，一部分为词频，最后为词性（可以省略），用空格隔开（此处强调）。

下面其中userdict.txt中的内容为自定义添加的词典，而前半部分为没有添加字典之后对text文档进行分词得到的结果，后半部分为添加字典之后分词的效果。
![](https://i.bmp.ovh/imgs/2019/10/36056e0094816ea3.png)
```
import jieba
import jieba.posseg as pseg
test_sent = (
"李小福是创新办主任也是云计算方面的专家; 什么是八一双鹿\n"
"例如我输入一个带“韩玉赏鉴”的标题，在自定义词库中也增加了此词为N类\n"
"「台中」正確應該不會被切開。mac上可分出「石墨烯」；此時又可以分出來凱特琳了。"
)

# 没有添加字典对text文档进行分词
words = jieba.cut(test_sent)
print('/'.join(words))

print("="*40)

# 添加自定义字典
jieba.load_userdict(r"C:\Users\15845\nlp_hst\ch2\res\userdict.txt")

# 添加字典之后分词
test_sent = (
"李小福是创新办主任也是云计算方面的专家; 什么是八一双鹿\n"
"例如我输入一个带“韩玉赏鉴”的标题，在自定义词库中也增加了此词为N类\n"
"「台中」正確應該不會被切開。mac上可分出「石墨烯」；此時又可以分出來凱特琳了。"
)
words = jieba.cut(test_sent)
print('/'.join(words))
```
![](https://i.bmp.ovh/imgs/2019/10/40ae53779541e58e.png)
#### 1.4	停用词词库
停用词是指在信息检索中，为节省存储空间和提高搜索效率，在处理自然语言数据（或文本）之前或之后会自动过滤掉某些字或词，这些字或词即被称为Stop Words（停用词）。这些停用词都是人工输入、非自动化生成的，生成后的停用词会形成一个停用词表。但是，并没有一个明确的停用词表能够适用于所有的工具。甚至有一些工具是明确地避免使用停用词来支持短语搜索的。

本任务中选用哈工大停用词表来进行实践演示。
	
首先我们选择一篇人工智能科普文章作为输入：
![](https://i.bmp.ovh/imgs/2019/10/931b7500575a8fd3.png)
```
from collections import Counter
import jieba

# 创建停用词list
def stopwordslist(filepath):
    stopwords = [line.strip () for line in open ( filepath, 'r' , encoding='UTF-8').readlines ()]
    return stopwords


# 对句子进行分词
def seg_sentence(sentence):
    sentence_seged = jieba.cut ( sentence.strip () )
    stopwords = stopwordslist ( r"C:\Users\15845\nlp_hst\ch2\res\stopword.txt" )  # 这里加载停用词的路径
    outstr = ''
    for word in sentence_seged:
        if word not in stopwords:
            if word != '\t':
                outstr += word
                outstr += " "
    return outstr


inputs = open ( r"C:\Users\15845\nlp_hst\ch2\res\ai.txt", 'r' , encoding='UTF-8')  # 加载要处理的文件的路径
outputs = open ( r"C:\Users\15845\nlp_hst\ch2\res\ai_after.txt", 'w' , encoding='UTF-8')  # 加载处理后的文件路径

for line in inputs:
    line_seg = seg_sentence ( line )  # 这里的返回值是字符串
    outputs.write ( line_seg )
outputs.close ()
inputs.close ()
# WordCount
with open ( r"C:\Users\15845\nlp_hst\ch2\res\ai_after.txt", 'r' , encoding='UTF-8') as fr:  # 读入已经去除停用词的文件
    data = jieba.cut ( fr.read () )
data = dict ( Counter ( data ) )

with open ( r"C:\Users\15845\nlp_hst\ch2\res\ai_count.txt", 'w' , encoding='UTF-8') as fw:  # 读入存储wordcount的文件路径
    for k, v in data.items ():
        fw.write ( '%s,%d\n' % (k, v) )
```
处理以后，得到去掉停用词的txt文件：
![](https://i.bmp.ovh/imgs/2019/10/c4e4c31e24a8048d.png)
并且计算出了词频：
![](https://i.bmp.ovh/imgs/2019/10/64f7f578da7ae1fb.png)

### 2.	Wordcloud库
#### 2.1	下载
```
pip install wordcloud
```
#### 2.2	使用
```
import worldcloud
```
#### 2.3	输入英文文本
```
# 导入wordcloud模块和matplotlib模块
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# 读取一个txt文件
text = open(r'C:\Users\15845\nlp_hst\ch2\res\english.txt','r').read()

# 生成词云
wordcloud = WordCloud(background_color='white',scale=1.5).generate(text)

# 显示词云图片
plt.imshow(wordcloud)
plt.axis('off')
plt.show()
```
![](https://i.bmp.ovh/imgs/2019/10/801964c6403cb72d.png)
## 3.	任务实施
### 实施思路
首先我们需要导入jieba库，接着读入中文文本文件，通过jieba分词，再生成中文词云。
#### 步骤
1. 读入中文文本
2. 结巴分词
3. 生成词云
```
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from scipy.misc import imread

# 读入背景图
mask = imread(r"C:\Users\15845\nlp_hst\ch2\res\heart.png")

import jieba

# 读入要处理的文本
text_from_file_with_apath = open(r'C:\Users\15845\nlp_hst\ch2\res\sunny.txt').read()

# jieba分词
wordlist_after_jieba = jieba.cut(text_from_file_with_apath, cut_all = True)
wl_space_split = " ".join(wordlist_after_jieba)

# 生成词云
my_wordcloud = WordCloud(font_path=r"C:\Users\15845\nlp_hst\ch2\res\simsun.ttf", # 兼容中文字体，不然中文会显示乱码
                         background_color="white",# 设置背景颜色
                         max_words=50,
                         mask = mask # 设置背景图
                         ).generate(wl_space_split)
plt.imshow(my_wordcloud)
plt.axis("off")
plt.show()
```
运行结果：
![image.png](https://i.loli.net/2019/10/14/FDzoqK85ZwE3N91.png)
## 任务拓展
wordcloud支持的所有参数：
```
font_path : string  //字体路径，需要展现什么字体就把该字体路径+后缀名写上，如：font_path = '黑体.ttf'
width : int (default=400)  //输出的画布宽度，默认为400像素
height : int (default=200)  //输出的画布高度，默认为200像素
prefer_horizontal : float (default=0.90) //词语水平方向排版出现的频率，默认 0.9 （所以词语垂直方向排版出现频率为 0.1 ）
mask : nd-array or None (default=None) //如果参数为空，则使用二维遮罩绘制词云。如果 mask 非空，设置的宽高值将被忽略，遮罩形状被 mask 取代。除全白（#FFFFFF）的部分将不会绘制，其余部分会用于绘制词云。如：bg_pic = imread('读取一张图片.png')，背景图片的画布一定要设置为白色（#FFFFFF），然后显示的形状为不是白色的其他颜色。可以用ps工具将自己要显示的形状复制到一个纯白色的画布上再保存，就ok了。
scale : float (default=1) //按照比例进行放大画布，如设置为1.5，则长和宽都是原来画布的1.5倍。
min_font_size : int (default=4) //显示的最小的字体大小
font_step : int (default=1) //字体步长，如果步长大于1，会加快运算但是可能导致结果出现较大的误差。
max_words : number (default=200) //要显示的词的最大个数
stopwords : set of strings or None //设置需要屏蔽的词，如果为空，则使用内置的STOPWORDS
background_color : color value (default=”black”) //背景颜色，如background_color='white',背景颜色为白色。
max_font_size : int or None (default=None) //显示的最大的字体大小
mode : string (default=”RGB”) //当参数为“RGBA”并且background_color不为空时，背景为透明。
relative_scaling : float (default=.5) //词频和字体大小的关联性
color_func : callable, default=None //生成新颜色的函数，如果为空，则使用 self.color_func
regexp : string or None (optional) //使用正则表达式分隔输入的文本
collocations : bool, default=True //是否包括两个词的搭配
colormap : string or matplotlib colormap, default=”viridis” //给每个单词随机分配颜色，若指定color_func，则忽略该方法。
 
fit_words(frequencies) //根据词频生成词云
generate(text) //根据文本生成词云
generate_from_frequencies(frequencies[, ...]) //根据词频生成词云
generate_from_text(text) //根据文本生成词云
process_text(text) //将长文本分词并去除屏蔽词（此处指英语，中文分词还是需要自己用别的库先行实现，使用上面的 fit_words(frequencies) ）
recolor([random_state, color_func, colormap]) //对现有输出重新着色。重新上色会比重新生成整个词云快很多。
to_array() //转化为 numpy array
to_file(filename) //输出到文件
```
## 任务实训
### 实训目的
1.	熟练使用jieba统计词频
2.	熟练使用wordcloud生成词云
### 实训内容
用python进行文本分词并生成词云，其中分词操作和生成词云操作要求封装到不同的函数中进行处理。

答案：
https://github.com/fuqiuai/wordCloud
