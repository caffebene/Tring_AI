[TOC]
# 任务4：自然语言处理中的数据清洗

## 1 任务目标

1. 了解数据清洗的意义
1. 掌握使用python的pandas库和numpy库来对数据进行清洗

## 2 任务描述

- 数据清洗，是整个数据分析过程中不可缺少的一个环节，其结果质量直接关系到模型效果和最终结论，就像做一道菜之前需要先择菜洗菜一样。
- 我们在书上看到的数据，譬如常见的iris数据集，房价数据，电影评分数据集等等，数据质量都很高，没有缺失值，没有异常点，也没有噪音，而在真实数据中，我们拿到的数据可能包含了大量的缺失值，可能包含大量的噪音，也可能因为人工录入错误导致有异常点存在，对我们挖据出有效信息造成了一定的困扰。
- 所以我们需要通过一些方法，尽量提高数据的质量。
- 在实际操作中，数据分析师经常需要花费大量的时间来清洗数据或者转换格式，这个工作甚至会占整个数据分析流程的80%左右的时间。


## 3 知识准备

### 3.1 数据清洗路径
![image](https://s2.ax1x.com/2019/10/14/KpIqKg.png)
#### 3.1.1 数据预处理
1. 部署环境，导入分析包和[数据](https://www.kaggle.com/carrie1/ecommerce-data)，这是一个来自英国零售商实际交易的电子商务数据集：

```
#导入数据分析包
import pandas as pd
import numpy as np

#导入csv数据
#dtype = str,最好读取的时候都以字符串的形式读入，不然可能会使数据失真
#比如一个0010008的编号可能会读取成10008

fileNameStr = r'..\ch3_data.csv'
DataDF = pd.read_csv(fileNameStr,encoding = "ISO-8859-1",dtype = str)

# encoding = "ISO-8859-1" -- 用什么解码，一般会默认系统的编码，如果是中文就用 "utf-8"
DataDF = pd.read_csv(fileNameStr,encoding = "ISO-8859-1",dtype = str)
```
2. 人工理解数据集
我们可以通过对数据集提问来判断这份数据能不能满足解答我们的问题，数据是否干净需不需要进一步处理，问题包括但不限于：
- 数据集多少数据？
- 包含了什么字段？字段格式是什么？
- 字段分别代表什么意义
- 字段之间的关系是什么？可以用做什么分析？或者说能否满足了对分析的要求？
- 有没有缺失值；如果有的话，缺失值多不多？
- 现有数据里面有没有脏数据？比如人工输入的数据，经常会出现名称写错，多输入空格等等的情况
- ...

3. 查看我们的数据
    1. 从宏观的角度去看数据
    
    ```
    查看dataframe的信息
    DataDF.info()
    ```
    ![image](https://s2.ax1x.com/2019/10/14/KpFJQe.png)
    由此可见，这个数据集分别包含了发票号码、股份代号、描述、数量、发票日期、单价、顾客ID和国家八个属性列。
    2. 一行一列看数据

    ```
    #查看每一列的数据类型
    print(DataDF.dtypes)
    ```
    ![image](https://s2.ax1x.com/2019/10/14/KpFadI.png)
    ```
    由此可以看出，本数据集每一列的数据类型都是对象类型。
    #有多少行，多少列
    print(DataDF.shape)
    ```
    ![image](https://s2.ax1x.com/2019/10/14/KpFsSS.png)
    由此可知，这个数据集总共有541909行，8列。


### 3.2 缺失数据的处理
#### 3.2.1 查看缺失数据
我们可以通过代码查看缺失数据：

```
# 检查缺失数据
# 如果你要检查每列缺失数据的数量，使用下列代码是最快的方法。
# 可以让你更好地了解哪些列缺失的数据更多，从而确定怎么进行下一步的数据清洗和分析操作。
DataDF.isnull() # 查看是否是缺失值
```
![image](https://s2.ax1x.com/2019/10/14/KpF4YV.png)
由于显示数据不够全，我们一下子并不能判断是哪些数据缺失，下面我们查看具体细节：
```
DataDF.isnull().any()  # 判断哪些"列"存在缺失值
```
![image](https://s2.ax1x.com/2019/10/14/KpkSSO.png)
由此可见，CustomerID和Description列是存在缺失值的。
```
DataDF[DataDF.isnull().values==True]  # 只显示存在缺失值的行列
```
![image](https://s2.ax1x.com/2019/10/14/KpkVtP.png)
```
DataDF.isnull().sum().sort_values(ascending=False) # 输出每个列丢失值也即值为NaN的数据和，并从多到少排序
```
![image](https://s2.ax1x.com/2019/10/14/KpktpT.png)
存在缺失值的列数情况分别是CustomerID有135080列存在缺失值，同时还存在1454列Description值缺失ID情况。

#### 3.2.2 处理缺失值

Python缺失值有三种：
1. Python内置的None值；
2. 在Pandas中，缺失值被表示为NA，即Not Available，不可用；
3. 对于数值数据，Pandas使用浮点值NaN（Not a Number）表示缺失数据。后面出来数据，如果遇到错误：说什么float错误，那就是有缺失值，需要处理掉。

    通过代码我们可以查看缺失值之间的区别：
    ```
    # None和NaN的区别
    print("None的数据类型", type(None))
    
    from numpy import NaN
    print("NaN的数据类型", type(NaN))
    ```
    ![image](https://s2.ax1x.com/2019/10/14/KpaSaR.png)

对于缺失值，我们有以下几种处理方法：
1. 去除缺失值；
    
    ```
    # 默认（axis＝0）是逢空值剔除整行，设置关键字参数axis＝1表示逢空值去掉整列
    DataDF= DataDF.drop(['CustomerID'], axis = 1)
    
    # 'any'如果一行（或一列）里任何一个数据有任何出现Nan就去掉整行，‘all’一行（或列）每一个数据都是Nan才去掉这整行
    DataDF.dropna(how='any')
    DataDF.dropna(how='all')
    
    # 去掉这个特征为空的行
    DataDF_new = DataDF.drop(DataDF[DataDF.UnitPrice.isnull()].index)
    # 返回已经去掉重复行的数据集
    DataDF.drop_duplicates()
    
    # 更精细的thresh参数，它表示留下此行（或列）时，要求有多少［非缺失值］
    DataDF.dropna(thresh = 6 )
    ```
    去除一些特征行以后我们仍旧通过info()函数查看本数据集信息：
    ![image](https://s2.ax1x.com/2019/10/14/KpaLFI.png)
    可以发现，我们已经删除掉了一些行和列。

2. 补全缺失值

    某些缺失值可以进行填充，一般遵循下面四种方法：
    1. 以业务知识或经验推测（默认值）填充缺失值
    2. 以同一指标的计算结果（均值、中位数、众数等）填充缺失值
    3. 用相邻值填充缺失值
    4. 以不同指标的计算结果填充缺失值，比如年龄字段缺失，但是有屏蔽后六位的身份证号可以推算具体的年龄是多少

这里简单介绍用不同方式进行填充的代码例子：

```
# 用默认值填充:df.fillna(' ')
DataDF.CustomerID= DataDF.CustomerID.fillna('Not Given')
# 借助函数loc来选择行的数据查看我们的填充结果
print(DataDF.loc[488696])
```
![image](https://s2.ax1x.com/2019/10/14/KpDjsK.png)
```
# 用平均值填充:df.fillna(df.mean())
DataDF.UnitPrice = DataDF.UnitPrice.fillna(DataDF.UnitPrice.mean())
# 使用中位数填充
DataDF.UnitPrice = DataDF.UnitPrice.fillna(DataDF.UnitPrice.median())
# 使用众数填充
DataDF.UnitPrice = DataDF.UnitPrice.fillna(DataDF.UnitPrice.mode().values)

# 用相邻的值进行填充
print(DataDF)
print(DataDF.UnitPrice.fillna(method='ffill')) # 前向后填充
print(DataDF.UnitPrice.fillna(method='bfill')) # 后向前填充
```
还有一些其他填充方式，大家自己动手试试看吧！


### 3.3 数据格式的处理
#### 3.3.1 调整数据类型
- 在上面的内容中我们学会了通过调用dtypes函数来查看每一列的数据类型，那么我们的数据集还存在一些其他什么问题要处理呢？
- 由于一开始用到了str来导入，打算后期再更换格式，需要调整数据类型。

```
#字符串转换为数值（整型）
DataDF['Quantity'] = DataDF['Quantity'].astype('int')
#字符串转换为数值（浮点型）
DataDF['UnitPrice'] = DataDF['UnitPrice'].astype('float')

# 查看数据集的信息
DataDF.info()
```
![image](https://s2.ax1x.com/2019/10/14/KpsFk4.png)
显而易见，我们的数据集的部分数据类型已经发生了变化。

- 处理日期类型，观察数据集中的每一列可知，时间字符串为：“年/月/日 时:分”的格式，但是其中的时间我们不需要。所以我们考虑把时间分离出来，并且将其删除。

```
#定义函数：分割销售日期，获取销售日期
#输入：timeColSer 销售时间这一列，是个Series数据类型
#输出：分割后的时间，返回也是个Series数据类型
def SplitTime(timeColSer):
    timeList=[]
    for value in timeColSer:
        #例如2012-01-01 10:40，分割后为：2018-01-01
        dateStr=value.split(' ')[0]
        timeList.append(dateStr)
    #将列表转行为一维数据Series类型
    timeSer=pd.Series(timeList)
    return timeSer
```
```
#数据类型转换:字符串转换为日期
#errors='coerce' 如果原始数据不符合日期的格式，转换后的值为空值NaT
print("日期格式调整前：")
print(DataDF.loc[:,'InvoiceDate'])

# 分割时间字符串
time = DataDF.loc[:,'InvoiceDate']
data_new = SplitTime(time)
print(data_new)

# 赋值
DataDF.loc[:,'InvoiceDate'] = data_new
# 格式转换
data_new=pd.to_datetime(DataDF.loc[:,'InvoiceDate'],
                                           format='%d/%m/%Y',
                                           errors='coerce')


# 查看数据集的信息
print("日期格式调整后：")
print(DataDF.loc[:,'InvoiceDate'])


#!!⚠️ format 是你［原始数据］中日期的格式

%y 两位数的年份表示（00-99）
%Y 四位数的年份表示（000-9999）
%m 月份（01-12）
%d 月内中的一天（0-31）
%H 24小时制小时数（0-23）
%I 12小时制小时数（01-12）
%M 分钟数（00-59）
%S 秒（00-59）
```
![image](https://s2.ax1x.com/2019/10/14/Kp5wff.png)

### 3.3 逻辑错误数据的处理
这里其实还是Dataframe.loc这个函数的知识点。由于loc还可以判断条件是否为True，而一般来说价格不能为负，所以从逻辑上来说如果价格是小于0的数据应该予以筛出。

```
#删除异常值：通过条件判断筛选出数据
#查询条件
querySer=DataDF.loc[:,'UnitPrice']>0
print("查询结果：")
print(querySer)
#应用查询条件
print('删除异常值前：',DataDF.shape)
DataDF=DataDF.loc[querySer,:]
print('删除异常值后：',DataDF.shape)
```
结果如下：

![image](https://s2.ax1x.com/2019/10/14/Kpoaz8.png)

## 4. 任务实施
根据前面的学习内容，我们已经了解了如何进行数据清洗，以及对数据的信息进行检索阅读。现在来动手做一下本章的任务吧。
本次的分析目标是从销售数据中分析出以下业务指标：
1. 月均消费次数
2. 月均消费金额

### 4.1 实施思路
我们通常按照以下流程进行数据分析，问题已经提出了，那么下面我们怎么来进行清洗并且解决问题呢？
![image](http://ddrvcn.oss-cn-hangzhou.aliyuncs.com/2019/7/3e67zm.jpg)


### 4.2 实施步骤


#### 步骤1：导入数据包，提取数据文件
在提取数据中，为了保证数据文件中的数据读取正常，通常会将函数的dtype参数设置成’object’。 object的意思是统一按照字符串的格式来读取数据。

```
# 导入numpy、pandas包
import numpy as np
import pandas as pd

# 导入数据
salesDf = pd.read_excel(r'..\朝阳医院2018年销售数据.xlsx',dtype='object')
```


#### 步骤2：查看导入数据的基本状况
我们可以通过输出来查看数据的基本情况
```
# 查看导入数据的基本状况

# 查看导入数据的类型
print("数据的类型: ")
print(type(salesDf))

# 查看导入数据的每个项目的类型
print("项目的类型： ")
print(salesDf.dtypes)

# 查看数据的基本大小
print("数据的基本大小: ")
print(salesDf.shape)

# 查看开头几行数据，看看数据是否读取正确
print("前五行数据为：")
print(salesDf.head())

# 用描述函数describ来查看一下整个数据的基本状况
print("数据的基本描述：")
print(salesDf.describe())
```
从这些函数中，我们可以基本了解到数据由购药时间、社保卡号、商品编码、商品名称、销售数量、应收金额、实收金额这七个基本项目组成，数据条数为6578条。
![image](https://s2.ax1x.com/2019/10/15/K9H0u8.png)



#### 步骤3：数据清洗
取得了数据，并不能马上就开始进行数据分析。我们得到的数据通常并不是完全符合我们分析要求的，而且可能存在缺失值、异常值，这些数据都会使我们的分析结果产生偏差。所以在分析之前，需要进行子集选择、缺失数据补充、异常值处理、数据类型转换等多个步骤。这些都属于数据清理的范畴。
所以我们利用前面所学的知识来进行数据清洗吧！
1. 列重命名
    销售数据集，购药时间显示为销售时间更为合理，我们先把这个项目名称做一下变更；
        
    ```
    # 购药时间->销售时间
    nameChangeDict = {'购药时间':'销售时间'}
    # 参数inplace=True表示覆盖元数据集
    salesDf.rename(columns = nameChangeDict,inplace=True)
    
    # 查看开头几行数据，看看数据是否修改正确
    print("前五行数据为：")
    print(salesDf.head())
    ```
    ![image](https://s2.ax1x.com/2019/10/15/K9H23q.png)
    可见我们已经成功修改数据列命名。

2. 缺失数据处理
    1. 首先查看一下那些目录存在缺失值
    
    ```
    # 查看一下哪些项目存在缺失值
    print(salesDf.isnull().any())
    ```
    ![image](https://s2.ax1x.com/2019/10/15/K9H759.png)
    虽然每个项目都存在缺失值。但是在这个销售数据中，销售时间和社保卡号是必须项目，不可或缺。所以我们在这里只把销售时间和社保卡号有缺失的数据做删除处理。我们来查看一下销售时间和社保卡缺失的数据大小，然后做删除处理。

    2. 查看缺失值的数量
    ```
    # 查看一下缺失值的数量
    # 通常可以用isnull函数来查找缺失值
    print(salesDf[salesDf[['销售时间','社保卡号']].isnull().values == True])
    ```

    ![image](https://s2.ax1x.com/2019/10/15/K9b95d.png)
    序号6574因为销售时间和社保卡号都缺失，所以会出现两次。所以我们要去掉一下重复数据。
    
    3. 去掉重复值
    ```
    # 去掉重复数据
    naDf = salesDf[salesDf[['销售时间','社保卡号']].isnull().values == True].drop_duplicates()
    print(naDf)
    ```
    ![image](https://s2.ax1x.com/2019/10/15/K9bGrT.png)
    从上面可以清楚看出销售时间和社保卡号缺失的数据一共有三条，当数据量大的时候我们可以用naDf.shape[0]来实现只显示条数，不显示数据内容的效果。
    4. 删除缺失数据
    
    ```
    print("删除前数据集规模显示:")
    print(salesDf.shape)
    
    
    #含有销售时间和社保卡号的缺失数据删除
    salesDf = salesDf.dropna(subset=['销售时间','社保卡号'],how = 'any')
    print("删除后数据集规模显示:")
    print(salesDf.shape)
    ```
    ![image](https://s2.ax1x.com/2019/10/15/K9brM6.png)
    在数据删除后要及时更新一下最新的序号，不然可能会产生问题。如果在这里没有更新数据序号，就会导致后续销售时间数据类型用函数转换后与元数据合并时发生了错位，数据发生了新的缺失。
    
    ```
    # 重命名行名（index）：排序后的列索引值是之前的行号，需要修改成从0到N按顺序的索引值
    salesDf=salesDf.reset_index(drop=True)
    ```


    

#### 步骤4：数据类型转换
 - 数量、金额项目应该从字符串类型转换为数值（浮点型）类型：

```
salesDf['销售数量'] = salesDf['销售数量'].astype('float')
salesDf['应收金额'] = salesDf['应收金额'].astype('float')
salesDf['实收金额'] = salesDf['实收金额'].astype('float')
print('转换后的数据类型：')
print(salesDf.dtypes)
```
![image](https://s2.ax1x.com/2019/10/15/K9bzQ0.png)
- 日期项目应该从字符串类型转换为日期类型：
销售日期中包含了日期和星期，我们只要保留日期内容即可。这里用一个自定义的函数dateChange来实现这个功能。

```
# 日期转换
def dateChange(dateSer):
    dateList = []
    for i in dateSer:
        #例如2018-01-01 星期五，分割后为：2018-01-01
        str = i.split(' ')[0]
        dateList.append(str)
    dateChangeSer = pd.Series(dateList)
    return dateChangeSer

dateChangeSer = dateChange(salesDf['销售时间'])
print(dateChangeSer)
```

![image](https://s2.ax1x.com/2019/10/15/K9qAY9.png)

- 在做完转化后再观察一下有没有产生新的缺失值

```
print(salesDf['销售时间'].isnull().any())
```
输出False证明我们的转换没有产生新的缺失，


#### 步骤5：数据排序
销售记录一般是以销售时间为顺序排列的，所以我们对数据进行一下排序

```
#按销售时间排序
salesDf = salesDf.sort_values(by='销售时间')
#再次更新一下序号
salesDf = salesDf.reset_index(drop = True)

print(salesDf.head())
```
原来的数据顺序：

![image](https://s2.ax1x.com/2019/10/15/K9qW0U.png)
排序后的数据顺序：
![image](https://s2.ax1x.com/2019/10/15/K9qf7F.png)

#### 步骤6：异常值处理
在下面数据集的描述指标中可以看出，存在销售数量为负的数据，这明显是不合理的，我们把这部分数据也进行删除：
![image](http://ddrvcn.oss-cn-hangzhou.aliyuncs.com/2019/7/JJz6Zr.jpg)
删除异常值：

```
#删除异常值：通过条件判断筛选出数据
#查询条件
querySer=salesDf.loc[:,'销售数量']>0
#应用查询条件
print('删除异常值前：',salesDf.shape)
salesDf=salesDf.loc[querySer,:]
print('删除异常值后：',salesDf.shape)
```
![image](https://s2.ax1x.com/2019/10/15/K9qO0O.png)

#### 步骤7：计算并且解答问题
1. 首先计算月均消费次数
我们知道：月均消费次数=总消费次数 / 月份数
而总消费次数指的是同一天内，同一个人发生的所有消费算作一次消费。这里我们根据列名（销售时间，社区卡号）结合，如果这两个列值同时相同，只保留1条，将重复的数据删除；
月份数指的是数据已经按照销售时间进行排序，只需将最后的数据与第一条数据相减就可换算出月份数。

```
# 月份数计算

def parse(time_str):
    """
    Parse time format.
    :param time_str: <str> time string
    :return: <datetime.date> date
    """
    time_list = time_str.split ( "-" )
    year = time_list[0]
    month = time_list[1]
    day = time_list[2]

    return datetime.date ( int ( year ), int ( month ), int ( day ) )

startDay = salesDf.loc[0,'销售时间'].split(' ')[0] # 最小时间值
dateChangeOut = dateChange(salesDf['销售时间'])
print('开始日期:',parse(startDay))
endDay = salesDf.loc[salesDf.shape[0]-1,'销售时间'].split(' ')[0]  #最大时间值
print('结束日期:',parse(endDay))
monthCount = (parse(endDay) - parse(startDay)).days #函数.days计算天数，“//”表示取整数
print('月份数:',monthCount)


# 月均消费次数=总消费次数 / 月份数
kpi1 = total / monthCount
print('业务指标1：月均消费次数=',kpi1)
```
![image](https://s2.ax1x.com/2019/10/15/K9vUIO.png
)

2. 月均消费金额计算

```
# 月均消费金额 = 总消费金额 / 月份数
totalMoney = salesDf['实收金额'].sum()
kpi2 = totalMoney / monthCount
print('月平均消费金额=',kpi2)
```

![image](https://s2.ax1x.com/2019/10/15/K9vdiD.png)


## 5 任务拓展
在分析过程中，通过遇到问题以及查询百度谷歌解决问题过程当中，总结以下小点。

1、每次删减数据和转换类型，都需要查询是否有异常值和缺失值，防止分析结果出现偏差。

2、删减数据行时，Python不能自动更新序号，需手动更新。

3、针对某些缺失值，可以通过观察和业务部门的沟通，根据实际情况修正填充该类数据。


## 6 任务实训


### 6.1 实训目的
- 熟练掌握对数据进行分析
- 熟练数据清洗的各种方法


### 6.2 实训内容
实训内容：
1. 针对上述数据，再计算一个业务指标：
    1. 指标3：客单价=总消费金额 / 总消费次数
答案：

```
答案：
# 指标3
kpi3 = kpi2 / kpi1
print('业务指标3：客单价=',kpi3)
```

