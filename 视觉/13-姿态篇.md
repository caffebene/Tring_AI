[TOC]
# 任务n：学习理解人体姿态估计相关知识
## 1.任务目标
<!-- 1. 
2. 
3. 
4.  -->
- 学习理解人体姿态估计的概念和理论
- 了解人体姿态估计领域常用数据集
- 掌握人体姿态估计领域的衡量标准以及损失函数
- 学会使用相关的人体姿态估计算法

## 2.任务描述
<!-- - 从自然实况入手，一步步从计算机视觉的角度去理解如何做人体姿态的估计
- 以二维的人体姿态估计为切入点,讲解围绕二维人体姿态估计的相关衡量标准以及损失函数
- 动手实践一个人体姿态估计算法,观察预测结果 -->


- 人体姿态估计是计算机视觉中一个很基础的问题。从名字的角度来看，可以理解为对“人体”的姿态（关键点，比如头，左手，右脚等）的位置估计。一般我们可以这个问题再具体细分成4个任务：

- 单人姿态估计 (Single-Person Skeleton Estimation)
- 多人姿态估计 (Multi-person Pose Estimation)
- 人体姿态跟踪 （Video Pose Tracking)
- 3D人体姿态估计 （3D Skeleton Estimation)


## 3.知识准备

### 单人姿态估计
-  输入是一个crop出来的行人，然后在行人区域位置内找出需要的关键点，比如头部，左手，右膝等。常见的数据集有MPII, LSP, FLIC, LIP。其中MPII是2014年引进的，目前可以认为是单人姿态估计中最常用的benchmark， 使用的是PCKh的指标（可以认为预测的关键点与GT标注的关键点经过head size normalize后的距离）。但是经过这几年的算法提升，整体结果目前已经非常高了（最高的已经有93.9%了）。下面是单人姿态估计的结果图（图片来源于CPM的paper)

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch13/1.jpg" width="700"/>
</div>

### 多人姿态估计
-  单人姿态估计算法往往会被用来做多人姿态估计。多人姿态估计的输入是一张整图，可能包含多个行人，目的是需要把图片中所有行人的关键点都能正确的做出估计。针对这个问题，一般有两种做法，分别是top-down以及bottom-up的方法。对于top-down的方法，往往先找到图片中所有行人，然后对每个行人做姿态估计，寻找每个人的关键点。单人姿态估计往往可以被直接用于这个场景。对于bottom-up，思路正好相反，先是找图片中所有parts （关键点），比如所有头部，左手，膝盖等。然后把这些parts（关键点）组装成一个个行人。

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch13/2.jpg" width="400"/>
</div>

- 对于测试集来讲，主要有COCO, 最近有新出一个数据集CrowdPose。下面是CPN算法在COCO上面的结果：
<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch13/3.jpg" width="400"/>
</div>

### 人体姿态跟踪
-  如果把姿态估计往视频中扩展的话，就有了人体姿态跟踪的任务。主要是针对视频场景中的每一个行人，进行人体以及每个关键点的跟踪。这个问题本身其实难度是很大的。相比行人跟踪来讲，人体关键点在视频中的temporal motion可能比较大，比如一个行走的行人，手跟脚会不停的摆动，所以跟踪难度会比跟踪人体框大。目前主要有的数据集是PoseTrack。


### 3D 人体姿态估计
-  同时，如果把人体姿态往3D方面进行扩展，输入RGB图像，输出3D的人体关键点的话，就是3D 人体姿态估计。这个有一个经典的数据集Human3.6M。最近，除了输出3D的关键点外，有一些工作开始研究3D的shape，比如数据集DensePose。长线来讲，这个是非常有价值的研究方向。3D人体姿态估计的结果图（来自算法a simple baseline)如下：

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch13/4.jpg" width="400"/>
</div>

- Densepose算法的结果输出：

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch13/5.jpg" width="400"/>
</div>




### 相关数据集
#### Benchmark
<!-- 还可以给每个数据集加点简单的描述 -->
- 单人估计: [MPII](http://human-pose.mpi-inf.mpg.de/), [FLIC](https://bensapp.github.io/flic-dataset.html), [LSP](http://sam.johnson.io/research/lsp.html), [LIP](http://sysu-hcp.net/lip/)

- 多人关键点预测: [COCO](http://cocodataset.org/#keypoints-2019), [CrowdPose](http://cocodataset.org/#keypoints-2019)

- 视频：[PoseTrack](https://posetrack.net/)

- 三维人体：[Human3.6M](http://vision.imar.ro/human3.6m/description.php), [DensePose](http://densepose.org/)


<!-- ### 评估指标
- 基于对象关键点相似度（OKS）的mAP：
- $$\frac{\sum}{2}$$
- AP(average precision)
- AP Across Scales
- AR(average recall)
- AR Across Scales -->

<!-- ### 损失函数



### 自下而上和自上而下的检测思想
#### 自下而上
- Mask R-CNN, CPN, MSPN
- 高性能（良好的本地化能力），高召回率


#### 自上而下
- Openpose, Associative Embeding
- 简洁的框架，可能更快 -->

### 挑战
- 模棱两可的外观
- 拥挤人群，遮挡现象
- 实时估计的速度

### 相关算法

#### OpenPose

- 在2016年COCO比赛中，当时的第一名就是OpenPose。CMU团队基于CPM为组件，先找到图片中的每个joint的位置，然后提出Part Affinity Field （PAF)来做人体的组装。


- PAF的基本原理是在两个相邻关键点之间，建立一个有向场，比如左手腕，左手肘。我们把CPM找到的所有的左手腕以及左手肘拿出来建立一个二分图，边权就是基于PAF的场来计算的。然后进行匹配，匹配成功就认为是同一个人的关节。依次类别，对所有相邻点做此匹配操作，最后就得到每个人的所有关键点。

- 在当时来讲，这个工作效果是非常惊艳的，特别是视频的结果图，具体可以参考Openpose的Github官网。在COCO的benchmark test-dev上面的AP结果大概是61.8。





### 应用

- 最后，讲了这么多的人体姿态估计，我们最后说一下人体姿态估计有什么用，这里的人体姿态估计是一个广义的人体姿态估计，包含2D/3D等。

- 首先的一个应用是人体的动作行为估计，要理解行人，人体的姿态估计其实是一个非常重要的中间层信息。目前有蛮多基于人体姿态估计直接做action recogntion的工作，比如把关键点当成graph的节点，然后是使用graph convolution network来整合各种信息做动作分类。我博士的研究课题是action recognition，我读完四年博士的一个总结是action这个问题，如果需要真正做到落地，人体姿态估计算法是必不可少的组成部分。

- 第二类应用是偏娱乐类的，比如人体交互，美体等。比如可以通过3d姿态估计来虚拟出一个动画人物来做交互，使用真实人体来控制虚拟人物。另外比如前一段时间比较火热的瘦腰，美腿等操作背后都可能依赖于人体姿态估计算法。

- 第三类应用是可以做为其他算法的辅助环节，比如Person ReID可以基于人体姿态估计来做alignment，姿态估计可以用来辅助行人检测，杀掉检测的FP之类的。


## 4. 任务实施
### 4.1 实施思路

- 通过调用百度api的图像检测接口，实现对目标的检测以及边界框的绘制。
- 在含有摄像头的设备，可以在[在线平台](https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html)中尝试实现我们的实时姿态估计。



### 4.2 实施步骤

### 4.2 实施步骤
#### 步骤1：导入 urllib 库

```
%matplotlib inline
import cv2
import numpy as np

from matplotlib import pyplot as plt

import sys
import json
import base64

import ssl
## make it work in both python2 both python3
IS_PY3 = sys.version_info.major == 3
if IS_PY3:
    from urllib.request import urlopen
    from urllib.request import Request
    from urllib.error import URLError
    from urllib.parse import urlencode
    from urllib.parse import quote_plus
else:
    import urllib2
    from urllib import quote_plus
    from urllib2 import urlopen
    from urllib2 import Request
    from urllib2 import URLError
    from urllib import urlencode



```


#### 步骤2：请求认证 token

```
## 跳过 HTTPS 身份验证
ssl._create_default_https_context = ssl._create_unverified_context
API_KEY = '你的API_KEY'
SECRET_KEY = '你的SECRET_KEY'
POSE_DETECT = "https://aip.baidubce.com/rest/2.0/image-classify/v1/body_analysis"
""" 开始 TOKEN """
TOKEN_URL = 'https://aip.baidubce.com/oauth/2.0/token'

""" 请求认证 token
"""
def fetch_token():
    params = {'grant_type': 'client_credentials',
        'client_id': API_KEY,
        'client_secret': SECRET_KEY}
    post_data = urlencode(params)
    if (IS_PY3):
        post_data = post_data.encode('utf-8')
    req = Request(TOKEN_URL, post_data)
    try:
        f = urlopen(req, timeout=5)
        result_str = f.read()
    except URLError as err:
        print(err)
    if (IS_PY3):
        result_str = result_str.decode()
    result = json.loads(result_str)
    if ('access_token' in result.keys() and 'scope' in result.keys()):
        if not 'brain_all_scope' in result['scope'].split(' '):
            print ('please ensure has check the ability')
            exit()
        return result['access_token']
    else:
        print ('please overwrite the correct API_KEY and SECRET_KEY')
        exit()



```


#### 步骤3：定义读取图片函数

```
""" 读取图片函数
"""

def read_file(image_path):
    f = None
    try:
        f = open(image_path, 'rb') #用二进制打开图片
        return f.read()
    except:
        print('read image file fail')
        return None
    finally:
        if f:
            f.close()

```




#### 步骤4：调用远程 HTTP 服务器


```
"""
call remote http server
"""
def request(url, data):
    req = Request(url, data.encode('utf-8'))
    has_error = False
    try:
        f = urlopen(req)
        result_str = f.read()
        if (IS_PY3):
            result_str = result_str.decode()
        return result_str
    except URLError as err:
        print(err)

```


#### 步骤5：程序入口

```
if __name__ == '__main__':
    ## get access token
    token = fetch_token()
    ## concat url
    url = POSE_DETECT + "?access_token=" + token
    
    file_content = read_file('./img1.jpg')
    response = request(url, urlencode(
    {
        'image': base64.b64encode(file_content),
        'image_type': 'BASE64',
    }))
    data = json.loads(response)
```



#### 步骤6：绘制人体关键点

```
sourceImg = cv2.imread('img1.jpg')
srcImage_new = cv2.cvtColor(sourceImg, cv2.COLOR_BGR2RGB)
plt.imshow(srcImage_new)

poseTrack = data['person_info'][0]['body_parts']
for i in poseTrack:
    plt.scatter(poseTrack[i]['x'],poseTrack[i]['y'],s=20,color='red',alpha=0.8)

plt.show()
```

## 5.任务拓展
### 5.1 实时的人体姿态估计是如何实现的？

- 现在我们看到大多数的实时人体姿态估计算法，都是基于单帧的人体姿态估计算法去实现的。这意味着，我们将视频分割成好多帧，然后对每一帧的图像做一次人体姿态估计，然后显示出来。

### 5.2 三维人体姿态估计的概述？

- 在给定图像或视频的情况下估计人体3D姿势最近受到了科学界的极大关注。这种趋势的主要原因是不断增加的新应用范围（例如，人机交互，游戏，运动性能分析），这些应用是由当前的技术进步驱动的。尽管最近的方法已经处理了若干挑战并且已经报告了显着的结果，但是3D姿态估计仍然是一个很大程度上未解决的问题，因为现实生活中的应用带来了一些挑战，现有方法没有完全解决 例如，估计室外环境中多人的3D姿势仍然是一个很大程度上未解决的问题。在本文中，我们回顾了RGB图像或图像序列的3D人体姿态估计的最新进展。我们基于输入（例如，单个图像或视频，单眼或多视图）提出方法的分类，并且在每种情况下，我们根据其关键特征对方法进行分类。为了概述当前的能力，我们对专门为此任务创建的综合数据集进行了广泛的实验评估，并将其基本事实公之于众，以供研究之用。最后，我们对通过审阅文献和实验结果所获得的见解进行了深入的讨论。确定了未来的方向和挑战。为了概述当前的能力，我们对专门为此任务创建的综合数据集进行了广泛的实验评估，并将其基本事实公之于众，以供研究之用。最后，我们对通过审阅文献和实验结果所获得的见解进行了深入的讨论。确定了未来的方向和挑战。为了概述当前的能力，我们对专门为此任务创建的综合数据集进行了广泛的实验评估，并将其基本事实公之于众，以供研究之用。最后，我们对通过审阅文献和实验结果所获得的见解进行了深入的讨论。确定了未来的方向和挑战。

## 6. 任务实训
### 6.1 实训目的

- 加深对人体姿态估计算法的理解
- 拓展人体姿态估计算法的应用知识

### 6.2 实训内容

- 使用提供的算法包，实现对图片的三维人体姿态估计。

### 6.3 示例代码



```
参考算法包
```