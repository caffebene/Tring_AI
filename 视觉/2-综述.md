[TOC]

# 任务2：学习计算机视觉相关概论

## 1.任务目标

<!-- 1. 
2. 
3. 
4.  -->

- 学习了解计算机视觉的多项任务
- 在综述介绍中理解该任务，并为后续课程打下基础

## 2.任务描述

- 计算机视觉是关于研究机器视觉能力的学科，或者说是使机器能对环境和其中的刺激进行可视化分析的学科。机器视觉通常涉及对图像或视频的评估，英国机器视觉协会（BMVA）将机器视觉定义为「对单张图像或一系列图像的有用信息进行自动提取、分析和理解」。
- 对我们环境的真正理解不是仅通过视觉表征就可以达成的。更准确地说，是视觉线索通过视觉神经传输到主视觉皮层，然后由大脑以高度特征化的形式进行分析的过程。从这种感觉信息中提取解释几乎包含了我们所有的自然演化和主体经验，即进化如何令我们生存下来，以及我们如何在一生中对世界进行学习和理解。
- 从这方面来说，视觉过程仅仅是传输图像并进行解释的过程，然而从计算的角度看，图像其实更接近思想或认知，涉及大脑的大量功能。因此，由于跨领域特性很显著，很多人认为计算机视觉是对视觉环境和其中语境的真实理解，并将引领我们实现强人工智能。



## 3.知识准备


### 3.1基础的计算机视觉任务
#### 分类/定位
- 图像分类任务通常是指为整张图像分配特定的标签，如下左图整张图像的标签为 CAT。而定位是指找到识别目标在图像中出现的位置，通常这种位置信息将由对象周围的一些边界框表示出来。目前 ImageNet上的分类/定位的准确度已经超过了一组训练有素的人类。因此相对于前一部分的基础，我们会着重介绍后面如语义分割、3D 重建等内容。

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch2/1.jpg" width="600"/>
</div>


- 然而随着目标类别的增加，引入大型数据集将为近期的研究进展提供新的度量标准。在这一方面，Keras创始人 Francois Chollet 将包括 Xception 等架构和新技术应用到谷歌内部的大型数据集中，该数据集包含 1.7 万个目标类别，共计 350M（Million）的多类别图像。


<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch2/2.jpg" width="600"/>
</div>

图 2：ILSVRC 竞赛中，分类/定位的逐年错误率，来源 Jia Deng (2016)，ILSVRC2016。
- ImageNet LSVRC（2016）亮点：

- 场景分类是指用「温室」、「体育场」和「大教堂」等特定场景对图像进行分类。ImageNet 去年举办了基于 Places2子数据的场景分类挑战赛，该数据集有 365 个场景共计 8 百万 训练图像。海康威视选择了深度类 Inception 的网络和并不太深的 ResNet，并利用它们的集成实现 9% 的 Top-5 误差率以赢得竞赛。。

- Trimps-Soushen 以 2.99% 的 Top-5 分类误差率和 7.71% 的定位误差率赢得了 ImageNet 分类任务的胜利。该团队使用了分类模型的集成（即 Inception、Inception-ResNet、ResNet 和宽度残差网络模块的平均结果）和基于标注的定位模型 Faster R-CNN来完成任务。训练数据集有 1000 个类别共计 120 万的图像数据，分割的测试集还包括训练未见过的 10 万张测试图像。。

- 机器视觉就是用机器代替人眼来做测量和判断。机器视觉系统是通过图像摄取装置将被摄取目标转换成图像信号，传送给专用的图像处理系统，得到被摄目标的形态信息，根据像素分布和亮度、颜色等信息，转变成数字化信号；图像系统对这些信号进行各种运算来抽取目标的特征，进而根据判别的结果来控制现场的设备动作。

- Facebook 的 ResNeXt 通过使用从原始 ResNet扩展出来的新架构而实现了 3.03% 的 Top-5 分类误差率。 

#### 目标检测

- 目标检测（Object Detection）即如字面所说的检测图像中包含的物体或目标。ILSVRC 2016对目标检测的定义为输出单个物体或对象的边界框与标签。这与分类/定位任务不同，目标检测将分类和定位技术应用到一张图像的多个目标而不是一个主要的目标。

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch2/3.jpg" width="600"/>
</div>

- 图 3：仅有人脸一个类别的目标检测。图为人脸检测的一个示例，作者表示目标识别的一个问题是小物体检测，检测图中较小的人脸有助于挖掘模型的尺度不变性、图像分辨率和情景推理的能力，来源 Hu and Ramanan (2016, p. 1)。

- 目标识别领域在 2016 年主要的趋势之一是转向更快、更高效的检测系统。这一特性在 YOLO、SSD 和 R-FCN 方法上非常显著，它们都倾向于在整张图像上共享计算。因此可以将它们与 Fast/Faster R-CNN 等成本较高的子网络技术区分开开来，这些更快和高效的检测系统通常可以指代「端到端的训练或学习」。


- 这种共享计算的基本原理通常是避免将独立的算法聚焦在各自的子问题上，因为这样可以避免训练时长的增加和网络准确度的降低。也就是说这种端到端的适应性网络通常发生在子网络解决方案的初始之后，因此是一种可回溯的优化（retrospective optimisation）。然而，Fast/Faster R-CNN 技术仍然非常有效，仍然广泛用于目标检测任务。


- SSD：Single Shot MultiBox Detector[22] 利用封装了所有必要计算并消除了高成本通信的单一神经网络，以实现了 75.1% mAP 和超过 Faster R-CNN 模型的性能（Liu et al. 2016）。


- 我们在 2016 年看到最引人注目的系统是「YOLO9000: Better, Faster, Stronger」[23]，它引入了 YOLOv2 和 YOLO9000 检测系统 [24]。YOLOv2 很大程度上提升了 2015 年提出的 YOLO 模型 [25] 性能，它能以非常高的 FPS（使用原版 GTX Titan X 在低分辨率图像上达到 90FPS）实现更好的结果。除了完成的速度外，系统在特定目标检测数据集上准确度要优于带有 ReNet 和 SSD 的 Faster RCNN。

- YOLO9000 实现了检测和分类的联合训练，并将其预测泛化能力扩展到未知的检测数据上，即它能检测从未见过的目标或物体。YOLO9000 模型提供了 9000 多个类别的实时目标检测，缩小了分类和检测数据集间的鸿沟。该模型其它详细的信息和预训练模型请查看：http://pjreddie.com/darknet/yolo/。


- Feature Pyramid Networks for Object Detection [27] 是 FAIR [28] 实验室提出的，它能利用「深度卷积网络的内部多尺度、金字塔型的层级结构构建具有边际额外成本的特征金字塔」，这意味着表征能更强大和快速。Lin et al. (2016) 在 COCO[29] 数据集上实现了顶尖的单模型结果。若与基础的 Faster R-CNN 相结合，将超过 2016 年最好的结果。

- R-FCN：Object Detection via Region-based Fully Convolutional Networks [30]，这是另一种在图像上避免应用数百次高成本的各区域子网络方法，它通过使基于区域的检测器在整张图像上进行全卷积和共享计算。「我们每张图像的测试时间只需要 170ms，要比 Faster R-CNN 快 2.5 到 20 倍」(Dai et al., 2016)。

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch2/4.jpg" width="600"/>
</div>

图 4：目标检测中的准确率权衡，来源 Huang et al. (2016, p. 9)。

- 注意：Y 轴表示的是平均准确率（mAP），X 轴表示不同元架构（meta-architecture）的各种特征提取器（VGG、MobileNet...Inception ResNet V2）。此外，mAP small、medium 和 large 分别表示对小型、中型和大型目标的检测平均准确率。即准确率是按「目标尺寸、元架构和特征提取器」进行分层的，并且图像的分辨率固定为 300。虽然 Faster R-CNN 在上述样本中表现得更好，但是这并没有什么价值，因为该元架构相比 R-FCN 来说慢得多。
- Huang et al. (2016)[32] 的论文提供了 R-FCN、SSD 和 Faster R-CNN 的深度性能对比。由于机器学习准确率对比中存在的问题，这里使用的是一种标准化的方法。这些架构被视为元架构，因为它们可以组合不同的特征提取器，比如 ResNet 或 Inception。
- 论文的作者通过改变元架构、特征提取器和图像分辨率研究准确率和速度之间的权衡。例如，对不同特征提取器的选择可以造成元架构对比的非常大的变化。
- 实时商业应用中需要低功耗和高效同时能保持准确率的目标检测方法，尤其是自动驾驶应用，SqueezeDet[33] 和 PVANet[34] 在论文中描述了这种发展趋势。
- COCO[36] 是另一个常用的图像数据集。然而，它相对于 ImageNet 来说更小，更常被用作备选数据集。ImageNet 聚焦于目标识别，拥有情景理解的更广泛的语境。组织者主办了一场包括目标检测、分割和关键点标注的年度挑战赛。在 ILSVRC[37] 和 COCO[38] 上进行的目标检测挑战赛的结果如下：

- ImageNet LSVRC 图像目标检测（DET）：CUImage 66% 平均准确率，在 200 个类别中有 109 个胜出。
- ImageNet LSVRC 视频目标检测（VID）：NUIST 80.8% 平均准确率。
- ImageNet LSVRC 视频追踪目标检测：CUvideo 55.8% 平均准确率。
- COCO 2016 目标检测挑战赛（边界框）：G-RMI（谷歌）41.5% 平均准确率（比 2015 的胜者 MSRAVC 高出 4.2% 绝对百分点）。

- 从以上结果可以看出，在 ImageNet 上的结果表明「MSRAVC 2015 的结果为『引入 ResNet』设置了很高的标准。在整个项目中对所有的类别的目标检测性能都有所提升。在两个挑战赛中，定位任务的性能都得到较大的提升。关于小型目标实例的大幅性能提升结果详见参考文献」（ImageNet,2016）。

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch2/5.jpg" width="600"/>
</div>

图 5.ILSVRC 的图像目标检测结果（2013-2016），来源 ImageNet. 2016. [Online] Workshop


#### 目标跟踪

- 目标追踪即在给定的场景中追踪感兴趣的一个或多个特定目标的过程，在视频和现实世界的交互中（通常是从追踪初始的目标检测开始的）有很多应用，且对于自动驾驶而言非常重要。
- Fully-Convolutional Siamese Networks for Object Tracking[40]，将一个连体网络（Siamese network）结合一个基础的追踪算法，使用端到端的训练方法，达到了当前最佳，图框显示率超过了实时应用的需求。这篇论文利用传统在线学习方法构建追踪模型。
- Learning to Track at 100 FPS with Deep Regression Networks[41]，该论文试图改善在线训练方法中存在的缺陷。他们构建了一个使用前馈网络学习目标运动、外观和方向中的普遍关系的追踪器，从而可以在没有在线训练的情况下有效地追踪到新的目标。该算法在一个标准的追踪基准测试中达到了当前最佳，同时可以 100FPS 的帧数追踪所有的目标（Held et al.,2016）。
- Deep Motion Features for Visual Tracking[43] 结合了手工设计的特征、深度外观特征（利用 CNN）和深度运动特征（在光流图像上训练），并取得了当前最佳的结果。虽然深度运动特征在动作识别和视频分类中很常见，但作者声称这是其首次被应用于视觉追踪上。该论文获得了 ICPR2016 的「计算机视觉和机器人视觉」的最佳论文。
- 「本论文展示了深度运动特征（motion features）对检测和追踪框架的影响。我们还进一步说明了手工制作的特征、深度 RGB 和深度运用特征包含互补信息。据我们所知，这是第一个提出融合外表信息和深度运动特征，并用于视觉追踪的研究。我们全面的实验表明融合方法具有深度运动特征，并超过了单纯依赖外表信息的方法。」
- Virtual Worlds as Proxy for Multi-Object Tracking Analysis [44] 方法解决了现有虚拟世界中缺乏真实可变性视频追踪基准和数据集。该论文提出了一种新的真实世界复制方法，该方法从头开始生成丰富、虚拟、合成和照片逼真的环境。此外，该方法还能克服现有数据集中存在的一些内容匮乏问题。生成的图像能自动通过正确的真值进行标注，并允许应用于除目标检测/追踪外其它如光流等任务。
- Globally Optimal Object Tracking with Fully Convolutional Networks [45] 专注处理目标变化和遮挡，并将它们作为目标追踪的两个根本限制。「我们提出的方法通过使用全卷积网络解决物体或目标外表的变化，还通过动态规划的方法解决遮挡情况」(Lee et al., 2016)。


### 3.2 进阶的计算机视觉任务

#### 图像分割

- 分割的过程是计算机视觉的核心，它将整个图像分为像素组，然后可以对其进行标记和分类。此外，语义分割通过尝试在语义上理解图像中每个像素的作用而进一步发展，例如图像。是猫，汽车还是其他类型的课程？实例细分通过细分不同的类实例（例如，用三种不同的颜色标记三只不同的狗。它是目前在自动驾驶技术套件中使用的计算机视觉应用程序的一大群。
- 分割领域中一些最好的改进也许要归功于FAIR，他们从2015年开始继续在DeepMask的工作基础上发展。[46] DeepMask在物体上生成粗糙的“蒙版”，作为分割的初始形式。2016年，Fair推出了SharpMask [47]，它完善了DeepMask提供的“蒙版”，纠正了细节损失并改善了语义分割。除此之外，MultiPathNet [48]标识每个掩码所描绘的对象。
- “要捕获一般的对象形状，您必须对要查看的内容（DeepMask）有一个高层次的了解，但是要准确地放置边界，您需要一路回顾一下较低层的特征，直到像素为止（SharpMask）。”-Piotr Dollar，2016年。


<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch2/6.jpg" width="600"/>
</div>

FAIR实验室的实际应用。
注意：上面的图片演示了FAIR使用的分割技术。这些包括按顺序应用的DeepMask，SharpMask和MultiPathNet技术的应用。该过程允许在各种场景中进行准确的分割和分类。

#### 超分辨率，风格迁移与着色

- 并非所有的计算机视觉研究都旨在扩展机器的伪认知能力，并且神经网络以及其他机器学习技术的寓言般的可延展性常常使它们适用于大量涌入公共空间的其他新颖应用。去年在超分辨率，样式转换和色彩化方面的进步为我们占据了这个空间。
- 超分辨率是指从低分辨率对应对象估计高分辨率图像的过程，还指在不同放大倍数下预测图像特征的过程，这是人脑几乎可以毫不费力地完成的工作。
- 最初，超分辨率是通过简单的技术（如双三次插值法和最近邻法）执行的。在商业应用方面，克服源质量和实现“ CSI Miami”样式图像增强带来的低分辨率限制的愿望推动了该领域的研究。以下是今年的一些进步及其潜在影响：神经增强是Alex J. Champandard的创意，结合了四篇不同研究论文的方法来实现其超分辨率方法。
- 2016年，在两个显着实例中还尝试了实时视频超分辨率。
- RAISR：Google提供的快速准确的图像超分辨率，通过训练具有低分辨率和高分辨率图像对的滤波器，避免了神经网络方法对内存和速度的昂贵要求。与基于神经网络的方法相比，RAISR作为一种基于学习的框架，比竞争算法快两个数量级，并且对内存的要求最小。因此，超分辨率可扩展到个人设备。这里有一个研究博客。

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch2/7.jpg" width="600"/>
</div>
注意：从左至右：双三次插值（客观上表现最差的目标），针对MSE优化的深层残差网络，针对对人类感知更敏感的损失而优化的深层残差生成对抗网络，原始高分辨率（HR）图像。相应的峰值信噪比（PSNR）和结构相似度（SSIM）显示在两个括号中。[4 x放大]读者可能希望放大中间的两个图像（SRResNet和SRGAN），以查看图像平滑度与更逼真的细节之间的差异。

- 生成对抗网络（GAN）的使用代表了当前的超分辨率SOTA：
- SRGAN [71]使用经过甄别以区分超分辨率和原始照片级逼真的图像的鉴别器网络，在公共基准上从大量降采样的图像中提供照片级逼真的纹理。在质量上，SRGAN表现最好，尽管SRResNet在峰值信噪比（PSNR）指标上表现最好，但是SRGAN获得了更好的纹理细节，并获得了最佳的平均意见评分（MOS）。
- “据我们所知，它是第一个能够针对4倍放大因子推断出逼真的自然图像的框架。” [72]所有先前的方法都无法在较大的放大因子下恢复出更精细的纹理细节。
- 图像超分辨率的摊销MAP推论[73]提出了一种使用卷积神经网络计算最大后验（MAP）推论的方法。但是，他们的研究提出了三种优化方法，目前，所有GAN在实际图像数据上的表现都明显更好。

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch2/8.jpg" width="600"/>
</div>

- 毫无疑问，Style Transfer对神经网络的一种新颖用法进行了深入研究，这种用法已逐渐进入公共领域，特别是通过去年的Facebook整合以及Prisma [74]和Artomatix [75]等公司。风格转移是一种较旧的技术，但随着“艺术风格的神经算法”的发布，在2015年转换为神经网络。[76]从那时起，样式转换的概念就由Nikulin和Novak进行了扩展[77]，并且也应用于视频[78]，这是Computer Vision的共同进步。

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch2/9.jpg" width="600"/>
</div>

注意：第一行（从左到右）代表艺术风格，该艺术风格被移到第一列（女人，金门大桥和草地环境）中显示的原始图像上。使用条件实例规范化，单个样式传输网络可以同时捕获32个样式，此处显示五个。原始图片的附录中提供了全套图片。这项工作将在2017年国际学习代表大会（ICLR）中进行介绍。


- 一旦可视化，样式转换作为一个主题就相当直观。拍摄一张图片，并想象一下另一张图片的风格特征。例如，以著名绘画或艺术家的风格。今年，Facebook发布了Caffe2Go，[80]他们的深度学习系统已集成到移动设备中。谷歌还发布了一些有趣的工作，试图融合多种样式以生成完全独特的图像样式：研究博客[81]和全文。
- 除了移动集成外，样式转移还可以用于创建游戏资产。我们团队的成员最近看到了Artomatix的创始人兼首席技术官Eric Risser的演讲，他讨论了该技术在游戏中生成内容（纹理变异等）的新颖应用，因此极大地减少了传统纹理艺术家的工作。
- 彩色化是将单色图像更改为新的全彩色版本的过程。最初，这是由精心选择颜色以表示每个图像中特定像素的人手动完成的。在2016年，实现这一过程的自动化，同时又保持了以人为本的着色过程的真实感。尽管人类可能无法准确地表示给定场景的真实色彩，但他们的现实世界知识允许以与图像一致的方式应用颜色，而另一个人则可以查看该图像。
- 着色的过程很有趣，因为网络会根据对对象位置，纹理和环境（例如颜色）的理解为图像分配最可能的颜色。它了解到皮肤是粉红色的，而天空是蓝色的。

- 年度最具影响力的三部作品如下：
- 张等。[83]产生了一种方法，能够在32％的试验中成功地欺骗人类。他们的方法与“着色图灵测试”相当。
- Larsson等人[84]使用深度学习直方图估算功能完全实现其图像着色系统的自动化。
- 最后，Lizuka，Simo-Serra和Ishikawa [85]展示了一种基于CNN的着色模型。这项工作的表现胜过现有的SOTA，我们（团队）认为这项工作在质量上似乎也是最好的，似乎是最现实的。图10提供了比较，但是图像取自Lizuka等人。


<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch2/10.jpg" width="600"/>
</div>

“此外，与大多数基于CNN的现有方法不同，我们的体系结构可以处理任何分辨率的图像。”在一项测试中，他们的色彩自然程度如何，向用户提供了来自其模型的随机图像，并被询问：“该图像对您来说看起来自然吗？”他们的方法达到了92.6％，基线达到了大约70％，并且地面真相（实际彩色照片）被认为是97.7％的时间是自然的。

#### 动作识别
- 动作识别的任务既指给定视频帧中动作的分类，又指的是在动作发生前仅给出几个帧就可以预测交互作用可能结果的算法。在这方面，我们看到最近的研究尝试将上下文嵌入算法决策中，类似于“计算机视觉”的其他领域。该领域的一些关键论文是：
- 动作识别的长期时间卷积[87]利用人类动作的时空结构（即特定的动作和持续时间）来使用CNN变体正确地识别动作。为了克服CNN进行的长期动作的次优时间建模，作者提出了一种具有长期时空卷积（LTC-CNN）的神经网络，以提高动作识别的准确性。简而言之，LTC可以查看视频的较大部分以识别动作。他们的方法使用并扩展了3D CNN，“从而可以在更大的时间尺度上进行动作表示”。
- “我们在两个具有挑战性的基准上报告了有关人类行为识别的最新结果：UCF101（92.7％）和HMDB51（67.2％）。”
- 用于视频动作识别的时空残留网络[88]将两种流CNN的变体应用于动作识别任务，该方法结合了来自传统CNN方法和最近流行的残留网络（ResNets）的技术。两流方法的灵感来自对视觉皮层功能的神经科学假设，即独立的路径识别物体的形状/颜色和运动。作者通过在两个CNN流之间注入残留连接来结合ResNets的分类优势。
- “每个流最初都会自行执行视频识别，对于最终分类，softmax分数将通过后期融合进行组合。迄今为止，这种方法是将深度学习应用于动作识别的最有效方法，尤其是在培训数据有限的情况下。在我们的工作中，我们将图像ConvNets直接转换为3D架构，并在两流基线上显示出大大提高的性能。”-UCF101为94％，HMDB51为70.6％。Feichtenhofer等。相对于传统的改进的密集轨迹（iDT）方法进行了改进，并通过使用这两种技术产生了更好的结果。尽管没有严格的动作分类，但从无标签视频中预测视觉表现[89]是一件有趣的论文。该程序会预测一个动作，如果在动作之前一秒钟内出现一系列视频帧，则该动作很可能会发生。该方法使用视觉表示，而不是逐像素分类，这意味着程序可以利用深层神经网络的特征学习特性来在没有标记数据的情况下运行。[90]
- “我们方法背后的关键思想是，我们可以训练深度网络来预测未来的图像视觉表示。视觉表示是一种很有前途的预测目标，因为它们以比像素更高的语义级别对图像进行编码，并且可以自动计算。我们然后将识别算法应用到我们的预测表示中，以预测对象和动作”。
- Thumos行动识别挑战赛[91]的组织者发表了一篇论文，描述了最近几年来行动识别的一般方法。本文还提供了2013-2015年挑战的摘要，挑战的未来方向以及如何通过动作识别使计算机更全面地了解视频的想法。


### 3.3 从三维角度对世界理解

- “计算机视觉的主要目标是从世界的2D观测中恢复底层3D结构。”-Rezende等。
- 如我们所见，在《计算机视觉》中，场景，对象和活动的分类以及边界框的输出和图像分割是许多新研究的重点。本质上，这些方法应用计算来获得图像的2D空间的“理解”。但是，批评者指出3D理解对于系统成功地解释和导航现实世界至关重要。
- 例如，网络可以在图像中定位一只猫，为其所有像素着色，然后将其分类为猫。但是，在猫的环境范围内，网络能否完全理解图中猫的位置？
- 有人可能会说计算机从上述任务中对3D世界的了解很少。与此相反，即使检查2D图片（例如视角，遮挡，深度，场景中的对象之间的关系等），人类也可以理解3D世界。将这些3D表示及其相关知识赋予人造系统代表着下一个伟大的成就之一计算机视觉前沿。普遍认为这是一个主要原因。
- “场景的2D投影是构成场景的摄像机，灯光和对象的属性和位置的复杂功能。如果具备3D理解能力，特工可以从这种复杂性中抽象出来，以形成稳定的，无纠结的表示形式，例如，认识到椅子是从上方或侧面，在不同光照条件下或在部分遮挡下看成是椅子。” 
- 但是，传统上3D理解面临许多障碍。第一个问题涉及``自身和正常遮挡''的问题以及适合给定2D表示的众多3D形状。由于无法将相同结构的不同图像映射到同一3D空间，并且无法处理这些表示形式的多模态，因此进一步加深了理解问题。[94]最后，真实的3D数据集传统上非常昂贵且难以获得，当与代表3D结构的不同方法结合使用时，可能会导致训练受限。
- 我们认为在此空间中进行的工作很重要，请谨记。从初期的，尽管令人激动的早期理论应用到未来的AGI系统和机器人技术，到沉浸式的，引人入胜的，在增强，虚拟和混合现实中令人着迷的应用，它们将在不久的将来影响我们的社会。由于获利丰厚的商业应用，我们谨慎地预测Computer Vision这一领域的指数增长，这意味着很快计算机可能会开始推理世界，而不仅仅是像素。

#### 三维物体
- 第一部分是散布的tad，用作应用于以3D数据表示的对象的计算，从2D图像推断3D对象形状和姿势估计的全部内容；从2D图像确定对象的3D姿势的变换。[95]重建的过程也紧接在接下来的章节的前面。但是，考虑到这些要点，我们提出了在这个一般领域最令我们的团队兴奋的工作：
- OctNet：以高分辨率学习深度3D表示[96]，延续了使用3D卷积对3D数据或Voxel（类似于3D像素）进行操作的卷积网络的最新发展。OctNet是“一种新颖的3D表示形式，可利用高分辨率输入使深度学习变得容易”。作者通过“分析分辨率对几个3D任务的影响（包括3D对象分类，方向估计和点云标记）来测试OctNet表示形式。本文的主要贡献是它利用了3D输入数据的稀疏性，从而可以更有效地利用内存和计算。
- ObjectNet3D：用于3D对象识别的大型数据库[97]-为3D对象识别提供数据库，可显示100种对象类别的2D图像和3D形状。“我们数据库中的图像中的对象（取自ImageNet）与3D形状（取自ShapeNet存储库）对齐，并且对齐方式为每个2D对象提供了精确的3D姿态标注和最接近的3D形状标注。”基线实验提供以下内容：区域提议生成，2D对象检测，联合2D检测和3D对象姿态估计以及基于图像的3D形状检索。
- 3D-R2N2：一种用于单视图和多视图3D对象重建的统一方法[98]-创建对象的重建“以3D占用网格的形式，使用任意角度的对象实例的单个或多个图像。”物体到3D形状的图像主要是使用合成数据学习的，并且网络可以进行训练和测试，而无需“任何图像注释或物体类别标签”。该网络包括一个2D-CNN，一个3D卷积LSTM（专门为特定目的而创建的体系结构）和一个3D反卷积神经网络。这些不同的组件如何交互以及端到端一起进行培训是神经网络功能分层的完美例证。


<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch2/11.jpg" width="600"/>
</div>


- 3D-R2N2使用ShapeNet模型生成“渲染的图像和体素化的模型”，并促进3D对象重建，其中运动（SfM）结构和同时定位和映射（SLAM）方法通常会失败：
- “我们广泛的实验分析表明，我们的重建框架i）优于单视图重建的最新方法，并且ii）在传统SFM / SLAM方法失败的情况下实现对象的3D重建。”
- 从多个对象的2D视图中进行3D形状归纳使用“投射生成对抗网络”（PrGAN），该网络训练了一个深度生成模型，可以准确表示3D形状，而鉴别器仅显示2D图像。投影模块捕获3D表示并将其转换为2D图像，然后再传递给鉴别器。通过迭代训练周期，生成器通过改进生成的3D体素形状来改善投影。

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/ch2/12.jpg" width="600"/>
</div>

- 在这种方式下，可以通过无人监督的环境来学习推理能力：
- “增加了投影模块，使我们可以推断基本的3D形状分布，而在学习阶段无需使用任何3D，视点信息或注释。”
- 另外，可以对形状的内部表示进行插值，这意味着体素形状中的离散共性允许从一个对象到另一个对象的变换，例如从汽车到飞机。
- 从图像对3D结构进行无监督学习提出了一种完全无监督的生成模型，该模型首次证明了“学习推断世界3D表示的可行性”。简而言之，DeepMind团队提出了一个模型，该模型“学习3D结构的强大的深度生成模型，并通过概率推断从3D和2D图像中恢复这些结构”，这意味着输入可以是3D和2D。DeepMind强大的生成模型可在基于体积和基于网格的表示形式上运行。在OpenGL中使用基于网格的表示形式可以内置更多知识，例如光线如何影响场景和使用的材料。“在循环中使用基于3D网格的表示法和完整的黑匣子渲染器进行训练，可以了解对象的颜色，材质和纹理，灯光的位置以及其他对象之间的相互作用。” 
- 这些模型具有高质量，捕获不确定性并且可以进行概率推断，从而可以在3D生成和仿真中应用。该团队在3D MNIST和ShapeNet上实现了3D密度建模的第一个定量基准。这种方法表明，可以在2D图像上无监督地对模型进行端到端训练，不需要真实的3D标签。

#### 人体姿态估计










- 当然，机器学习的出现其实是伴随着一个必要条件出现的，就是在2000年左右，整个互联网的出现和爆发，产生了海量的数据，大规模数据集也相伴而生，这为通过机器学习的方法来做计算机视觉提供了很好的土壤。在这期间，出现了大量学术官方的，针对不同领域评测的数据集。比如像刚刚谈到的人脸检测，这里面最具有代表性是一个叫FDDB的数据集。这个数据集包含了超过5000多张人脸数据，每一张人脸，都人为的用框给框出来，机器就可以从这些框好的数据里面，通过一些机器学习的手段去学习从任何一张图片中找到人脸区域。
- 另外，大家可能耳熟能详的一个数据集叫LFW。提到人脸识别，大家可能都会听到一句话，说人脸识别比人做得都要准确。其实这句话是有一定问题的。在很多的实际场景里面，人脸识别连人的百分之六七十可能都达不到。那么，在什么情况下，它比人准呢？其实就是在LFW上。LFW数据集里面有1万多张人脸和5000多个人的数据，每个人都有在不同情况下拍摄的真实场景的多张人脸。基于这样的一个数据集，我们可以在上面评估人脸识别的精度。人脸识别领域发展非常快，尤其是最近几年深度学习的兴起后，这个数据基本上快被刷爆了。人在LFW上面的识别正确率大概是97.75%，而机器已经在LFW上可以做到99.75%甚至更要高。
- 在这期间，还出现了其他非常有影响力的数据集，其中比较有代表性的就是IMAGEMET。这是由非常著名的教授李飞飞发起的一个项目，她通过众包的方式，大概标注1400万张图片，分了大概2万多个类别，这些类别包罗万物，比如像动物，里边可能分为鸟类、鱼类等；植物，里面可能会分为树和花。她的理想很宏大，就是希望提供这样一个数据集，为计算机视觉算法提供一个数据土壤，让未来的机器能够认识世界上的一切物品。
- 到2010年代，进入了一个最激动人心的年代，它就是深度学习的年代。深度学习从本质上给整个人工智能带来了一次革命。在80年代，有人做了一个猫的实验。他们在猫脑袋上开了一个洞，然后在猫的前面放各种各样的物品，观察猫在不同物品下的反应。通过实验发现，当我们放一些形状非常类似的物品时，猫会表现出同样的刺激反应，这就表明了人的认知是分层的。在早期多层神经网络经历2000年左右的一个低谷后，2006年Hinton教授在《科学》发表了对于深层神经网络的训练方法，带来了深度学习的蓬勃发展。

### 3.3市场现状

- 学术上的突破带来了商业上的繁荣，从2010年开始，计算机视觉开始成为国内外各个公司关注的焦点，不管是初创企业还是巨头纷纷开始布局。

- 初创企业方面，国内计算机视觉创业热度递增且深入行业，但整体仍处于早期阶段。
- 巨头方面，国外巨头采用自研和收购双管齐下的方式，将视觉技术广泛应用于自身产品升级，并基于自身基因打造技术服务平台和新品类持续提升影响力。
- 国内巨头百度相对激进，阿里巴巴、腾讯基于自身产品进行功能试水

### 3.4技术应用场景

#### 安全/娱乐/营销成最抢先落地的商业化领域

- 计算机视觉技术已经步入应用早期阶段，不仅渗透到传统领域的升级过程中，还作为最重要的基础人工智能技术参与到前沿创新的研究中。其中，计算机对静态内容的识别应用主要体现在搜索变革和照片管理等基础服务层面，意在提升产品体验；伴随内容形式的变迁（文字→图片→视频），动态内容识别的需求愈加旺盛，安全、娱乐、营销成为最先落地的商业化领域。

- 这三类领域均有一定的产业痛点，且均是视频内容产出的重地，数据体量巨大，适合利用深度学习的方式予以改进。与此同时，行业潜在的商业变现空间也是吸引创业者参与的重要原因。

- 另一方面，当前计算机视觉主要应用于二维信息的识别，研究者们还在积极探索计算机对三维空间的感知能力，以提高识别深度。
<!-- <br></br> -->
<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/cv1/16cv应用场景.jpg" width="600"/>
</div>

#### 从软硬件两个层面优化安防人员的作业效率和深度

- 安防是环境最为复杂的应用领域，通常的应用场景以识别犯罪嫌疑人、目标车辆（含套牌车/假牌车）以及真实环境中的异常为主。

- 传统安防产品主要功能在于录像收录，只能为安防人员在事后取证的环节提供可能的线索，且需要人工进行反复地逐帧排查，耗时耗力；智能安防则是将视频内容结构化处理，通过大数据分析平台进行智能识别搜索，大大简化了工作难度，提高工作效率。

- 计算机视觉的应用从行业痛点出发，以软硬件的方式大大优化了安防人员的作业效率与参考深度，是顺应行业升级的利好。不过，在实际应用过程中，对公安、交警、金融等常见安防需求方而言，更强的视觉识别效果往往意味着更多基础成本（存储、带宽等）的投入，安防厂商的未来将不只以技术高低作为唯一衡量标准，产品的实用性能与性价比的平衡才是进行突围、实现量产的根本，因此市场除了有巨大的应用空间外，还会引发一定的底层创新。

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/cv1/16cv优化.jpg" width="600"/>
</div>

#### 有效迎合直播平台的前端用户体验和后端监管要求

- 直播平台的爆红丰富了网民的娱乐生活方式，产生的海量内容也为平台的监管造成了巨大的压力，传统人工审核效果不稳定，基于深度学习的图像识别平台可以有效缓解这一痛点，同时也可对前端的内容运营进行优化，提高用户体验和活跃度。
- 直播平台为计算机视觉创造了新的应用场景，伴随行业的发展，除刚需之外，平台定制化、差异化的需求也会为计算机视觉应用提供更多的增量空间。另外，与直播的UGC性质类似，其他的应用场景还有长短视频平台、社交平台、云存储平台、CDN以及社区平台。

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/cv1/16cv个案.jpg" width="600"/>
</div>

#### 促进视频环境中广告主和用户间交互闭环的落地

- 挖掘视频中广告位及视频电商购物一直是视频平台作为中间方探索营销创收的新方式，此前以人工贴标、投放的方式实现，因效率低下仅作为小范围试水。

- 计算机视觉将加速这一探索的进程，除技术成熟度之外，用户体验和用户习惯（从PC端向移动端迁移）也是对应用程度比较重要的影响因素，需要合理设计及长期培育。

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/cv1/16cv广告位.jpg" width="600"/>
</div>

#### 逐步拓展服务和工商业等多重应用场景

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/cv1/16cv多场景.jpg" width="600"/>
</div>


#### 与其他技术融合共同推动创新型行业应用的发展

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/cv1/16cv多场景2.jpg" width="600"/>
</div>

### 3.5问题引入——图像分类

- 这是计算机视觉中真正核心的任务，通常我们期望，计算机得到我们输入的一张图片后，能够给这张图片打上标签，究竟是猫狗，还是汽车，还有一些固定类别标签集合。对于人来说，这是一个很简单的问题，因为大家的视觉系统中，天生就是能够达成这些视觉任务的。但是对机器而言，是非常困难的问题，如果你深入思考，当计算机看到这些图片的时候，它看到的是什么，他肯定是没有一只猫咪的概念，而不像你们看着的时候，计算机呈现图片的方式其实就是一大堆的数字。
<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/cv1/cat.jpg" width="600"/>
</div>

- 每个像素对应三个数字，是一个巨大的数字阵列
- 同时，面临着角度，照明，形变，姿势和位置，遮挡，复杂背景等挑战，对于人的认识是非常简单的事情，但是对于电脑一个算法去鲁棒识别这世间万物是一件比较困难的 事情，但是现在已经有比较好的算法能够帮助我们去达成这件事情了。


## 4. 任务实施

### 4.1 实施思路

- 如果你上算法课，遇到排序，凸包，最优路径，你可以用特定，简洁明了的的方法去解决这些问题，但是对于分类，并没有简洁明了的方法去直接完成这些识别。

- 我们可以写一些规则算法去识别一只猫，写一些规则，但是这种方法并不好，很容易出错，更何况我们并不单只要识别猫，如果还有其他的许许多多的类别。

- 基于此，我们可能想到采取数据驱动的方法，不写具体分类规则去识别一只猫或鱼，取而代之，在网络上收集猫鱼等物体的大量数据，譬如用搜索引擎搜索，用爬虫爬取，收集到大量的数据，形成不同类别的示例图，这种方法其实挺耗费时间的，好在现在已经有好多很高质量可用的图片数据集，一旦有了数据集，我们可以通过训练机器去帮我们完成分类的任务（机器会收集所有数据，用某种方式总结，然后生成一个模型，总结识别出这些不同的类的对象的核心知识要素），然后用模型去帮我们识别新的物体，看看分类机器能不能真实生效。

- 所以我们的接口是这样的，写一个函数，不仅仅是输入图片，然后识别它是一只猫，我们会有两个函数，一个是训练函数，这函数接受图片和标签，然后输出模型，另一种是预测函数，接收一个模型，对图片种类进行预测。

- 正是用了这种方法，在过去的十几二十年间，图像识别领域进步非常大，虽然我们主要讨论卷积神经网络和深度学习，但这种数据驱动类的算法是比深度学习更广义的一种理念。

### 4.2 实施步骤
#### 步骤1：
- 通过这个过程，对于一个简单的分类器，也许是最简单的分类器，成为我们最邻近，这种算法像诚实的默者，在训练机器过程中，我们什么也不做。
<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/cv1/o1.jpg" width="600"/>
</div>

- 我们只是单纯记录所有的训练数据，this is simple!
- 预测阶段，我们会拿一些新图片去在训练数据中，寻找与新突破最相似的，然后基于此，来给出一个标签。
- 看似简单的算法，这里很多所用到的属性，都是数据驱动的。

#### 步骤2：
- 譬如一个CIFAR-10的数据集。
  
<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/cv1/o2.jpg" width="600"/>
</div>

- 所以当我们将最邻近算法用于这些图片，我们就能在训练集中找到最接近的示例，因为它们来自训练集，因此我们知道这些最接近示例的标签。

#### 步骤3：
- 我们需要知道一个细节，如果给出两幅图片，我们应该怎样对他们进行比较，因为如果我们要将测试图片与所有训练图片进行比较，我们将有很多不同的选择，来确定需要什么样的比较函数，所以上一个图中，我们使用了L1距离，，有时我们也称其为曼哈顿距离。
  
<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/cv1/o3.jpg" width="600"/>
</div>

- 这是一个简单易行的方法，只对图片中的单个像素进行比较，如图，假设我们的测试图片只是4x4的小图片，我们只取图片左上角的像素减去训练图像对应像素的值，然后取绝对值，得到这两幅图像这个像素的差别，将图像中所有像素的这个差值相加，看似笨方法，但某些场景也存在其合理性，给出了我们对比两张图像的具体方法。这个例子中，两张图片有456处不同。

#### 步骤4：

<div align=center>
    <!-- ![应用场景](./img/16cv应用场景.jpg) -->
    <img src="./img/cv1/o4.jpg" width="600"/>
</div>

- 这是最邻近算法的全部代码，看似非常简洁，因为我们采用了numpy的向量计算，存储训练数据，然后用L1距离函数，将我们的测试图片与训练实例进行比较，然后在训练集中找的到最相似的实例，这些向量化的操作，只需要一两行的python代码就可以实现。


## 5.任务拓展
### 5.1 KNN和SVM的区别

- KNN对每个样本都要考虑。SVM是要去找一个函数把达到样本可分。
- 朴素的KNN是不会去自助学习特征权重的，SVN的本质就是在找权重。
- KNN不能处理样本维度太高的东西，SVM处理高纬度数据比较优秀。


## 6. 任务实训
### 6.1 实训目的
- 深入理解KNN算法
- 对KNN算法的超参数进行实操调整

### 6.2 实训内容
- 通过使用一个循环使用部分矢量化来加快距离矩阵的计算。 实现函数compute_distances_one_loop。
- 执行k倍交叉验证，以找到k的最佳值。 对于k的每个可能值，运行k_nearest-neighbor算法num_folds次，在每种情况下，您将除其中一个折叠以外的所有折叠用作训练数据，将最后一个折叠用作验证集。 将所有折叠和所有k值的精度存储在k_to_accuracies字典中。

### 6.3 示例代码



```
# Now lets speed up distance matrix computation by using partial vectorization
# with one loop. Implement the function compute_distances_one_loop and run the
# code below:
dists_one = classifier.compute_distances_one_loop(X_test)

# To ensure that our vectorized implementation is correct, we make sure that it
# agrees with the naive implementation. There are many ways to decide whether
# two matrices are similar; one of the simplest is the Frobenius norm. In case
# you haven't seen it before, the Frobenius norm of two matrices is the square
# root of the squared sum of differences of all elements; in other words, reshape
# the matrices into vectors and compute the Euclidean distance between them.
difference = np.linalg.norm(dists - dists_one, ord='fro')
print('Difference was: %f' % (difference, ))
if difference < 0.001:
    print('Good! The distance matrices are the same')
else:
    print('Uh-oh! The distance matrices are different')
#     robenius norm. In case
# you haven't seen it before, the Frobenius norm of two 



# Let's compare how fast the implementations are
def time_function(f, *args):
    """
    Call a function f with args and return the time (in seconds) that it took to execute.
    """
    import time
    tic = time.time()
    f(*args)
    toc = time.time()
    return toc - tic

two_loop_time = time_function(classifier.compute_distances_two_loops, X_test)
print('Two loop version took %f seconds' % two_loop_time)

one_loop_time = time_function(classifier.compute_distances_one_loop, X_test)
print('One loop version took %f seconds' % one_loop_time)

no_loop_time = time_function(classifier.compute_distances_no_loops, X_test)
print('No loop version took %f seconds' % no_loop_time)

# you should see significantly faster performance with the fully vectorized implementation



num_folds = 5
k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]

X_train_folds = []
y_train_folds = []
################################################################################
# TODO:                                                                        #
# Split up the training data into folds. After splitting, X_train_folds and    #
# y_train_folds should each be lists of length num_folds, where                #
# y_train_folds[i] is the label vector for the points in X_train_folds[i].     #
# Hint: Look up the numpy array_split function.                                #
################################################################################
X_train_folds=np.array_split(X_train,num_folds)
y_train_folds=np.array_split(y_train,num_folds)
################################################################################
#                                 END OF YOUR CODE                             #
################################################################################

# A dictionary holding the accuracies for different values of k that we find
# when running cross-validation. After running cross-validation,
# k_to_accuracies[k] should be a list of length num_folds giving the different
# accuracy values that we found when using that value of k.
k_to_accuracies = {}
for i in k_choices:
    k_to_accuracies[i]=[]
################################################################################
# TODO:                                                                        #
# Perform k-fold cross validation to find the best value of k. For each        #
# possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #
# where in each case you use all but one of the folds as training data and the #
# last fold as a validation set. Store the accuracies for all fold and all     #
# values of k in the k_to_accuracies dictionary.                               #
################################################################################
for ki in k_choices:
    for fi in range(num_folds):
        #prepare the data
        valindex=fi
        X_traini = np.vstack((X_train_folds[0:fi]+X_train_folds[fi+1:num_folds]))
        y_traini = np.hstack((y_train_folds[0:fi]+ y_train_folds[fi+1:num_folds]))

        X_vali=np.array(X_train_folds[valindex])
        y_vali = np.array(y_train_folds[valindex])
        num_val=len(y_vali)

        #initialize the KNN
        classifier = KNearestNeighbor()
        classifier.train(X_traini,y_traini)

        #calculate the accuracy
        dists = classifier.compute_distances_one_loop(X_vali)
        y_val_pred = classifier.predict_labels(dists, k=ki)
        num_correct = np.sum(y_val_pred == y_vali)
        accuracy = float(num_correct) / num_val
        print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))
        k_to_accuracies[ki].append(accuracy)
################################################################################
#                                 END OF YOUR CODE                             #
################################################################################

# Print out the computed accuracies
for k in sorted(k_to_accuracies):
    for accuracy in k_to_accuracies[k]:
        print('k = %d, accuracy = %f' % (k, accuracy))



# plot the raw observations
for k in k_choices:
    accuracies = k_to_accuracies[k]
    plt.scatter([k] * len(accuracies), accuracies)

# plot the trend line with error bars that correspond to standard deviation
accuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])
accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])
plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)
plt.title('Cross-validation on k')
plt.xlabel('k')
plt.ylabel('Cross-validation accuracy')
plt.show()


# Based on the cross-validation results above, choose the best value for k,   
# retrain the classifier using all the training data, and test it on the test
# data. You should be able to get above 28% accuracy on the test data.
best_k = 1

classifier = KNearestNeighbor()
classifier.train(X_train, y_train)
y_test_pred = classifier.predict(X_test, k=best_k)

# Compute and display the accuracy
num_correct = np.sum(y_test_pred == y_test)
accuracy = float(num_correct) / num_test
print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))


```