[TOC]

<!-- ref: https://lab.datafountain.cn/forum?id=113 -->

# ä»»åŠ¡9ï¼šå¤šå°ºåº¦ç›®æ ‡æ£€æµ‹

## 1.ä»»åŠ¡ç›®æ ‡

<!-- 1. 
2. 
3. 
4.  -->

- å›é¡¾å…ˆå‰ç›®æ ‡æ£€æµ‹è¯¾ç¨‹çš„åŸºç¡€
- è¿ç”¨ç›®æ ‡æ£€æµ‹çš„æ€æƒ³è¿›è¡Œå¤šå°ºåº¦ç›®æ ‡æ£€æµ‹
- åŠ¨æ‰‹å®ç°ä¸€ä¸ªå¤šå°ºåº¦çš„äººè„¸ç›®æ ‡æ£€æµ‹å™¨




## 2.ä»»åŠ¡æè¿°

æœ¬è¯¾ç¨‹æ—¨åœ¨ä»‹ç»å¦‚ä½•åˆ©ç”¨pytorchæ·±åº¦å­¦ä¹ å·¥å…·å®ç°ä¸€ä¸ªäººç¾¤è®¡æ•°æ¨¡å‹ï¼ˆMCNNï¼‰ã€‚é€šè¿‡åŠ è½½æ•°æ®ã€ç”Ÿæˆå¯†åº¦å›¾ã€æ„å»ºæ¨¡å‹ã€è®­ç»ƒæ¨¡å‹å’Œæµ‹è¯•ç”¨ä¾‹ä¾æ¬¡å®ç°ä¸€ä¸ªäººç¾¤è®¡æ•°å·¥å…·ï¼Œåœ¨è®­ç»ƒå’Œé¢„å¤„ç†è¿‡ç¨‹ä¸­é€šè¿‡å¯è§†åŒ–ç›‘ç£è®­ç»ƒè¿‡ç¨‹ã€‚ 


## 3.çŸ¥è¯†å‡†å¤‡


### 3.1 èƒŒæ™¯

è¿‘å¹´æ¥ï¼Œéšç€è¸©è¸äº‹ä»¶çš„ä¸æ–­å‘ç”Ÿï¼Œäººç¾¤è®¡æ•°çš„é—®é¢˜å—åˆ°äº†è¶Šæ¥è¶Šå¤šç ”ç©¶äººå‘˜çš„å…³æ³¨ã€‚äººç¾¤è®¡æ•°çš„æ–¹æ³•å¤§è‡´å¯ä»¥åˆ†ä¸ºä¸‰ç§ï¼š

ä»¥æ£€æµ‹ä¸ºåŸºç¡€ï¼Œå³é€šè¿‡å¯¹å›¾åƒä¸­æ¯ä¸€ä¸ªäººè¿›è¡Œæ£€æµ‹ä»è€Œå¾—åˆ°äººç¾¤æ•°ç›®ã€‚
ä»¥å›å½’ä¸ºåŸºç¡€ï¼Œå³é€šè¿‡å»ºç«‹å›¾åƒç‰¹å¾å’Œå›¾åƒäººæ•°çš„å›å½’æ¨¡å‹ï¼Œé€šè¿‡æµ‹é‡å›¾åƒç‰¹å¾ä»è€Œä¼°è®¡åœºæ™¯ä¸­çš„äººæ•°ã€‚

ä»¥å¯†åº¦å›¾ä¸ºåŸºç¡€ï¼Œè¿™ç±»æ–¹æ³•æ˜¯ç›®å‰äººç¾¤ç»Ÿè®¡çš„ä¸»æµæ–¹æ³•ã€‚ä¸åŸºäºæ£€æµ‹çš„æ–¹æ³•å’ŒåŸºäºå›å½’çš„æ–¹æ³•ç›¸æ¯”ï¼Œå¯†åº¦å›¾æ—¢èƒ½ç»™å‡ºè¡Œäººæ•°é‡ä¿¡æ¯ï¼Œåˆèƒ½åæ˜ è¡Œäººçš„åˆ†å¸ƒæƒ…å†µï¼Œä½¿æ¨¡å‹èƒ½æ›´å¥½åœ°æ‹Ÿåˆç›¸åº”çš„åŸå§‹å›¾åƒã€‚


### 3.2 æ•°æ®é›†

æ­¤æ¬¡è¯¾ç¨‹é‡‡ç”¨çš„æ•°æ®é›†ä¸º ShanghaiTech_Crowd_Counting_Datasetï¼Œç‚¹å‡»è¿™é‡Œè¿›è¡Œä¸‹è½½ã€‚å› ä¸ºç”Ÿæˆå¯†åº¦å›¾éœ€è¦ä¸€å®šæ—¶é—´ï¼Œæ‰€ä»¥æä¾›äº†åŒ…å« partA éƒ¨åˆ†å¯†åº¦å›¾çš„æ•°æ®é›†ã€‚

## 4. ä»»åŠ¡å®æ–½



### 4.1 å®æ–½æ€è·¯

<div align=center>
    <!-- ![åº”ç”¨åœºæ™¯](./img/16cvåº”ç”¨åœºæ™¯.jpg) -->
    <img src="./img/ch9/1.png" width="700"/>
</div>


### 4.2 å®æ–½æ­¥éª¤

ğŸ“¦root
<br>â”£ ğŸ“‚datasetsâ€ƒâ€ƒ # å®éªŒæ‰€ç”¨æ•°æ®é›†ä¸‹è½½å¹¶è§£å‹åˆ°å½“å‰ç›®å½•
<br>â”ƒâ€ƒâ”£ ğŸ“‚part_A_finalâ€ƒ # A éƒ¨åˆ†æ•°æ®é›†
<br>â”ƒâ€ƒâ”ƒâ€ƒâ”£ ğŸ“‚test_dataâ€ƒ
<br>â”ƒâ€ƒâ”ƒâ€ƒâ”— ğŸ“‚train_dataâ€ƒ
<br>â”ƒâ€ƒâ”— ğŸ“‚part_B_finalâ€ƒ # B éƒ¨åˆ†æ•°æ®é›†
<br>â”ƒâ€ƒ â€ƒâ”£ ğŸ“‚test_dataâ€ƒ
<br>â”ƒâ€ƒ â€ƒâ”— ğŸ“‚train_dataâ€ƒ
<br>â”£ ğŸ“‚picturesâ€ƒâ€ƒ # å­˜æ”¾ notebook ä¸­æ‰€éœ€å›¾ç‰‡
<br>â”£ ğŸ“‚resultâ€ƒâ€ƒ # å­˜æ”¾è¾“å‡ºçš„ç»“æœ
<br>â”£ ğŸ“‚modelsâ€ƒâ€ƒ # å­˜æ”¾è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ–­ç‚¹å’Œè®­ç»ƒå¥½çš„æ¨¡å‹
<br>â”£ ğŸ“œäººç¾¤è®¡æ•°åˆçº§æ¡ˆä¾‹.ipynbâ€ƒ # å½“å‰ notebook
<br>â”— ğŸ“œäººç¾¤è®¡æ•°åˆçº§æ¡ˆä¾‹-æ ‡å‡†æäº¤ä»¶æ¸…å•.xlsxâ€ƒ # æ¡ˆä¾‹é…å¥—ææ–™è¯´æ˜æ–‡æ¡£

#### æ­¥éª¤1ï¼šå¯¼å…¥ç›¸å…³åº“

```
# !pip install torchsummary tensorboardX

# ç»Ÿä¸€å¯¼å…¥packages

import glob
import os
import re

import cv2
import h5py
import matplotlib.pyplot as plt
import numpy as np
import PIL.Image as Image
import scipy
import scipy.ndimage
import scipy.io as io
import scipy.spatial as spatial
import torch
import torch.nn as nn
from torch.utils.data import Dataset
from torchsummary import summary
from tensorboardX import SummaryWriter

%matplotlib inline

```

```
torch.backends.cudnn.benchmark = False # å› ä¸ºè¾“å…¥å›¾ç‰‡å¤§å°ä¸ç¡®å®šï¼Œæ‰€ä»¥è®¾ç½®å…¶ä¸º Falseï¼Œä¸‹æ–‡æœ‰è¯¦ç»†ä»‹ç»
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # ä½¿ç”¨ gpu è¿›è¡Œè®­ç»ƒ
```

#### æ­¥éª¤2ï¼šæ•°æ®å‡†å¤‡

å› ä¸º MCNN æ–¹æ³•æ˜¯åŸºäºå¯†åº¦å›¾çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å…ˆå°† ShanghaiTech æ•°æ®ä¸­çš„ Ground-Truth åˆ¶ä½œæˆå¯†åº¦å›¾å½¢å¼ã€‚è¯¥å°èŠ‚æ•´ä¸ªæ•°æ®å‡†å¤‡çš„æµç¨‹å¯ä»¥åˆ†ä¸ºï¼š

1. æ•°æ®é›†å±•ç¤º
2. å¯†åº¦å›¾åˆ¶ä½œ
3. æ„é€  dataset ç±»
4. æ•°æ®åŠ è½½ dataloader 

##### æ•°æ®é›†å±•ç¤º
é€šè¿‡ 0.1 å°èŠ‚ä¸­ç»™çš„é“¾æ¥ï¼Œå°†æ•°æ®é›† ShanghaiTech_Crowd_Counting_Dataset ä¸‹è½½å¹¶è§£å‹åˆ° dataset æ ¹ç›®å½•ä¸‹ï¼Œæœ€ç»ˆæ•°æ®é›†ç›®å½•å¦‚ 0.2 å°èŠ‚æ‰€ç¤ºã€‚

è¯»å–æ•°æ®é›†ä¸­çš„æŸä¸ªæ ·æœ¬ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ä¸ºäº†æ–¹ä¾¿è‡ªç”±ä¿®æ”¹æŸ¥çœ‹ï¼Œå°†è·¯å¾„æ‹†åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†ã€‚
```
sample_idx = 6 # æ ·æœ¬ç¼–å·
sample_root = "datasets/part_A_final/train_data/" # æ ·æœ¬æ ¹ç›®å½•
img_path = os.path.join(sample_root, f"images/IMG_{sample_idx}.jpg") # å›¾ç‰‡
gt_mat_path = os.path.join(sample_root, f"ground_truth/GT_IMG_{sample_idx}.mat") # æ ‡ç­¾

img = plt.imread(img_path)
gt = io.loadmat(gt_mat_path)
```

å¯è§†åŒ–å›¾ç‰‡å¦‚ä¸‹å›¾æ‰€ç¤º:
```
plt.imshow(img);plt.show()
```

```
print(gt)
```
gt ä¸­åŒ…å«è¯¥å›¾ç‰‡çš„æ‰€æœ‰éœ€è¦çš„ä¿¡æ¯ï¼Œimage_info åŒ…å«æœ‰å›¾ç‰‡çš„æ¯ä¸ªäººå¤´çš„äºŒç»´åæ ‡ï¼Œä»¥åŠå›¾ç‰‡ä¸­äººå¤´çš„æ•°ç›®ï¼Œå°†å…¶åœ¨å›¾ä¸­æ ‡æ³¨å‡ºæ¥ï¼Œç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

```
xys = gt["image_info"][0][0][0][0][0] # è·å–å›¾åƒä¸­æ‰€æœ‰çš„äººå¤´ä½ç½®
xs = [i[0] for i in xys]; ys = [i[1] for i in xys] # åˆ†åˆ«è·å–æ¨ªåæ ‡å’Œçºµåæ ‡ï¼Œæ–¹ä¾¿å±•ç¤º
plt.imshow(img); plt.plot(xs,ys,'rx'); plt.show()
```

##### å¯†åº¦å›¾åˆ¶ä½œ
å‰æ–‡æåˆ°ï¼ŒMCNN æ˜¯åŸºäºå¯†åº¦å›¾æ¥è¿›è¡Œäººç¾¤è®¡æ•°çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æ ¹æ® .mat æ–‡ä»¶åˆ¶ä½œè®­ç»ƒæ‰€éœ€çš„å¯†åº¦å›¾ã€‚

å®šä¹‰å¯†åº¦å›¾è½¬æ¢çš„å‡½æ•° make_density å¦‚ä¸‹æ‰€ç¤ºï¼Œé‡‡ç”¨ k-nearest neighbors å’Œé«˜æ–¯æ»¤æ³¢ï¼Œå½“å›¾åƒä¸­äººæ•°è¾ƒå¤šæ—¶ï¼Œå¯¹åº”èŠ±è´¹æ—¶é—´ä¹Ÿè¾ƒä¹…ã€‚

k-nearest neighbors (KNN)å’Œä»£ç ä¸­æ‰€ä½¿ç”¨çš„çš„KDTreeçš„è¯¦ç»†è¯´æ˜å¯ä»¥å‚è€ƒæ­¤æ–‡

é«˜æ–¯æ»¤æ³¢ä»¥åŠå…¶å…·ä½“ä½¿ç”¨å¯ä»¥å‚è€ƒæ­¤æ–‡

ä¸ºäº†èŠ‚çœæ—¶é—´æˆæœ¬ï¼Œå·²å°† part_A éƒ¨åˆ†åˆ¶ä½œå®Œæ¯•å¹¶åŒ…å«åœ¨å‹ç¼©åŒ…ä¸­ã€‚

```
def make_density(img, points):
    img_shape = [img.shape[0],img.shape[1]] # è·å–å›¾åƒå½¢çŠ¶
    print(f"shape of imgs: {img_shape} , number of gaussian kernels: {len(points)}",end="\t")
    density = np.zeros(img_shape, dtype=np.float32) # åˆå§‹åŒ–ä¸€ä¸ªå…¨ 0 çŸ©é˜µ
    gt_count = len(points)
    if gt_count == 0: # å¦‚æœå›¾åƒä¸­æ²¡æœ‰äººï¼Œåˆ™è¿”å›å…¨ 0 çŸ©é˜µ
        return density

    leafsize = 2048
    # æ„å»º kdtree
    tree = spatial.KDTree(points.copy(), leafsize=leafsize)
    distances, locations = tree.query(points, k=4) # åœ¨è¿™é‡Œé€‰å– k ä¸º4

    print('processing...', end='\t')
    for i, pt in enumerate(points):
        pt2d = np.zeros(img_shape, dtype=np.float32)
        if int(pt[1]) < img_shape[0] and int(pt[0]) < img_shape[1]:
            pt2d[int(pt[1]), int(pt[0])] = 1.
        else:
            continue
        if gt_count > 1:
            sigma = (distances[i][1]+distances[i][2]+distances[i][3])*0.1
        else:
            sigma = np.average(np.array(gt.shape))/2./2.
        density += scipy.ndimage.filters.gaussian_filter(
            pt2d, sigma, mode='constant')
    print('done.')
    return density
```

è°ƒç”¨å‡½æ•°æ¥åˆ¶ä½œå¯†åº¦å›¾ï¼ˆpart_Aï¼‰ï¼Œæ³¨æ„è¿™é‡Œè¾“å‡ºåªä¿ç•™äº†å‰åé¡¹ã€‚

```
# æ ¹ç›®å½•
root = 'datasets/'

# æ€»çš„è·¯å¾„åˆ—è¡¨
path_sets = []
# æ·»åŠ è·¯å¾„åˆ—è¡¨ï¼ˆpart_Aï¼‰
path_sets.append(os.path.join(root, 'part_A_final/train_data', 'images'))
path_sets.append(os.path.join(root, 'part_A_final/test_data', 'images'))
# æ·»åŠ è·¯å¾„åˆ—è¡¨ï¼ˆpart_Bï¼‰
# path_sets.append(os.path.join(root,'part_B_final/train_data','images'))
# path_sets.append(os.path.join(root,'part_B_final/test_data','images'))

# ä¿å­˜è·¯å¾„
for save_root in path_sets:
    save_root = save_root.replace('datasets','temp').replace('images', 'ground_truth')
    if not os.path.exists(save_root):
        os.makedirs(save_root)

img_paths = []
for path in path_sets:
    for img_path in glob.glob(os.path.join(path, '*.jpg')): # è¿”å›æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        img_paths.append(img_path)

for idx,img_path in enumerate(img_paths):
    save_path = img_path.replace('.jpg', '.npy').replace('images', 'ground_truth').replace('datasets','temp')
    if os.path.exists(save_path):
        continue
    print(img_path, end='\t')
    mat = io.loadmat(img_path.replace('.jpg', '.mat').replace(
        'images', 'ground_truth').replace('IMG_', 'GT_IMG_'))
    img = plt.imread(img_path) 
    k = np.zeros((img.shape[0], img.shape[1]))
    points = mat["image_info"][0, 0][0, 0][0]
    k = make_density(img, points)
    np.save(save_path, k)
#     if idx>2:break #æ§åˆ¶æ—¶é—´ï¼Œå¯æ”¾å¼€
```


è½¬åŒ–åçš„å¯†åº¦å›¾å¦‚ä¸‹æ‰€ç¤ºï¼Œä¸ºäº†æ–¹ä¾¿è‡ªç”±ä¿®æ”¹æŸ¥çœ‹ï¼Œå°†è·¯å¾„æ‹†åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†ã€‚

```
density_idx = 138 # æ ·æœ¬ç¼–å·
density_root = "./temp/part_A_final/train_data/" # æ ·æœ¬æ ¹ç›®å½•
density_path = os.path.join(density_root, f"ground_truth/IMG_{density_idx}.npy") # å›¾ç‰‡
density = np.load(density_path)
plt.imshow(density); plt.show()
```


##### æ„é€ datasetç±»
æ„é€ ä¸€ä¸ª dataset ç±»ï¼Œç»§æ‰¿äº torch.utils.data.Datasetï¼Œæˆ‘ä»¬åªéœ€è¦é‡å†™ä»–çš„åˆå§‹åŒ–å‡½æ•° __init__ï¼Œè¿”å›æ•°æ®é›†é•¿åº¦çš„å‡½æ•° __len__ï¼Œä»¥åŠå¦‚ä½•è¯»å–æ¯ä¸€ä¸ªæ ·æœ¬çš„ __getitem__ã€‚

- __len__ ï¼šè¾ƒä¸ºç®€å•ï¼Œåªéœ€è°ƒç”¨ len() æ¥è·å–æ•°æ®é›†é•¿åº¦å³å¯
- __init__ : å®šä¹‰ä¸€äº›åŸºæœ¬å‚æ•°ï¼Œå¦‚å›¾ç‰‡çš„æ ¹ç›®å½•ç­‰ï¼Œéœ€è¦ä¸ __getitem__ ä¸€èµ·é…åˆå®Œæˆæ•°æ®çš„è¯»å–
- __getitem__ : æ¥å— int ç±»å‹çš„å‚æ•° indexï¼Œå³æƒ³è¦è¯»å–çš„æ ·æœ¬çš„ç´¢å¼•ï¼Œæ ¹æ®ç´¢å¼•è¿”å›è¯¥æ ·æœ¬ã€‚

```
class CrowdDataset(Dataset):
    def __init__(self, img_root, gt_dmap_root, gt_downsample=1):
        '''
        img_root: å›¾ç‰‡çš„æ ¹ç›®å½•.
        gt_dmap_root: çœŸå®å¯†åº¦å›¾çš„æ ¹ç›®å½•.
        gt_downsample: é»˜è®¤ä¸º0ï¼Œè¡¨ç¤ºæ¨¡å‹çš„è¾“å‡ºä¸è¾“å…¥å›¾åƒå¤§å°ç›¸åŒ.
        '''
        self.img_root = img_root
        self.gt_dmap_root = gt_dmap_root
        self.gt_downsample = gt_downsample

        self.img_names = [filename for filename in os.listdir(img_root)
                          if os.path.isfile(os.path.join(img_root, filename))] # è·å–æ‰€æœ‰å›¾ç‰‡çš„æ–‡ä»¶å
        self.n_samples = len(self.img_names) # æ•°æ®é›†çš„é•¿åº¦

    def __len__(self):
        return self.n_samples

    def __getitem__(self, index):
        assert index <= len(self), 'index range error'
        img_name = self.img_names[index]
        img = plt.imread(os.path.join(self.img_root, img_name))
        if len(img.shape) == 2:  # å°†å•é€šé“ç°åº¦å›¾æ‰©å±•ä¸ºä¸‰é€šé“
            img = img[:, :, np.newaxis]
            img = np.concatenate((img, img, img), 2)

        gt_dmap = np.load(os.path.join(self.gt_dmap_root, img_name.replace('.jpg', '.npy')))
        # å¯¹å›¾åƒå’Œå¯†åº¦å›¾è¿›è¡Œä¸‹é‡‡æ ·
        if self.gt_downsample > 1:
            ds_rows = int(img.shape[0]//self.gt_downsample)
            ds_cols = int(img.shape[1]//self.gt_downsample)
            img = cv2.resize(img, (ds_cols*self.gt_downsample,
                                   ds_rows*self.gt_downsample))
            # é¡ºåºè½¬æ¢ä¸º (channel,rows,cols)
            img = img.transpose((2, 0, 1))
            gt_dmap = cv2.resize(gt_dmap, (ds_cols, ds_rows))
            gt_dmap = gt_dmap[np.newaxis, :, :] * \
                self.gt_downsample*self.gt_downsample

            img_tensor = torch.tensor(img, dtype=torch.float)
            gt_dmap_tensor = torch.tensor(gt_dmap, dtype=torch.float)

        return img_tensor, gt_dmap_tensor
```

æ•°æ®åŠ è½½ dataloader
æ ¹æ®å‰é¢å®šä¹‰å¥½çš„ç±» CrowdDataset æ¥æ„é€ ä¸€ä¸ª dataloaderï¼Œè®­ç»ƒæ—¶é€šè¿‡è¿­ä»£ dataloader æ¥è·å–æ•°æ®ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡dataloader æ¥å®ç°ä¸€äº›åŸºç¡€åŠŸèƒ½ï¼Œå¦‚å°†æ•°æ®é›†æ‰“ä¹±ï¼ˆshuffle=Trueï¼‰ï¼Œè®¾ç½®batch_sizeï¼Œå¤šçº¿ç¨‹åŠ è½½æ•°æ®ï¼ˆnum_workersï¼‰ç­‰ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç”±äºæ•°æ®é›†ä¸­å›¾ç‰‡å¤§å°ä¸ä¸€æ ·ï¼Œæ‰€ä»¥è®¾ç½® batch_size=1ï¼ŒåŒæ ·çš„åŸå› ï¼Œåœ¨ä¸Šæ–‡ä¸­ï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½® torch.backends.cudnn.benchmark=Falseã€‚

```
# è®­ç»ƒé›†
img_root = './datasets/part_A_final/train_data/images'
gt_dmap_root = './temp/part_A_final/train_data/ground_truth'
dataset = CrowdDataset(img_root, gt_dmap_root, 4)
dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)
# æµ‹è¯•é›†
test_img_root = './datasets/part_A_final/test_data/images'
test_gt_dmap_root = './temp/part_A_final/test_data/ground_truth'
test_dataset = CrowdDataset(test_img_root, test_gt_dmap_root, 4)
test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
```


#### æ­¥éª¤3ï¼šæ¨¡å‹æ„å»º
æœ¬å°èŠ‚é€šè¿‡ pytorch çš„ä¸€äº›å†…ç½®å‡½æ•°ï¼Œæ­å»ºäº†è®ºæ–‡ä¸­çš„ MCNN æ¨¡å‹ï¼Œå…·ä½“åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š

1. æ¨¡å‹ä»‹ç»
2. æ¨¡å‹å®šä¹‰
3. å®ä¾‹åŒ–æ¨¡å‹
4. æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨

##### æ¨¡å‹ä»‹ç»
æœ¬æ•™ç¨‹é‡‡ç”¨çš„çš„æ¨¡å‹ MCNN çš„ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<div align=center>
    <!-- ![åº”ç”¨åœºæ™¯](./img/16cvåº”ç”¨åœºæ™¯.jpg) -->
    <img src="./img/ch9/2.png" width="600"/>
</div>

MCNN åŒ…å«äº†ä¸‰åˆ—å…·æœ‰ä¸åŒæ»¤æ³¢å™¨å¤§å°çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚ç½‘ç»œçš„æ¯ä¸€åˆ—å¹¶è¡Œçš„å­ç½‘ç»œæ·±åº¦ç›¸åŒï¼Œä½†æ˜¯æ»¤æ³¢å™¨çš„å¤§å°ä¸åŒï¼ˆå¤§ï¼Œä¸­ï¼Œå°ï¼‰ï¼Œå› æ­¤æ¯ä¸€åˆ—å­ç½‘ç»œçš„æ„Ÿå—é‡ä¸åŒï¼Œèƒ½å¤ŸæŠ“ä½ä¸åŒå¤§å°äººå¤´çš„ç‰¹å¾ï¼Œæœ€åå°†ä¸‰åˆ—å­ç½‘ç»œçš„ç‰¹å¾å›¾åšçº¿æ€§åŠ æƒï¼ˆç”± 1x1 çš„å·ç§¯å®Œæˆï¼‰å¾—åˆ°è¯¥å›¾åƒçš„äººç¾¤å¯†åº¦å›¾ï¼Œç±»ä¼¼æ¨¡å‹èåˆçš„æ€æƒ³ã€‚é‡‡ç”¨äº† 2x2 çš„ max-pooling å’Œ ReLU æ¿€æ´»å‡½æ•°ã€‚

##### æ¨¡å‹å®šä¹‰
é€šè¿‡ pytorch çš„å†…ç½®æ¥å£ï¼Œæ­å»ºæ¨¡å‹ç»“æ„å¦‚ä¸‹æ‰€ç¤ºã€‚ å…¶ä¸­ self.branch1ï¼Œself.branch2 å’Œ self.branch3 ä¸ºä¸Šå›¾ä¸­çš„ä¸‰ä¸ªåˆ†æ”¯ï¼Œself.fuse ä¸ºä¸Šå›¾ä¸­æœ€åçš„èåˆæ¨¡å—ã€‚_initialize_weights ç”¨äºå¯¹æ¨¡å‹è¿›è¡Œåˆå§‹åŒ–ã€‚

æ­å»ºæ¨¡å‹æ‰€ç”¨æ¥å£å®˜æ–¹æ–‡æ¡£
```
class MCNN(nn.Module):

    def __init__(self, load_weights=False):
        super(MCNN, self).__init__()

        self.branch1 = nn.Sequential(
            nn.Conv2d(3, 16, 9, padding=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 7, padding=3),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 16, 7, padding=3),
            nn.ReLU(inplace=True),
            nn.Conv2d(16, 8, 7, padding=3),
            nn.ReLU(inplace=True)
        )

        self.branch2 = nn.Sequential(
            nn.Conv2d(3, 20, 7, padding=3),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(20, 40, 5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(40, 20, 5, padding=2),
            nn.ReLU(inplace=True),
            nn.Conv2d(20, 10, 5, padding=2),
            nn.ReLU(inplace=True)
        )

        self.branch3 = nn.Sequential(
            nn.Conv2d(3, 24, 5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(24, 48, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(48, 24, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(24, 12, 3, padding=1),
            nn.ReLU(inplace=True)
        )

        self.fuse = nn.Sequential(nn.Conv2d(30, 1, 1, padding=0))

        if not load_weights:
            self._initialize_weights()

    def forward(self, img_tensor):
        x1 = self.branch1(img_tensor)
        x2 = self.branch2(img_tensor)
        x3 = self.branch3(img_tensor)
        x = torch.cat((x1, x2, x3), 1)
        x = self.fuse(x)
        return x

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.normal_(m.weight, std=0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
```

##### å®ä¾‹åŒ–æ¨¡å‹
```
print(device)
mcnn = MCNN().to(device) # æ¨¡å‹å®ä¾‹åŒ–
```

```
summary(mcnn, (3, 1000, 500)) # ä»¥è¾“å…¥å›¾ç‰‡å¤§å°ä¸º 3x1000x500 ä¸ºä¾‹æ¥å±•ç¤ºç½‘ç»œç»“æ„

[out]:
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1        [-1, 16, 1000, 500]           3,904
              ReLU-2        [-1, 16, 1000, 500]               0
         MaxPool2d-3         [-1, 16, 500, 250]               0
            Conv2d-4         [-1, 32, 500, 250]          25,120
              ReLU-5         [-1, 32, 500, 250]               0
         MaxPool2d-6         [-1, 32, 250, 125]               0
            Conv2d-7         [-1, 16, 250, 125]          25,104
              ReLU-8         [-1, 16, 250, 125]               0
            Conv2d-9          [-1, 8, 250, 125]           6,280
             ReLU-10          [-1, 8, 250, 125]               0
           Conv2d-11        [-1, 20, 1000, 500]           2,960
             ReLU-12        [-1, 20, 1000, 500]               0
        MaxPool2d-13         [-1, 20, 500, 250]               0
           Conv2d-14         [-1, 40, 500, 250]          20,040
             ReLU-15         [-1, 40, 500, 250]               0
        MaxPool2d-16         [-1, 40, 250, 125]               0
           Conv2d-17         [-1, 20, 250, 125]          20,020
             ReLU-18         [-1, 20, 250, 125]               0
           Conv2d-19         [-1, 10, 250, 125]           5,010
             ReLU-20         [-1, 10, 250, 125]               0
           Conv2d-21        [-1, 24, 1000, 500]           1,824
             ReLU-22        [-1, 24, 1000, 500]               0
        MaxPool2d-23         [-1, 24, 500, 250]               0
           Conv2d-24         [-1, 48, 500, 250]          10,416
             ReLU-25         [-1, 48, 500, 250]               0
        MaxPool2d-26         [-1, 48, 250, 125]               0
           Conv2d-27         [-1, 24, 250, 125]          10,392
             ReLU-28         [-1, 24, 250, 125]               0
           Conv2d-29         [-1, 12, 250, 125]           2,604
             ReLU-30         [-1, 12, 250, 125]               0
           Conv2d-31          [-1, 1, 250, 125]              31
================================================================
Total params: 133,705
Trainable params: 133,705
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 5.72
Forward/backward pass size (MB): 815.63
Params size (MB): 0.51
Estimated Total Size (MB): 821.86
----------------------------------------------------------------
```

##### æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
å…¶ä¸­ï¼Œä½¿ç”¨å¸¦åŠ¨é‡çš„ SGD ä½œä¸ºä¼˜åŒ–å™¨ï¼Œåˆå§‹å­¦ä¹ ç‡ä¸º 1e-6ï¼Œmomentum è®¾ç½®ä¸º 0.95ï¼ŒæŸå¤±å‡½æ•°ä½¿ç”¨æœ€å¸¸è§çš„å‡æ–¹æŸå¤±å‡½æ•°ï¼ˆMSELossï¼‰ã€‚
åœ¨ pytorch ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è°ƒç”¨å®šä¹‰å¥½çš„æ¥å£ï¼ˆMSEå’ŒSGDï¼‰æ¥å®ç°ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨ä½¿ç”¨æŸå¤±å‡½æ•°è®­ç»ƒæ¨¡å‹æ—¶ï¼Œè®¡ç®—çš„æ˜¯æ¨¡å‹è¾“å‡ºå¯†åº¦å›¾å’Œground-truthçš„å¯†åº¦å›¾ä¹‹é—´çš„å·®å¼‚ï¼Œè€Œä¸æ˜¯è¾“å‡ºäººç¾¤æ•°ç›®å’Œå®é™…äººç¾¤æ•°ç›®ä¹‹é—´çš„å·®å¼‚ã€‚

MSE è®¡ç®—æ–¹æ³•ä¸ºï¼š
$$M S E=\frac{1}{M \times N \sum_{i=1}^{M} \sum_{j=1}^{N}\left(f^{\prime}(i, j)-f(i, j)\right)^{2}}$$

å…¶ä¸­ $f^{\prime}(i,j)$ å’Œ $f(i,j)$ åˆ†åˆ«ä¸ºæ¨¡å‹è¾“å‡ºå¯†åº¦å›¾å’Œground-truthçš„å¯†åº¦å›¾ä¸Šåæ ‡ä¸º$(i,j)$çš„åƒç´ ï¼ŒM,Nåˆ†åˆ«è¡¨ç¤ºå¯†åº¦å›¾çš„é•¿ä¸å®½ã€‚

```
criterion = nn.MSELoss(reduction='sum').to(device) # æŸå¤±å‡½æ•°
optimizer = torch.optim.SGD(mcnn.parameters(), lr=1e-6, momentum=0.95) # ä¼˜åŒ–å™¨
```



#### æ­¥éª¤4ï¼šæ¨¡å‹è®­ç»ƒ

æœ¬å°èŠ‚å¯¹ä¸Šä¸€å°èŠ‚å®šä¹‰å¥½çš„æ¨¡å‹è¿›è¡Œäº†è®­ç»ƒï¼Œå…·ä½“åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š

1. è®­ç»ƒæ¨¡å‹
2. è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–

##### è®­ç»ƒæ¨¡å‹
åœ¨è®­ç»ƒå¼€å§‹ä¹‹å‰ï¼Œå¦‚ 0.2 å°èŠ‚æ‰€ç¤ºï¼Œè¦å…ˆåˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œç”¨æ¥ä¿å­˜æ–­ç‚¹å’Œè®­ç»ƒå¥½çš„æ¨¡å‹ã€‚
```
if not os.path.exists('./temp/models'):
    os.makedirs('./temp/models')
```

åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ¯è®­ç»ƒå®Œä¸€ä¸ª epoch æµ‹è¯•ä¸€ä¸‹æ¨¡å‹æ•ˆæœå¦‚ä½•ï¼Œå¹¶å°†æ¨¡å‹çš„å‚æ•°ä¿å­˜ä¸‹æ¥ï¼Œé˜²æ­¢è®­ç»ƒè¢«æ„å¤–ä¸­æ–­ä»¥åŠæ–¹ä¾¿æµ‹è¯•ã€‚æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸æ–­å¯¹æ¯”æ¯ä¸ª epoch çš„ç»“æœï¼Œå°†æœ€å¥½çš„ç»“æœè®°å½•ä¸‹æ¥ã€‚

ä¸è®­ç»ƒé›†ä¸åŒï¼Œæµ‹è¯•é›†éƒ¨åˆ†ï¼Œä¸ºäº†ä¸è®ºæ–‡ä¿æŒä¸€è‡´ï¼Œæˆ‘ä»¬æµ‹è¯•çš„æ˜¯æŒ‡æ ‡ MAEã€‚

å…·ä½“çš„è®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼š

åˆå§‹åŒ–å„ä¸ªå‚æ•°

```
min_mae, min_epoc = 10000, 0 # æœ€å°çš„ MAE å’Œå…¶å¯¹åº”çš„ epoch
train_loss_list, epoch_list, test_error_list = [], [], [] # æ–¹ä¾¿å¯è§†åŒ–
```
å¼€å§‹è®­ç»ƒï¼Œå¹¶æµ‹è¯•æ¯ä¸€ä¸ª epoch çš„ç»“æœã€‚

åœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œå¦‚æœå·²ç»è¾¾åˆ°äº†é¢„æœŸç»“æœæˆ–è€…è§‚å¯ŸæŒ‡æ ‡ MAE é•¿æ—¶é—´æ²¡æœ‰å˜åŒ–ï¼Œå¯ä»¥åŠæ—¶åœæ­¢ï¼Œå³ early stoppingã€‚

åœ¨è¿™é‡Œï¼Œåªä¿ç•™äº†è¯¥ cell çš„å‰åé¡¹è¾“å‡ºã€‚

```
writer = SummaryWriter('./temp/logs/') # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
```

åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ

tensorboard --logdir=result/logs/

å¼€å¯ tensorboardï¼Œå¹¶åœ¨æ˜¾ç¤ºçš„æŒ‡å®šåœ°å€æŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹


```
num_epochs = 1 #1000
mcnn.to(device)
for epoch in range(0,num_epochs):
    mcnn.train()
    epoch_loss=0
    for i,(img,gt_dmap) in enumerate(dataloader):
        current_batch = epoch * len(dataloader) + i
        img=img.to(device)
        gt_dmap=gt_dmap.to(device)
        # å‰å‘ä¼ æ’­
        et_dmap=mcnn(img)
        # è®¡ç®—æŸå¤±å‡½æ•°
        loss=criterion(et_dmap,gt_dmap)
        epoch_loss+=loss.item()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        writer.add_scalar('train/batch_mse_loss', loss.item(), current_batch)
        print(i)
        if i>1:break
    epoch_list.append(epoch)
    train_loss_list.append(epoch_loss/len(dataloader))
    torch.save(mcnn.state_dict(),'./temp/models/epoch_'+str(epoch)+".param")
    writer.add_scalar('train/epoch_mse_loss', epoch_loss/len(dataloader), epoch)

    mcnn.eval()
    mae=0
    for i,(img,gt_dmap) in enumerate(test_dataloader):
        img=img.to(device)
        gt_dmap=gt_dmap.to(device)
        # å‰å‘ä¼ æ’­
        et_dmap=mcnn(img)
        # è®¡ç®— MAE
        mae+=abs(et_dmap.data.sum()-gt_dmap.data.sum()).item()
        del img,gt_dmap,et_dmap
        print(i)
        if i>1:break
    if mae/len(test_dataloader)<min_mae:
        min_mae=mae/len(test_dataloader)
        min_epoch=epoch
    test_error_list.append(mae/len(test_dataloader))   
    writer.add_scalar('test/epoch_mae_loss', mae/len(test_dataloader), epoch)
    print(f"epoch:{epoch} | loss:{epoch_loss/len(dataloader)} | error:{mae/len(test_dataloader)} | min_mae:{min_mae} | min_epoch:{min_epoch}")
```


##### è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜çš„æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥å°†è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–, é€šè¿‡tensorboardæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°è®­ç»ƒè¿‡ç¨‹å¦‚å›¾æ‰€ç¤ºï¼š
<div align=center>
    <!-- ![åº”ç”¨åœºæ™¯](./img/16cvåº”ç”¨åœºæ™¯.jpg) -->
    <img src="./img/ch9/3.png" width="600"/>
</div>

é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åˆ©ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰€ä¿ç•™çš„ä¸€äº›å˜é‡æ¥è¿›è¡Œå¯è§†åŒ–ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```
plt.plot(epoch_list,train_loss_list); plt.title("MSE during the training process")
```

#### æ­¥éª¤5ï¼šæ¨¡å‹è¯„ä¼°

åœ¨æœ¬å°èŠ‚ä¸­ï¼Œå°†å¯¹è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•ä¸åˆ†æï¼Œè§‚å¯Ÿå…¶æ•ˆæœå¦‚ä½•ã€‚å…·ä½“åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š

1. è¯„ä»·æŒ‡æ ‡
2. æ¨¡å‹æµ‹è¯•
3. æŒ‡æ ‡å±•ç¤º

##### è¯„ä»·æŒ‡æ ‡
åœ¨è¿™é‡ŒåŒè®ºæ–‡ä¸­ä¸€è‡´ï¼Œé‡‡ç”¨çš„è¯„ä»·æŒ‡æ ‡æ˜¯å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMean Absolute Error, MAEï¼‰ï¼Œå…¶è®¡ç®—æ–¹æ³•å¦‚ä¸‹æ‰€ç¤ºï¼š
$$M A E=\frac{1}{n} \sum_{i=1}^{n}\left|f_{i}-y_{i}\right|=\frac{1}{n} \sum_{i=1}^{n}\left|e_{i}\right|$$
å…¶ä¸­$f_{i}$â€‹ä¸ºé¢„æµ‹å€¼ï¼Œ$y_{i}$â€‹ ä¸ºè¾“å‡ºå€¼ã€‚

##### æ¨¡å‹æµ‹è¯•
åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥çœ‹åˆ°æœ€å¥½çš„ checkpoint çš„ç¼–å·ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å…¶è¿›è¡ŒåŠ è½½å’Œæµ‹è¯•ã€‚

åœ¨è¿™é‡Œï¼Œæä¾›äº†ä¸¤ä¸ªé€‰é¡¹ï¼Œä¸€ä¸ªæ˜¯æµ‹è¯•æ‰€æœ‰æ–­ç‚¹ï¼ˆoptionAï¼‰ï¼Œä¸€ä¸ªæ˜¯æµ‹è¯•éƒ¨åˆ†æ–­ç‚¹ï¼ˆoptionBï¼‰ï¼Œå…·ä½“è§ä»£ç æ³¨é‡Šã€‚

è€ƒè™‘åˆ° notebook è®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼Œå¯èƒ½ä¼šå‘ç”Ÿä¸­æ–­ï¼Œæ•°æ®ä¸¢å¤±ï¼Œå› æ­¤åˆ©ç”¨ä¸‹é¢çš„ä»£ç å¯¹ models/ ä¸‹é¢æ‰€æœ‰çš„æ–­ç‚¹è¿›è¡Œæµ‹è¯•ã€‚æ¥è·å¾—æœ€å¥½çš„ checkpoint çš„ç¼–å·ã€‚

```
mae_list, epoch_list = [], []
# cp_list = glob.glob("models/*.param") # optionA: æ‰€æœ‰ checkpoints
# cp_list = [f"temp/models/epoch_{idx}.param" for idx in [50, 100, 150, 200, 250, 300, 400, 500, 600]] # optionB: éƒ¨åˆ† checkpoints
cp_list = [f"temp/models/epoch_0.param"]
for cp in cp_list:
    epoch = int(re.findall(r"\d+", cp)[0])
    mcnn.load_state_dict(torch.load(cp))
    mcnn.eval()
    mae=0
    for i,(img,gt_dmap) in enumerate(test_dataloader):
        img=img.to(device)
        gt_dmap=gt_dmap.to(device)
        # forward propagation
        et_dmap=mcnn(img)
        mae+=abs(et_dmap.data.sum()-gt_dmap.data.sum()).item()
        print(i)
        if i>1:break
    mean_mae=mae/len(test_dataloader)
    mae_list.append(mean_mae)
    epoch_list.append(epoch)
    print(f"current epoch: {epoch} | mae: {mean_mae}")
```

##### æŒ‡æ ‡å±•ç¤º
```
plt.plot(epoch_list,mae_list)
plt.scatter(epoch_list,mae_list)
best_mae = min(mae_list)
best_epoch = epoch_list[mae_list.index(best_mae)]
print(f"best epoch in the selected epochs: {best_epoch} | best mae: {best_mae}")
```



#### æ­¥éª¤6ï¼šæ¨¡å‹é¢„æµ‹
æœ¬å°èŠ‚ä¸ºæ¨¡å‹é¢„æµ‹éƒ¨åˆ†ï¼Œå…·ä½“åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†

1. ç»˜å›¾å‡½æ•°å®šä¹‰
2. æ¨¡å‹åŠ è½½
3. é¢„æµ‹ç»“æœå±•ç¤º

##### ç»˜å›¾å‡½æ•°å®šä¹‰
å®šä¹‰ä¸€ä¸ªç»˜åˆ¶ tensor çš„å‡½æ•°ï¼Œæ–¹ä¾¿è°ƒç”¨:
```
def plot_tensor(tensor, show=True):
    ret = tensor.squeeze(0).squeeze(0).cpu().detach().numpy() # å»é™¤ä¸¤ä¸ªç»´åº¦ï¼Œè½¬åŒ–ä¸º numpy ç±»å‹
    if show:
        plt.imshow(ret) 
    return ret
```


##### æ¨¡å‹åŠ è½½

æ ¹æ®è®­ç»ƒè¿‡ç¨‹é€‰æ‹©å¹¶åŠ è½½æ•ˆæœæœ€å¥½çš„ checkpointï¼Œå¹¶å¯¹å…¶è¿›è¡Œæµ‹è¯•.

åœ¨çœŸå®åœºæ™¯ä¸­æˆ‘ä»¬ä¸€èˆ¬ä½¿ç”¨çš„æ˜¯ cpu è¿›è¡Œé¢„æµ‹ï¼Œæ‰€ä»¥è¿™é‡Œå°†æ¨¡å‹æ•°æ®æ”¾åˆ° cpu ä¸Šï¼ˆto("cpu)ï¼‰ã€‚

```
best_epoch = 0 #560 # ç”±è®­ç»ƒè¿‡ç¨‹å¾—åˆ°
best_cp = f"temp/models/epoch_{best_epoch}.param"
mcnn.to("cpu").load_state_dict(torch.load(best_cp))
mcnn = mcnn.eval()
```

##### é¢„æµ‹ç»“æœå±•ç¤º

###### æµ‹è¯•é›†

é€‰å–æŒ‡å®šå›¾ç‰‡è¿›è¡Œæµ‹è¯•ï¼ˆæ¥è‡ªæµ‹è¯•é›†ï¼‰ï¼Œå¹¶å°†è¾“å‡ºçš„çš„ç»“æœä¿å­˜åœ¨ result/ ä¸‹
```
# è¯»å–æŒ‡å®šå›¾ç‰‡ï¼ˆåŸå›¾ç‰‡ï¼Œå¯†åº¦å›¾å’Œ ground-truthï¼‰
idx = 4
img_path = f"datasets/part_A_final/test_data/images/IMG_{idx}.jpg"
gt_path = f"temp/part_A_final/test_data/ground_truth/IMG_{idx}.npy"
gtmat_path = f"datasets/part_A_final/test_data/ground_truth/GT_IMG_{idx}.mat"
img = plt.imread(img_path)
img_tensor = torch.tensor(img.transpose((2, 0, 1)),dtype=torch.float).unsqueeze(0).to("cpu") # è½¬åŒ–ä¸ºtensorç±»å‹
gt = np.load(gt_path)
output = mcnn(img_tensor)
output_number = output.sum() # æ±‚å’Œä»¥è·å¾—æ¨¡å‹æ£€æµ‹å¾—åˆ°çš„äººç¾¤æ€»æ•°
gt_number = io.loadmat(gtmat_path)["image_info"][0][0][0][0][1][0][0] # è¯»å–ground-truthä¸­çš„äººç¾¤æ•°ç›®

plt.figure(figsize=(12,4))
plt.subplot(131);plt.imshow(img)
plt.subplot(132);plt.imshow(gt)
plt.subplot(133);plot_tensor(output)

print(f"è¾“å‡ºç»“æœï¼š{output_number.int()}ï¼ŒçœŸå®ç»“æœï¼š{gt_number}")

```

<div align=center>
    <!-- ![åº”ç”¨åœºæ™¯](./img/16cvåº”ç”¨åœºæ™¯.jpg) -->
    <img src="./img/ch9/4.png" width="600"/>
</div>


ä¿å­˜å›¾ç‰‡ï¼Œä¸€ä¸ªå›¾ç‰‡å¯¹åº”ä¸€ä¸ª .txt ç»“æœæ–‡ä»¶ã€‚
```
save_root = "temp/results"
if not os.path.exists(save_root):
    os.makedirs(save_root)
save_name = img_path.split("/")[-1]
save_path = os.path.join(save_root,save_name)
plt.imsave(save_path, plot_tensor(output, show=False)) # ä¿å­˜è¾“å‡ºå›¾ç‰‡

with open(save_path.replace(".jpg",".txt"),"w") as f: # ä¿å­˜txtç»“æœæ–‡ä»¶
    f.write(f"img:{save_path}, people:{output_number.int()}")
```

###### ç½‘ç»œå›¾ç‰‡
é€‰å–æŒ‡å®šå›¾ç‰‡è¿›è¡Œæµ‹è¯•ï¼ˆæ¥è‡ªç½‘ç»œå›¾ç‰‡ï¼‰

è¿™é‡Œä¸€å…±æä¾›äº†ç½‘ç»œä¸Šä¸‹è½½çš„å››å¼ äººç¾¤ç…§ç‰‡ï¼Œä¿å­˜åœ¨ pictures/ ç›®å½•ä¸‹ï¼Œå…‹é€šè¿‡æ”¹å˜ä»£ç ä¸­çš„ idx æ¥æµ‹è¯•ä¸åŒå›¾ç‰‡

```
idx = 3 # 1,2,3,4
img_path = f"pictures/test{idx}.jpg"
img = plt.imread(img_path)
img_tensor = torch.tensor(img.transpose((2, 0, 1)),dtype=torch.float).unsqueeze(0).to("cpu")
output = mcnn(img_tensor) # æ¨¡å‹æµ‹è¯•è¾“å‡º
output_number = output.sum() # äººç¾¤æ•°ç›®æ£€æµ‹ç»“æœ

plt.figure(figsize=(8,4))
plt.subplot(121);plt.imshow(img)
plt.subplot(122);plot_tensor(output)
plt.show()

print(f"è¾“å‡ºç»“æœï¼š{output_number.int()}")
```

ä¿å­˜ç»“æœ
```
save_root = "result/"
save_name = img_path.split("/")[-1]
save_path = os.path.join(save_root,save_name)
plt.imsave(save_path, plot_tensor(output, show=False)) # ä¿å­˜å›¾ç‰‡

with open(save_path.replace(".jpg",".txt"),"w") as f:
    f.write(f"img:{save_path}, people:{output_number.int()}") #ä¿å­˜ç»“æœä¸ºtxtæ–‡ä»¶
```



## 5.ä»»åŠ¡æ‹“å±•

### 5.1 ä¼˜åŒ–æ€è·¯å’Œæ–¹æ³•
- ä½¿ç”¨ FPN æ•æ‰å›¾åƒä¸­ä¸åŒå°ºåº¦çš„äººå¤´ä¿¡æ¯
- ä½¿ç”¨ç©ºæ´å·ç§¯æ“ä½œæ‰©å¤§æ„Ÿå—é‡


## 6. ä»»åŠ¡å®è®­
1. åˆ¶ä½œå¯†åº¦å›¾æ—¶ç”¨åˆ°çš„æ ¸å¿ƒç®—æ³•æ˜¯ ã€åˆ†å€¼ï¼š20ã€‘
   A.å†³ç­–æ ‘ B.é«˜æ–¯æ»¤æ³¢ C.NMS D.åŒçº¿æ€§æ’å€¼
2. ä¸‹åˆ—å“ªäº›æ˜¯äººç¾¤è®¡æ•°å¸¸ç”¨çš„æ–¹æ³•ï¼Ÿ ã€åˆ†å€¼ï¼š20ã€‘
   A.ä»¥æ£€æµ‹ä¸ºåŸºç¡€ B.ä»¥åˆ†å‰²ä¸ºåŸºç¡€ C.ä»¥å›å½’ä¸ºåŸºç¡€ D.ä»¥å¯†åº¦å›¾ä¸ºåŸºç¡€ E.ä»¥åˆ†ç±»ä¸ºåŸºç¡€
3. MCNNä¸­ä½¿ç”¨äº†3åˆ—ä¸åŒå°ºå¯¸çš„å·ç§¯æ ¸æ˜¯ä¸ºäº†æ•æ‰ä¸åŒå°ºåº¦çš„äººå¤´ä¿¡æ¯ ã€åˆ†å€¼ï¼š20ã€‘
4. MCNNä¸­è®¡ç®—lossæ—¶æ¯”è¾ƒçš„é¢„æµ‹äººæ•°å’ŒGround Truthäººæ•°çš„å·®åˆ« ã€åˆ†å€¼ï¼š20ã€‘
5. MCNNçš„æµ‹è¯•æŒ‡æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ ã€åˆ†å€¼ï¼š20ã€‘
6. ç›¸æ¯”æ£€æµ‹å’Œå›å½’ï¼ŒåŸºäºå¯†åº¦å›¾é¢„æµ‹äººæ•°çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ ã€åˆ†å€¼ï¼š0ã€‘