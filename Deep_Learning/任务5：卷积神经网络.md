[toc]
# 任务5：卷积神经网络


## 1 任务目标

1. 理解卷积神经网络的基本要素

2. 用TensorFlow实现卷积神经网络
                       

## 2 任务描述

还记得在任务3中的任务实施里面，我们构建了一个简单的神经网络去识别手写数字吗？那是一个仅有输入输出层的简单全连接神经网络，它能达到近91%的准确率，准确率低、计算速度慢、容易过拟合的缺点，因此需要优化。
      
卷积神经网络（CNN）提供了更好的优化，它通过卷积计算逐层提取出图像的特征，同时通过池化缩小数据的规模，最终也是以特征作为分类的依据。这些优化使数据更好地拟合，达到更高的准确率。



## 3 知识准备


### 3.1 卷积(Convolution)
- 卷积神经网络，顾名思义卷积就是其中的关键。
- 什么是卷积？卷积神经网络可以简单地理解为，用**滤波器(Filter)**(**也称卷积核**)将相邻像素之间的"轮廓"过滤出来。
- 全连接神经网络是以y = wx + b 的形式连接，而卷积神经网络的权重w就是卷积核的数值，它常常是一个矩阵。

![](https://p.pstatp.com/origin/dc120002c19b2d19f59b)

- 卷积的滤波器是如何工作的呢？其实就是矩阵相同位置上的元素相乘之后求和。

我们来看一个最简单的例子：“边界检测（edge detection）”，假设我们有这样的一张图片，大小8×8：

![](https://ae01.alicdn.com/kf/Hc3222d304284494cbb913f6ab6d23c96A.jpg)

图片中的数字代表该位置的像素值，我们知道，像素值越大，颜色越亮，所以为了示意，我们把右边小像素的地方画成深色。图的中间两个颜色的分界线就是我们要检测的边界。

怎么检测这个边界呢？我们可以设计这样的一个卷积核，大小3×3：

![](https://ae01.alicdn.com/kf/H97e9b1787a6b44b6a49a6e2ba9a4fc57j.jpg)

然后，我们用这个卷积核，往我们的图片上“盖”，覆盖一块跟卷积核一样大的区域之后，对应元素相乘，然后求和。计算一个区域之后，就向其他区域挪动，接着计算，直到把原图片的每一个角落都覆盖到了为止。这个过程就是 “卷积”。

![](https://ae01.alicdn.com/kf/H005319e99ad345949041905eaa6c3ecfC.png)


这里的“挪动”，就涉及到一个步长了，假如我们的步长是1，那么覆盖了一个地方之后，就挪一格，容易知道，总共可以覆盖6×6个不同的区域。

那么，我们将这6×6个区域的卷积结果，拼成一个矩阵：

![](https://ae01.alicdn.com/kf/Ha09643deb6e642b1896ae953cce0d26bS.jpg)

发现了什么？这个图片，中间颜色浅，两边颜色深，这说明咱们的原图片中间的边界，在这里被反映出来了!



从上面这个例子中，我们发现，我们可以通过设计特定的卷积核，让它去跟图片做卷积，就可以识别出图片中的某些特征，比如边界。因此卷积得出的图片称为**特征图或特征矩阵（Feature Map）**

那么问题来了，我们怎么可能去设计这么多各种各样的卷积核呀？首先，我们都不一定清楚对于一大推图片，我们需要识别哪些特征，其次，就算知道了有哪些特征，想真的去设计出对应的卷积核，恐怕也并非易事，要知道，特征的数量可能是成千上万的。

其实学过神经网络之后，我们就知道，这些卷积核根本就不用我们去设计，每个卷积核中的各个数字，不就是参数吗，我们可以通过大量的数据，来让机器自己去“学习”这些参数嘛。这，就是CNN的原理。

### 3.2 填充（Padding）

从上面的例子中，我们可以知道，原图像在经过卷积核卷积之后，变小了，从(8,8)变成了(6,6)。假设我们再卷一次，那大小就变成了(4,4)了。

这样有啥问题呢？
主要有两个问题：

- 每次卷积，图像都缩小，这样卷不了几次就没了；
- 相比于图片中间的点，图片边缘的点在卷积中被计算的次数很少。这样的话，边缘的信息就易于丢失。

为了解决这个问题，我们可以采用填充的方法。我们每次卷积前，先给图片周围都补一圈空白，让卷积之后图片跟原来一样大，同时，原来的边缘也被计算了更多次。

![](https://ae01.alicdn.com/kf/Hbdbfbc29f1da44708fda92c0d4f631a4F.jpg)

我们把上面这种“让卷积之后的大小不变”的padding方式，称为 “Same”方式，把不经过任何填白的；称为 “Valid”方式。这个是我们在使用一些框架的时候，需要设置的超参数。

### 3.3 步长（stride）

前面我们所介绍的卷积，都是默认步长是1，但实际上，我们可以设置步长为其他的值。
- 比如，对于(8,8)的输入，我们用(3,3)的卷积核。
- 如果stride=1，则输出为(6,6);
- 如果stride=2，则输出为(3,3)，这里卷积核恰好卷不到图片的尽头（因为步长为2，尽头只剩下一格），所以向下取整。

### 3.4 池化（pooling）

用卷积核进行窗口滑动过程中，实际上"重叠"计算了很多冗余的信息，而池化操作就是去除这些冗余信息，提取这个区域的主要特征，防止过拟合。

比如下面的MaxPooling，采用了一个2×2的窗口，取这个窗口中的最大值，并取步长=2。

![](https://ae01.alicdn.com/kf/Hdf3b468f040346f6aecbc4735d4c2a1aX.jpg)

除了MaxPooling,还有AveragePooling，顾名思义就是取那个区域的平均值。

### 3.5 通道（Channel）与深度（depth）

这里的深度是指输出层图片的深度，通常图片有红绿蓝(RGB)三个颜色通道(Channel)，那么一个卷积核也需要三层卷积核对每个颜色通道进行特征提取。

上一个例子中假设输入的图片是 8x8 的RGB图片，即输入为（8，8，3），如果要用 3x3 的卷积核去处理，那么卷积核应该为（3，3，3），它的最后一维要跟输入的channel维度一致。因此，输出的维度并不会变化。还是(6，6)。

但是，一般情况下，我们会使用多了卷积核同时卷积，比如，如果我们同时使用4个卷积核的话，那么输出的维度则会变为(6，6，4)。这里的 4 ，就是深度为 4。

![](https://ae01.alicdn.com/kf/H69ba13920826479e8322278910114143j.jpg)

再具体地画出完整的网络，需要加上偏置以及激活函数，在这里激活函数选用的是 Relu 。

![](https://ae01.alicdn.com/kf/Hd72c0f948fb841388c261659d964a9b7b.jpg)

## 4 任务实施

### 4.1 实施思路

利用TensorFlow框架的API来搭建卷积神经网络。

### 4.2 实施步骤


#### 构建卷积层
- TensorFlow已提供了卷积的实现 tf.nn.conv2d()


```
tf.nn.conv2d(input, filter, strides, padding)
```

```
#例
tf.nn.conv2d(x_input, W_1, strides = [1,1,1,1],padding = 'SAME')
```

参数：

- input : 输入的要做卷积的图片，要求为一个张量，shape为 [ batch, in_height, in_weight, in_channel ]，其中batch为图片的数量，in_height 为图片高度，in_weight 为图片宽度，in_channel 为图片的通道数，灰度图该值为1，彩色图为3。

- filter： 卷积核，要求也是一个张量，shape为 [ filter_height, filter_weight, in_channel, out_channels ]，其中 filter_height 为卷积核高度，filter_weight 为卷积核宽度，in_channel 是图像通道数 ，和 input 的 in_channel 要保持一致，out_channel 是卷积核数量。

- strides： 卷积时在图像每一维的步长，这是一个一维的向量，[ 1, strides, strides, 1]，第一位和最后一位固定必须是1，第二位和第三位分别为横向步长和纵向步长。

- padding： string类型，值为“SAME” 和 “VALID”，表示的是卷积的形式，是否考虑边界。"SAME"是考虑边界，不足的时候用0去填充周围，使输出的特征矩阵与输入矩阵维度相同，"VALID"则不考虑。

- 返回一个Tensor，shape是[batch, height, width, channels]这种形式。






#### 构建池化层
- 卷积层后面一般紧跟一层池化层，这里以 MaxPooling为例。


```
    tf.nn.max_pool(value, ksize, strides, padding）
```

```
    #例
    tf.nn.max_pool(x, ksize = [1,2,2,1], strides = [1,2,2,1], padding='SAME')
```

- value：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape；
- ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1；
- strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1] 
- padding：和卷积类似，不再赘述；
- 返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式。

## 5 任务拓展

TensorFlow 的卷积 API 除了 tf.nn.conv2d() 之外，还有 conv1d() 以及 conv3d。

- tf.nn.conv1d()

在自然语言处理(NLP)领域，甚至图像处理的时候，我们可能会用到一维卷积(conv1d)。一维卷积可以看作是二维卷积(conv2d)的简化，二维卷积是将一个特征图在width和height两个方向上进行滑动窗操作，对应位置进行相乘求和；而一维卷积则只是在width或者说height方向上进行滑动窗口并相乘求和。

- tf.nn.conv3d()

 3D卷积一般用在视频上，就是在图像的长和宽的卷积的基础上加上视频中的帧也就是时间这一维上的卷积。有兴趣的同学可以去了解一下。



## 6 任务实训

### 6.1 实训目的

充分理解卷积神经网络的概念。

### 6.2 实训内容

根据所学知识，比较全连接神经网络与卷积神经网络的区别，回答下列问题：

- 全连接神经网络有什么缺点？
- 卷积神经网络作出了什么改进？
- 作出的改进使神经网络有了什么提升？

