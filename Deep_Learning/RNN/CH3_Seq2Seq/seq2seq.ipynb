{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tf version:1.14.0\n"}],"source":"#导入要用的包\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\ntf.enable_eager_execution()\nprint(\"tf version:{}\".format(tf.__version__)) #查看一下tensorflow版本\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\nimport unicodedata\nimport re\nimport numpy as np\nimport os\nimport io\nimport time"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# 步骤1：准备数据集"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"# 下载数据\npath_to_zip = tf.keras.utils.get_file(\n    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n    extract=True)\n\npath_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"<start> may i borrow this book ? <end>\nb'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"}],"source":"# 定义个函数将字符编码转换成ascii\ndef unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn')\n\n#对句子做预处理\ndef preprocess_sentence(w):\n    w = unicode_to_ascii(w.lower().strip())\n\n    # 在标点符号与单词之间添加空格\n    # eg: \"he is a boy.\" => \"he is a boy .\"\n    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n    w = re.sub(r'[\" \"]+', \" \", w)\n\n    # 除了 (a-z, A-Z, \".\", \"?\", \"!\", \",\")都用空格替换\n    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n\n    w = w.rstrip().strip()\n\n    # 给句子添加句头和句尾的标识符，模型才能找到句子的开始和结束。\n    w = '<start> ' + w + ' <end>'\n    return w\n\n\n#测试一下上面函数的效果\nen_sentence = u\"May I borrow this book?\"\nsp_sentence = u\"¿Puedo tomar prestado este libro?\"\nprint(preprocess_sentence(en_sentence))\nprint(preprocess_sentence(sp_sentence).encode('utf-8'))\n\n"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"}],"source":"#读取path路径下的文件，然后按[英语，西班牙语]的格式返回单词对\ndef create_dataset(path, num_examples):\n    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n\n    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n\n    return zip(*word_pairs)\n    \nen, sp = create_dataset(path_to_file, None)\n#输出一下看看数据样例。\nprint(en[-1])\nprint(sp[-1])\n\n\n\ndef max_length(tensor):\n    return max(len(t) for t in tensor)\n#将文本转换成整数序列\ndef tokenize(lang):\n  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n      filters='')\n  lang_tokenizer.fit_on_texts(lang)\n\n  tensor = lang_tokenizer.texts_to_sequences(lang)\n\n  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n                                                         padding='post')\n\n  return tensor, lang_tokenizer\n  \n#将path路径下的文件，生成我们模型要的【输入，输出】对。\ndef load_dataset(path, num_examples=None):\n    # creating cleaned input, output pairs\n    targ_lang, inp_lang = create_dataset(path, num_examples)\n\n    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n\n    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"8000 8000 2000 2000\nInput Language; index to word mapping\n1 ----> <start>\n15 ----> lo\n4055 ----> reconozco\n3 ----> .\n2 ----> <end>\n\nTarget Language; index to word mapping\n1 ----> <start>\n4 ----> i\n2071 ----> recognize\n37 ----> him\n3 ----> .\n2 ----> <end>\n"}],"source":"# 这里可以设置数据集的大小，原始数据有超过100,000条句子，由于训练时间的原因我们设小一点，当然模型效果也会因此下降\nnum_examples = 10000\ninput_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n\n# 计算目标张量（也就是例句）的最大长度\nmax_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n\n# 将数据集按8：2比例分成训练集和测试集\ninput_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n\n# 看看样本量\nprint(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n\n\n#看看句子对应的数字序列是什么样子的。\ndef convert(lang, tensor):\n  for t in tensor:\n    if t!=0:\n      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n\nprint (\"Input Language; index to word mapping\")\nconvert(inp_lang, input_tensor_train[0])\nprint ()\nprint (\"Target Language; index to word mapping\")\nconvert(targ_lang, target_tensor_train[0])"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"(64, 12) (64, 8)\n"}],"source":"#设置一下数据集的一些参数\nBUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 64\nsteps_per_epoch = len(input_tensor_train)//BATCH_SIZE\nembedding_dim = 256\nunits = 1024\nvocab_inp_size = len(inp_lang.word_index)+1\nvocab_tar_size = len(targ_lang.word_index)+1\n\n#用tf.data来封装我们前面准备的【输入，输出】对，这样可以用tensorflow的api做一些分batch，shuffle数据等功能\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\nexample_input_batch, example_target_batch = next(iter(dataset))\nprint(example_input_batch.shape, example_target_batch.shape)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# 步骤2 :搭建模型"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Encoder output shape: (batch size, sequence length, units) (64, 12, 1024)\nEncoder Hidden state shape: (batch size, units) (64, 1024)\n"}],"source":"#定义一个Encoder类\nclass Encoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n    super(Encoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.enc_units = enc_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.enc_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform') #这里使用GRU模块，是LSTM的一种变种\n\n  def call(self, x, hidden):\n    x = self.embedding(x)\n    output, state = self.gru(x, initial_state = hidden)\n    return output, state\n\n  def initialize_hidden_state(self):\n    return tf.zeros((self.batch_sz, self.enc_units))\n\nencoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n\n# 拿个例子查看一下encoder的输出\nsample_hidden = encoder.initialize_hidden_state()\nsample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\nprint ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\nprint ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Decoder output shape: (batch_size, vocab size) (64, 2306)\n"}],"source":"#定义一个Decoder类\nclass Decoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n    super(Decoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.dec_units = dec_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.dec_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc = tf.keras.layers.Dense(vocab_size)\n\n\n  def call(self, x, hidden, enc_output):\n    # enc_output shape == (batch_size, max_length, hidden_size)\n    context_vector =  tf.reduce_sum(enc_output, axis=1)\n\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # output shape == (batch_size * 1, hidden_size)\n    output = tf.reshape(output, (-1, output.shape[2]))\n\n    # output shape == (batch_size, vocab)\n    x = self.fc(output)\n\n    return x, state\n\ndecoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n\nsample_decoder_output, _  = decoder(tf.random.uniform((64, 1)),\n                                      sample_hidden, sample_output)\n\nprint ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"#设置一下优化方法和损失函数\noptimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n\n  return tf.reduce_mean(loss_)\n\n#将之后训练过程中的一些变量的取值保存下来\ncheckpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# 步骤3：训练模型"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"WARNING:tensorflow:Entity <function train_step at 0x000002738DF31948> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function train_step at 0x000002738DF31948>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <function train_step at 0x000002738DF31948> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function train_step at 0x000002738DF31948>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:Entity <bound method Encoder.call of <__main__.Encoder object at 0x0000027398261F08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Encoder.call of <__main__.Encoder object at 0x0000027398261F08>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING: Entity <bound method Encoder.call of <__main__.Encoder object at 0x0000027398261F08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Encoder.call of <__main__.Encoder object at 0x0000027398261F08>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:From D:\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000273B98AAF78> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000273B98AAF78>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000273B98AAF78> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000273B98AAF78>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:Entity <function train_step at 0x000002738DF31948> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function train_step at 0x000002738DF31948>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <function train_step at 0x000002738DF31948> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function train_step at 0x000002738DF31948>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:Entity <bound method Encoder.call of <__main__.Encoder object at 0x0000027398261F08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Encoder.call of <__main__.Encoder object at 0x0000027398261F08>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING: Entity <bound method Encoder.call of <__main__.Encoder object at 0x0000027398261F08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Encoder.call of <__main__.Encoder object at 0x0000027398261F08>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nWARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x0000027398286248>>: AttributeError: module 'gast' has no attribute 'Num'\nEpoch 1 Batch 0 Loss 5.1286\nEpoch 1 Batch 100 Loss 1.9447\nEpoch 1 Loss 2.4890\nTime taken for 1 epoch 535.5978331565857 sec\n\nEpoch 2 Batch 0 Loss 2.0414\nEpoch 2 Batch 100 Loss 1.4324\nEpoch 2 Loss 1.5956\nTime taken for 1 epoch 290.2805449962616 sec\n\nEpoch 3 Batch 0 Loss 1.4356\nEpoch 3 Batch 100 Loss 1.0107\nEpoch 3 Loss 1.1649\nTime taken for 1 epoch 320.09608578681946 sec\n\nEpoch 4 Batch 0 Loss 1.0011\nEpoch 4 Batch 100 Loss 0.7447\nEpoch 4 Loss 0.8497\nTime taken for 1 epoch 332.5002987384796 sec\n\nEpoch 5 Batch 0 Loss 0.7181\nEpoch 5 Batch 100 Loss 0.4894\nEpoch 5 Loss 0.6059\nTime taken for 1 epoch 330.11133646965027 sec\n\nEpoch 6 Batch 0 Loss 0.5038\nEpoch 6 Batch 100 Loss 0.3248\nEpoch 6 Loss 0.4170\nTime taken for 1 epoch 258.35612320899963 sec\n\nEpoch 7 Batch 0 Loss 0.3253\nEpoch 7 Batch 100 Loss 0.2684\nEpoch 7 Loss 0.2995\nTime taken for 1 epoch 235.4794692993164 sec\n\nEpoch 8 Batch 0 Loss 0.2184\nEpoch 8 Batch 100 Loss 0.1608\nEpoch 8 Loss 0.2186\nTime taken for 1 epoch 237.53576159477234 sec\n\nEpoch 9 Batch 0 Loss 0.1865\nEpoch 9 Batch 100 Loss 0.1148\nEpoch 9 Loss 0.1669\nTime taken for 1 epoch 287.65180134773254 sec\n\nEpoch 10 Batch 0 Loss 0.1089\nEpoch 10 Batch 100 Loss 0.1153\nEpoch 10 Loss 0.1247\nTime taken for 1 epoch 349.3731667995453 sec\n\n"}],"source":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n  loss = 0\n\n  with tf.GradientTape() as tape:\n    enc_output, enc_hidden = encoder(inp, enc_hidden)\n\n    dec_hidden = enc_hidden\n\n    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n\n    #将这个时刻的目标输出当作下时刻的输入\n    for t in range(1, targ.shape[1]):\n   \n      predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n\n      loss += loss_function(targ[:, t], predictions)\n\n      dec_input = tf.expand_dims(targ[:, t], 1)\n\n  batch_loss = (loss / int(targ.shape[1]))\n\n  variables = encoder.trainable_variables + decoder.trainable_variables\n\n  gradients = tape.gradient(loss, variables)\n\n  optimizer.apply_gradients(zip(gradients, variables))\n\n  return batch_loss\n\n\n#利用数据集循环训练10次\nEPOCHS = 10\n\nfor epoch in range(EPOCHS):\n  start = time.time()\n\n  enc_hidden = encoder.initialize_hidden_state()\n  total_loss = 0\n\n  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n    batch_loss = train_step(inp, targ, enc_hidden)\n    total_loss += batch_loss\n\n    if batch % 100 == 0:\n        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                     batch,\n                                                     batch_loss.numpy()))\n  # 每2个epoch，保存一次前面定义的(checkpoint)\n  if (epoch + 1) % 2 == 0:\n    checkpoint.save(file_prefix = checkpoint_prefix)\n\n  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                      total_loss / steps_per_epoch))\n  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# 步骤4：使用模型进行翻译"},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Input: <start> esta es mi vida . <end>\nPredicted translation: this is my life . <end> \n"}],"source":"def evaluate(sentence):\n    sentence = preprocess_sentence(sentence)\n\n    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n                                                           maxlen=max_length_inp,\n                                                           padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n\n    result = ''\n\n    hidden = [tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n\n    for t in range(max_length_targ):\n        predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_out)\n\n        predicted_id = tf.argmax(predictions[0]).numpy()\n\n        result += targ_lang.index_word[predicted_id] + ' '\n\n        if targ_lang.index_word[predicted_id] == '<end>':\n            return result, sentence\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    return result, sentence\n\n\n\ndef translate(sentence):\n    result, sentence = evaluate(sentence)\n\n    print('Input: %s' % (sentence))\n    print('Predicted translation: {}'.format(result))\n\n#翻译括号中的句子\ntranslate(u'esta es mi vida.')"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Input: <start> hace mucho frio aqui . <end>\nPredicted translation: it s be true . <end> \nInput: <start> ¿ todavia estan en casa ? <end>\nPredicted translation: are at home ? <end> \n"}],"source":"#看看更多的例子\ntranslate(u'hace mucho frio aqui.')\ntranslate(u'¿todavia estan en casa?')"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}