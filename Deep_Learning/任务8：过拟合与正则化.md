[TOC]
# 任务8：过拟合与正则化

## 1 任务目标

1. 理解过拟合的概念

2. 理解用正则化的方法解决过拟合的原理

3. 正则化的 TensorFlow 实现


## 2 任务描述

在任务6的任务拓展中，已经简单地介绍了过拟合的问题，以及用 dropout 的方法来缓解过拟合；在本次任务中，将面对过拟合的问题，提出一种叫做正则化的解决方案。


## 3 知识准备


### 3.1 过拟合（overfiting）

所谓过拟合，指的是当一个模型过为复杂之后，它可以很好地“记忆”每一个训练数据中随机噪音的部分而忘记了要去“学习”训练数据中通用的趋势。

过拟合直观的体现就是在训练集表现得很好，在测试集表现得较差。任务7中的任务实施就是一个例子，经过几千次的迭代后，模型在训练集上的准确率比在测试集上的准确率高十几个百分点。

![](https://p.pstatp.com/origin/dc11000298b384d7a672)

### 3.2 正则化（regularizer）

前面我们讲到了 dropout，而在实际使用中，现在的网络往往不使用 dropout，而是用另外一个技术，叫正则化。

简单来说，正则化就是在损失函数 loss 中加入一个数据项，这个数据项在模型训练时会增加一个惩罚项，使其权重不至于过大，从而避免过于看重训练集。

正则化的思想就是在损失函数中加入刻画模型复杂程度的指标。假设用于刻画模型在训练数据上表现的损失函数为：`$J(\theta )$`，那么在优化时不是直接优化`$J(\theta )$`的，而是优化`$ f=J(\theta )+ \lambda R(w)$`。

其中`$R(w)$`刻画的是模型的复杂程度，而正则项的系数 λ 表示模型复杂损失在总损失中的比例。注意这里 θ 表示的是一个神经网络中所有的参数，它包括边上的权重w和偏置项b。一般来说模型复杂度只由权重w决定。常用的刻画模型复杂度的函数R(w)有两种，一种是L1正则化，计算公式是：

```math
R(w)= ||w_i||_1 = \sum_i|w_i|
```
另一种是L2 正则化，计算公式是：


```math
R(w)= ||w_i||_2^2 = \sum_i|w_i^2|
```
如果我们对新的损失函数 f 求导进行梯度下降，就有：


```math
\frac{\partial f}{\partial w_i} = \frac{\partial j(\theta)}{\partial w_i} + 2 \lambda w_i
```

那么在更新参数的时候就有


```math
w_i \rightarrow w_i - \eta (\frac{\partial J(\theta )}{\partial w_i} + 2 \lambda w_i) = w_i - \eta \frac{\partial J(\theta )}{\partial w_i} - 2 \eta \lambda w_i 
```
可以看到  `$w_i - \eta \frac{\partial J(\theta )}{\partial w_i}$` 和没加正则项要更新的部分一样，而后面的 `$ 2 \eta \lambda w_i$`  就是正则项的影响，可以看到加完正则项之后会对参数做更大程度的更新，这也被称为权重衰减(weight decay)。

无论是哪一种正则化方式，基本的思想都是希望通过限制权重的大小，使得模型不能任意拟合训练数据中的随机噪音。但这两种正则化的方法也有很大的区别。首先，L1 正则化会让参数变得更稀疏，而 L2 正则化不会。所谓参数变得更稀疏是指会有更多的参数变为 0 ，这样可以达到类似特征选取的功能。之所以L2 正则化不会让参数变得稀疏的原因是当参数很小时，比如0.001 ，这个参数的平方基本上就可以忽略了，于是模型不会进一步将这个参数调整为 0 。

注意：正则项的系数`$ \lambda$`的大小非常重要，如果太大，会极大的抑制参数的更新，导致欠拟合，如果太小，那么正则项这个部分基本没有贡献，所以选择一个合适的权重衰减系数非常重要，这个需要根据具体的情况去尝试，初步尝试可以使用 1e-4 或者 1e-3


## 4 任务实施


### 4.1 实施思路

调用 tensorflow 中已封装好的方法实现正则化

### 4.2 实施步骤

#### 4.2.1 方法一

正则化常用到集合，下面是最原始的添加正则化办法，直接在变量声明后将之添加进'losses'集合


```
import tensorflow as tf

#这是一个权重初始化的例子
def get_weights(shape, lambd):
    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)
    #只需在这里（也就是声明后），将带正则项添加进'losses'集合
    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(lambd)(var))
    return var
 
y = tf.placeholder(tf.float32, shape=(None, 1))
y_ = tf.placeholder(tf.float32, shape=(None, 1)) 
   
#这是一个损失函数的例子 
mess_loss = tf.reduce_mean(tf.square(y_-y)) 

#将其也添加进'losses'集合里
tf.add_to_collection('losses', mess_loss)

#然后从该集合取得加入了正则项的损失函数
loss = tf.add_n(tf.get_collection('losses'))
```

#### 4.2.2 方法二

利用函数 `$tf.contrib.layers.apply\_regularization(regularizer, weights\_list=None)$`

先看参数：
- regularizer: 就是我们方法一中创建的正则化方法 `$tf.contrib.layers.l2\_regularizer()$`
- weights_list: 想要执行正则化方法的参数列表,如果为None的话,就取GraphKeys.WEIGHTS中的`$weights$`。

使用时只需像数学式子那样将此方法的返回值添加到目标函数上就可以了，但记得要将需要正则化的权重添加至GraphKeys.WEIGHTS当中。


```
#添加两个权重至 tf.GraphKeys.WEIGHTS 当中
tf.add_to_collection(tf.GraphKeys.WEIGHTS, W_2)
tf.add_to_collection(tf.GraphKeys.WEIGHTS, W_3)

#应用该方法，weights_list不传参，默认使用 tf.GraphKeys.WEIGHTS
lambda = 0.001
regularizer = tf.contrib.layers.l2_regularizer(lambda)
reg_term = tf.contrib.layers.apply_regularization(regularizer)

 
#这里列举一个损失函数，直接加上 reg_term
#loss = (tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=z_3)) + reg_term)
```


## 5 任务拓展


**奥卡姆剃刀原理**（Occam's Razor, Ockham's Razor）它是由14世纪英格兰的逻辑学家、圣方济各会修士奥卡姆的威廉（William of Occam，约1285年至1349年）提出。这个原理称为“如无必要，勿增实体”，即“简单有效原理”。正如他在《箴言书注》2卷15题说“切勿浪费较多东西去做，用较少的东西，同样可以做好的事情。”正则化正符合奥卡姆剃刀原理。

> 用奥卡姆剃刀原理解释：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。——李航《统计学方法》

> 从贝叶斯估计角度来看：正则项对应于模型的先验概率，可以假设复杂的模型具有较小的先验概率，而简单的模型具有加大的先验概率。（类似奥卡姆剃刀解释） 



## 6 任务实训

### 6.1 实训目的

正则化体验与实战

### 6.2 实训内容

对于任务7中识别cifar-10数据集的模型，根据本任务中任务实施的做法，为该模型添加正则化后再次训练，观察前后的差别。
